This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: R/**/*.R, R/**/*.r, *.Rmd, *.rmd, DESCRIPTION, tests/**/*.R, tests/**/*.r
- Files matching patterns in .gitignore are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
R/
  all_generic.R
  anchor_selection_mra.R
  geo_hatsa_core_algorithm.R
  gev_helpers.R
  hatsa_core_algorithm.R
  hatsa_projector.R
  hatsa_qc_plots.R
  hatsa_validation_metrics.R
  hatsa-package.R
  metrics.R
  procrustes_alignment.R
  projection_helpers.R
  riemannian_geometry.R
  riemannian_methods_hatsa.R
  spd_representations.R
  spectral_graph_construction.R
  task_graph_construction.R
  task_hatsa_helpers.R
  task_hatsa_main.R
  task_hatsa_projector.R
  task_hatsa.R
  utils.R
  voxel_projection.R
  weighted_procrustes.R
tests/
  testthat/
    test-gev.R
    test-hatsa_core_functionality.R
    test-hatsa_validation_metrics.R
    test-metrics.R
    test-spectral_graph_construction.R
    test-task_graph_construction.R
    test-task_hatsa_graphs.R
    test-task_hatsa_integration.R
    test-task_hatsa_main.R
    test-task_hatsa.R
    test-voxel_projection.R
  testthat.R
DESCRIPTION
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="R/all_generic.R">
scores <- multivarious::scores
</file>

<file path="R/anchor_selection_mra.R">
#' Manifold-Regularized Anchor Selection (MRA-Select)
#'
#' Selects a set of anchor parcels by optimizing a criterion that balances
#' the stability of the mean anchor representation (via condition number) and
#' the dispersion of subject-specific covariance matrices derived from these anchors
#' on the SPD manifold.
#'
#' @param U_original_list_pilot A list of original (unaligned) sketch matrices
#'   (V_p x k) for a set of pilot subjects. These are used to evaluate candidate anchors.
#' @param k_spectral_rank Integer, the spectral rank k used to generate `U_original_list_pilot`.
#' @param m_target Integer, the desired number of anchors to select.
#' @param total_parcels Integer, the total number of parcels (V_p) available for selection.
#'   If NULL, inferred from `U_original_list_pilot`.
#' @param max_kappa Numeric, the maximum allowable condition number for the mean anchor matrix.
#'   Candidates leading to a kappa above this are penalized or excluded. Default: 100.
#' @param weight_inv_kappa Numeric, weight for the inverse condition number term in the score.
#'   Higher values prioritize lower condition numbers (more stable mean). Default: 1.0.
#' @param weight_dispersion Numeric, weight for the negative dispersion term in the score.
#'   Higher values prioritize lower dispersion. Default: 1.0.
#' @param initial_selection Optional. A vector of pre-selected anchor indices (1-based).
#'   The algorithm will try to add `m_target - length(initial_selection)` new anchors.
#' @param candidate_pool Optional. A vector of parcel indices (1-based) from which to select
#'   new anchors. If NULL, all non-initially-selected parcels are candidates.
#' @param parcel_quality_info Optional. Data frame or list providing parcel quality or
#'   network information. Currently unused in the core MRA logic but reserved for future
#'   extensions (e.g., pre-filtering candidates).
#' @param riemannian_dispersion_options A list of options to pass to
#'   `hatsa::riemannian_dispersion_spd` (e.g., `spd_metric`, `use_geometric_median`).
#' @param min_anchors_for_metrics Integer. Minimum number of selected anchors required before
#'   kappa and dispersion metrics are considered stable enough to compute. Default is `k_spectral_rank`.
#' @param verbose Logical. If TRUE, print progress messages. Default: TRUE.
#'
#' @return A vector of selected anchor indices (1-based), sorted.
#' @export
#' @importFrom stats cov sd svd
#'
#' @examples
#' # Conceptual example (requires U_original_list_pilot from actual or synthetic data)
#' # N_pilot <- 5
#' # V_p_total <- 50
#' # k_rank <- 10
#' # U_pilot_list <- replicate(N_pilot,
#' #                           matrix(rnorm(V_p_total * k_rank), V_p_total, k_rank),
#' #                           simplify = FALSE)
#' # selected_anchors_mra <- select_anchors_mra(
#' #   U_original_list_pilot = U_pilot_list,
#' #   k_spectral_rank = k_rank,
#' #   m_target = 15,
#' #   total_parcels = V_p_total,
#' #   verbose = TRUE
#' # )
#' # print(selected_anchors_mra)
select_anchors_mra <- function(U_original_list_pilot,
                               k_spectral_rank,
                               m_target,
                               total_parcels = NULL,
                               max_kappa = 100,
                               weight_inv_kappa = 1.0,
                               weight_dispersion = 1.0,
                               initial_selection = integer(0),
                               candidate_pool = NULL,
                               parcel_quality_info = NULL, # Reserved for future use
                               riemannian_dispersion_options = list(),
                               min_anchors_for_metrics = NULL,
                               verbose = TRUE) {

  # --- Input Validation and Setup ---
  if (!is.list(U_original_list_pilot) || length(U_original_list_pilot) == 0) {
    stop("U_original_list_pilot must be a non-empty list.")
  }
  if (is.null(total_parcels)) {
    if (!is.null(U_original_list_pilot[[1]]) && is.matrix(U_original_list_pilot[[1]])) {
      total_parcels <- nrow(U_original_list_pilot[[1]])
    } else {
      stop("total_parcels not provided and cannot be inferred from U_original_list_pilot.")
    }
  }
  if (m_target <= 0 || m_target > total_parcels) {
    stop("m_target must be positive and not exceed total_parcels.")
  }
  if (is.null(min_anchors_for_metrics)) {
    min_anchors_for_metrics <- k_spectral_rank
  }
  if (min_anchors_for_metrics < 1) min_anchors_for_metrics <- 1
  
  # Validate initial_selection and candidate_pool
  initial_selection <- unique(as.integer(initial_selection))
  if (any(initial_selection < 1 | initial_selection > total_parcels)) {
    stop("initial_selection contains invalid parcel indices.")
  }
  if (length(initial_selection) >= m_target) {
    if (verbose) message("Initial selection already meets or exceeds m_target. Returning initial selection.")
    return(sort(initial_selection))
  }

  if (is.null(candidate_pool)) {
    candidate_pool <- setdiff(1:total_parcels, initial_selection)
  } else {
    candidate_pool <- unique(as.integer(candidate_pool))
    if (any(candidate_pool < 1 | candidate_pool > total_parcels)) {
      stop("candidate_pool contains invalid parcel indices.")
    }
    candidate_pool <- setdiff(candidate_pool, initial_selection)
  }
  if (length(candidate_pool) == 0 && length(initial_selection) < m_target) {
     warning("No candidate anchors available to reach m_target. Returning current selection.")
     return(sort(initial_selection))
  }

  selected_anchors <- initial_selection
  num_to_select_additionally <- m_target - length(selected_anchors)

  if (verbose) {
    message_stage(sprintf("Starting MRA-Select: Target %d anchors (%d pre-selected, %d to find).", 
                          m_target, length(initial_selection), num_to_select_additionally), interactive_only = TRUE)
  }

  # --- Helper: Calculate Kappa ---
  .calculate_kappa <- function(matrix_in) {
    if (is.null(matrix_in) || !is.matrix(matrix_in) || min(dim(matrix_in)) == 0) return(Inf)
    # Kappa is typically for m x k where m >= k.
    # If m < k, it will be rank deficient, min singular value often 0 (or near 0 due to numerics).
    # svd() handles non-square matrices.
    s_vals <- tryCatch(svd(matrix_in, nu=0, nv=0)$d, error = function(e) NULL)
    if (is.null(s_vals) || length(s_vals) == 0) return(Inf)
    min_s_val <- min(s_vals[s_vals > .Machine$double.eps]) # Smallest positive singular value
    if (length(min_s_val) == 0 || min_s_val < .Machine$double.eps) return(Inf)
    return(max(s_vals) / min_s_val)
  }
  
  # --- Helper: Calculate Metrics for a Tentative Set ---
  .evaluate_anchor_set <- function(current_anchor_indices) {
    num_current_anchors <- length(current_anchor_indices)
    if (num_current_anchors < 1) return(list(kappa = Inf, dispersion = Inf, score = -Inf))

    # Metric 1: Kappa of the Euclidean mean of anchor rows from U_original_list_pilot
    kappa_val <- Inf
    if (num_current_anchors >= min_anchors_for_metrics || num_current_anchors >= k_spectral_rank) {
        anchor_matrices_pilot <- lapply(U_original_list_pilot, function(U_subj) {
            if (is.null(U_subj) || nrow(U_subj) < max(current_anchor_indices)) return(NULL)
            U_subj[current_anchor_indices, , drop = FALSE]
        })
        valid_anchor_matrices <- Filter(Negate(is.null), anchor_matrices_pilot)
        if (length(valid_anchor_matrices) > 0) {
            # Check all matrices have same dim: num_current_anchors x k_spectral_rank
            if (!all(sapply(valid_anchor_matrices, function(m) all(dim(m) == c(num_current_anchors, k_spectral_rank))))) {
                warning("Inconsistent dimensions in pilot anchor matrices for kappa calculation.")
            } else {
                mean_anchor_matrix <- Reduce("+", valid_anchor_matrices) / length(valid_anchor_matrices)
                kappa_val <- .calculate_kappa(mean_anchor_matrix)
            }
        } else {
            warning("No valid pilot anchor matrices for kappa calculation.")
        }
    }
    
    # Metric 2: Riemannian Dispersion of covariance matrices of anchor rows
    dispersion_val <- Inf
    if (num_current_anchors >= min_anchors_for_metrics || num_current_anchors >= k_spectral_rank) { # Need enough rows for cov
        spd_list_pilot <- lapply(U_original_list_pilot, function(U_subj) {
            if (is.null(U_subj) || nrow(U_subj) < max(current_anchor_indices)) return(NULL)
            anchor_rows_subj <- U_subj[current_anchor_indices, , drop = FALSE]
            if (nrow(anchor_rows_subj) < 2 || nrow(anchor_rows_subj) < k_spectral_rank) return(NULL) # cov needs multiple observations
            tryCatch(stats::cov(anchor_rows_subj), error = function(e) NULL)
        })
        valid_spd_list <- Filter(Negate(is.null), spd_list_pilot)
        
        if (length(valid_spd_list) > 1) {
            # Ensure all SPD matrices are k x k
            valid_spd_list <- Filter(function(S) is.matrix(S) && all(dim(S) == c(k_spectral_rank, k_spectral_rank)), valid_spd_list)
            if (length(valid_spd_list) > 1) {
                disp_opts <- riemannian_dispersion_options
                # Construct a temporary projector-like object or pass list directly if API allows
                # For now, assuming riemannian_dispersion_spd can take a raw list of SPDs if object=NULL
                # This requires modification to riemannian_dispersion_spd or a new helper.
                # HACK: Create minimal list structure that riemannian_dispersion_spd might accept if it looks for object$N_subjects
                # This is not ideal. Better: riemannian_dispersion_spd should have a method for lists.
                # For now, let's assume we pass to a conceptual helper: calculate_dispersion_for_spd_list(valid_spd_list, disp_opts)
                # For a quick implementation, let's assume riemannian_dispersion_spd can be adapted or we use a direct calc:
                
                # Placeholder for direct calculation if riemannian_dispersion_spd cannot be easily used:
                # 1. Compute Frechet mean of valid_spd_list (e.g., using hatsa::frechet_mean_spd)
                # 2. Compute sum of squared distances to mean.
                # This is simplified here. Proper use of riemannian_dispersion_spd is preferred.
                
                # Simulate calling riemannian_dispersion_spd (needs a method for raw lists or dummy object)
                # This is a temporary workaround. The actual call would depend on how `riemannian_dispersion_spd` can handle raw SPD lists.
                # We will assume for now it fails gracefully if it cannot. The ticket implies using it.
                dummy_projector_for_disp <- list(
                    parameters = list(N_subjects = length(valid_spd_list)),
                    # get_spd_representations would be mocked to return valid_spd_list
                    # This part needs careful handling based on actual API of riemannian_dispersion_spd
                    .get_spd_representations_output = valid_spd_list # Internal convention for this example
                )
                class(dummy_projector_for_disp) <- "hatsa_projector" # To satisfy S3 dispatch if needed

                current_disp_opts <- riemannian_dispersion_options
                current_disp_opts$object <- NULL # Indicate we are providing spd_matrices_list directly
                current_disp_opts$spd_matrices_list <- valid_spd_list
                current_disp_opts$verbose <- FALSE # Suppress verbose from internal call usually

                # Need to ensure riemannian_dispersion_spd can accept spd_matrices_list
                # if object is NULL. This is a new requirement for that function.
                # For now, let's assume a direct calculation for simplicity of MRA-select draft:
                if (length(valid_spd_list) > 1) {
                    mean_spd <- tryCatch(hatsa::frechet_mean_spd(valid_spd_list, 
                                                                metric = current_disp_opts$spd_metric %||% "logeuclidean",
                                                                tol = current_disp_opts$frechet_tol %||% 1e-7,
                                                                max_iter = current_disp_opts$frechet_max_iter %||% 50,
                                                                verbose = FALSE), error = function(e) NULL)
                    if (!is.null(mean_spd)){
                        distances_sq <- sapply(valid_spd_list, function(S_i) {
                            dist_val <- tryCatch(hatsa::riemannian_distance_spd(S_i, mean_spd, 
                                                                             metric=current_disp_opts$spd_metric %||% "logeuclidean", 
                                                                             epsilon = current_disp_opts$spd_regularize_epsilon %||% 1e-6)^2,
                                                 error = function(e) NA)
                            return(dist_val)
                        })
                        dispersion_val <- mean(na.omit(distances_sq)) # Mean squared distance
                        if(is.nan(dispersion_val)) dispersion_val <- Inf
                    } else { dispersion_val <- Inf }
                } else { dispersion_val <- Inf }
            } else { dispersion_val <- Inf }
        } else { dispersion_val <- Inf }
    }

    # Combine metrics into a score (maximize this score)
    # Score = w1 * (1/kappa) - w2 * dispersion
    # If kappa is Inf or > max_kappa, 1/kappa term becomes 0 or score penalized
    inv_kappa_term <- 0
    if (is.finite(kappa_val) && kappa_val > .Machine$double.eps) {
        if (kappa_val <= max_kappa) {
            inv_kappa_term <- 1 / kappa_val
        } else {
            inv_kappa_term <- 1 / max_kappa # Penalize but don't make it zero if slightly over
                                         # or simply make score -Inf: if (kappa_val > max_kappa) score <- -Inf
        }
    }
    current_score <- weight_inv_kappa * inv_kappa_term - weight_dispersion * ifelse(is.finite(dispersion_val), dispersion_val, Inf)
    if (kappa_val > max_kappa) current_score <- -Inf # Hard constraint

    return(list(kappa = kappa_val, dispersion = dispersion_val, score = current_score))
  }

  # --- Greedy Selection Loop ---
  for (i in 1:num_to_select_additionally) {
    best_current_iteration_score <- -Inf
    best_candidate_this_iteration <- NULL

    if (length(candidate_pool) == 0) {
      if (verbose) message("No more candidates to select from.")
      break
    }

    if (verbose) {
        message(sprintf("  MRA Iteration %d/%d: Evaluating %d candidates to add to current %d anchors...", 
                        i, num_to_select_additionally, length(candidate_pool), length(selected_anchors)))
    }
    
    iter_scores <- data.frame(candidate_idx = integer(), score = numeric(), kappa = numeric(), dispersion = numeric())

    for (candidate_anchor_idx in candidate_pool) {
      tentative_anchors <- sort(unique(c(selected_anchors, candidate_anchor_idx)))
      metrics <- .evaluate_anchor_set(tentative_anchors)
      
      iter_scores <- rbind(iter_scores, data.frame(candidate_idx = candidate_anchor_idx, 
                                                    score = metrics$score, 
                                                    kappa = metrics$kappa, 
                                                    dispersion = metrics$dispersion))

      if (metrics$score > best_current_iteration_score) {
        best_current_iteration_score <- metrics$score
        best_candidate_this_iteration <- candidate_anchor_idx
      }
    }
    
    # Sort iter_scores for review (optional)
    # iter_scores <- iter_scores[order(-iter_scores$score), ]
    # if(verbose && nrow(iter_scores) > 0) {
    #    print(head(iter_scores))
    # }

    if (is.null(best_candidate_this_iteration) || best_current_iteration_score == -Inf) {
      if (verbose) message(sprintf("  MRA Iteration %d: No suitable candidate found to improve score. Stopping.", i))
      break
    }

    selected_anchors <- sort(unique(c(selected_anchors, best_candidate_this_iteration)))
    candidate_pool <- setdiff(candidate_pool, best_candidate_this_iteration)

    if (verbose) {
      sel_metrics <- .evaluate_anchor_set(selected_anchors) # Re-evaluate for the chosen set
      message(sprintf("  MRA Iteration %d: Selected anchor %d. Total selected: %d. Score: %.4f (InvKappa: %.4f, Disp: %.4f)",
                      i, best_candidate_this_iteration, length(selected_anchors), 
                      sel_metrics$score, ifelse(is.finite(sel_metrics$kappa), 1/sel_metrics$kappa, 0), sel_metrics$dispersion))
    }
  }

  if (verbose) {
    final_metrics <- .evaluate_anchor_set(selected_anchors)
    message_stage(sprintf("MRA-Select Finished. Selected %d anchors. Final Score: %.4f (InvKappa: %.4f, Disp: %.4f)", 
                          length(selected_anchors), final_metrics$score, 
                          ifelse(is.finite(final_metrics$kappa), 1/final_metrics$kappa, 0), 
                          final_metrics$dispersion), interactive_only = TRUE)
  }
  
  if (length(selected_anchors) == 0 && m_target > 0 && verbose){
      warning("MRA-Select did not select any anchors.")
  }

  return(sort(selected_anchors))
}

# Helper for null or default in function (already in hatsa_qc_plots.R, define locally or ensure source)
# Consider moving to a utils.R file if used commonly
if (!exists("%||%", mode = "function")) {
  `%||%` <- function(a, b) if (is.null(a)) b else a
}
</file>

<file path="R/geo_hatsa_core_algorithm.R">
#' Run Geometric Harmonized Tensors SVD Alignment (Geo-HATSA) Core Algorithm
#'
#' This function implements a variant of the HATSA core algorithm where the
#' Generalized Procrustes Analysis (GPA) step for refining rotations and the
#' group anchor template uses a geometric approach on SO(k) via
#' `perform_geometric_gpa_refinement`.
#'
#' @param subject_data_list A list of subject-level parcel time-series matrices.
#'   Each element is a numeric matrix (T_i time points x V_p parcels).
#' @param anchor_indices A numeric vector of indices for the anchor parcels
#'   (1-based, referring to columns of matrices in `subject_data_list`).
#' @param spectral_rank_k Integer, the number of spectral components (k) to retain.
#' @param k_conn_pos Integer, number of positive degree neighbors for k-NN graph construction.
#' @param k_conn_neg Integer, number of negative degree neighbors (if applicable, often same as pos).
#' @param n_refine Integer, number of GPA refinement iterations.
#' @param V_p Integer, number of parcels (vertices per subject). If NULL, inferred.
#' @param use_dtw Logical, whether to use Dynamic Time Warping for FC graph (passed to
#'   `compute_subject_connectivity_graph_sparse`). Default FALSE.
#' @param graph_mode Character string, specifies the graph construction method.
#'   Options include `"anchor_block"` (default), `"schur"` (Schur complement),
#'   `"full"` (standard kNN graph). Passed to
#'   `compute_subject_connectivity_graph_sparse`.
#' @param schur_eps Numeric, epsilon for Schur complement graph construction if
#'   `graph_mode = "schur"`. Default: 0.01.
#' @param eigengap_tol Numeric, tolerance for checking the eigengap after spectral
#'   sketch computation. If `lambda_k - lambda_{k+1} < eigengap_tol`, a warning
#'   is issued for that subject. Default: 1e-9.
#' @param rotation_mode Character string, passed to `perform_geometric_gpa_refinement`.
#'   One of `"svd"` (default) or `"riemannian"`. See that function for details.
#' @param verbose Logical, if TRUE, prints progress messages. Default TRUE.
#' @param frechet_mean_options A list of options to pass to `hatsa::frechet_mean_so_k`
#'   within `perform_geometric_gpa_refinement`. Default: `list()`.
#' @param gpa_tol Numeric, tolerance for convergence in `perform_geometric_gpa_refinement`.
#'   Default: 1e-7.
#'
#' @return A list containing the core Geo-HATSA results and QC metrics:
#'   \itemize{
#'     \item{\code{U_aligned_list}: List of subject-specific aligned sketch matrices (V_p x k).}
#'     \item{\code{R_final_list}: List of subject-specific rotation matrices (k x k).}
#'     \item{\code{U_original_list}: List of subject-specific original sketch matrices (V_p x k).}
#'     \item{\code{Lambda_original_list}: List of subject-specific original eigenvalues (length k).}
#'     \item{\code{Lambda_original_gaps_list}: List of subject-specific eigengap ratios.}
#'     \item{\code{T_anchor_final}: The final group anchor template matrix (N_anchors x k).}
#'     \item{\code{R_bar_final}: The final Fréchet mean of rotation matrices from Geo-GPA.}
#'     \item{\code{qc_metrics}: A data frame with per-subject QC flags (laplacian_computed_ok, 
#'           sketch_computed_ok, eigengap_sufficient).}
#'   }
#'   This list is intended to be used as input to the `hatsa_projector` constructor,
#'   though `R_bar_final` is an additional output specific to Geo-HATSA.
#'
#' @export
#' @seealso \code{\link{perform_geometric_gpa_refinement}}, \code{\link{hatsa_projector}}
#' @importFrom stats cor
run_geo_hatsa_core <- function(subject_data_list, 
                               anchor_indices, 
                               spectral_rank_k, 
                               k_conn_pos = 10, k_conn_neg = 10, 
                               n_refine = 10, 
                               V_p = NULL,
                               use_dtw = FALSE,
                               graph_mode = "anchor_block",
                               schur_eps = 0.01,
                               eigengap_tol = 1e-9,
                               rotation_mode = "svd",
                               verbose = TRUE,
                               frechet_mean_options = list(),
                               gpa_tol = 1e-7) {

  if (verbose) message_stage("Starting Geometric HATSA (Geo-HATSA) Core Algorithm...", interactive_only = TRUE)

  # --- Input Validation (simplified from run_hatsa_core, focusing on key aspects) ---
  if (!is.list(subject_data_list) || length(subject_data_list) == 0) {
    stop("subject_data_list must be a non-empty list of matrices.")
  }
  if (is.null(V_p)) {
    first_subj_data <- Filter(Negate(is.null), subject_data_list)
    if (length(first_subj_data) > 0 && is.matrix(first_subj_data[[1]])) {
      V_p <- ncol(first_subj_data[[1]])
    } else {
      stop("V_p not specified and cannot infer from first subject data.")
    }
  }
  if (V_p == 0) stop("V_p (number of parcels) cannot be zero.")
  
  pnames <- paste0("P", 1:V_p) # Default parcel names
  val_results <- validate_hatsa_inputs(subject_data_list, anchor_indices, spectral_rank_k,
                                       k_conn_pos, k_conn_neg, n_refine, V_p) # Reuse existing validator
  unique_anchor_indices <- val_results$unique_anchor_indices
  num_anchors <- length(unique_anchor_indices)

  num_subjects <- length(subject_data_list)
  U_original_list <- vector("list", num_subjects)
  Lambda_original_list <- vector("list", num_subjects)
  Lambda_original_gaps_list <- vector("list", num_subjects)

  # Initialize QC metrics dataframe
  qc_metrics <- data.frame(
      subject_id = seq_len(num_subjects),
      laplacian_computed_ok = TRUE,
      sketch_computed_ok = TRUE,
      eigengap_sufficient = TRUE,
      stringsAsFactors = FALSE
  )

  # --- Stage 1: Compute initial spectral sketches (same as run_hatsa_core) ---
  if (verbose) message_stage("Geo-HATSA Stage 1: Computing initial spectral sketches...", interactive_only = TRUE)
  if (num_subjects > 0) {
    for (i in 1:num_subjects) {
      X_i <- subject_data_list[[i]]
      if (is.null(X_i) || !is.matrix(X_i) || ncol(X_i) != V_p) {
          message(sprintf("  Geo-HATSA Stage 1: Subject %d has invalid data. Skipping.", i))
          qc_metrics$laplacian_computed_ok[i] <- FALSE
          qc_metrics$sketch_computed_ok[i] <- FALSE
          U_original_list[[i]] <- matrix(NA, nrow=V_p, ncol=spectral_rank_k)
          Lambda_original_list[[i]] <- rep(NA, spectral_rank_k)
          next
      }
      current_pnames <- colnames(X_i)
      if(is.null(current_pnames) || length(current_pnames) != V_p) current_pnames <- pnames

      W_conn_i_sparse <- tryCatch({
          compute_subject_connectivity_graph_sparse(X_i, current_pnames, k_conn_pos, k_conn_neg, use_dtw, 
                                                    graph_mode = graph_mode, schur_options = list(eps = schur_eps))
      }, error = function(e) {
          message(sprintf("  Geo-HATSA Stage 1: Error computing connectivity graph for subject %d: %s", i, e$message))
          qc_metrics$laplacian_computed_ok[i] <- FALSE
          NULL
      })
      
      if (is.null(W_conn_i_sparse)) {
          qc_metrics$sketch_computed_ok[i] <- FALSE
          U_original_list[[i]] <- matrix(NA, nrow=V_p, ncol=spectral_rank_k)
          Lambda_original_list[[i]] <- rep(NA, spectral_rank_k)
          next
      }
      L_conn_i_sparse <- compute_graph_laplacian_sparse(W_conn_i_sparse)
      
      sketch_result <- tryCatch({
          compute_spectral_sketch_sparse(L_conn_i_sparse, spectral_rank_k)
      }, error = function(e) {
          message(sprintf("  Geo-HATSA Stage 1: Error computing spectral sketch for subject %d: %s", i, e$message))
          qc_metrics$sketch_computed_ok[i] <- FALSE
          NULL
      })
      
      if (is.null(sketch_result) || is.null(sketch_result$vectors) || is.null(sketch_result$values)){
          U_original_list[[i]] <- matrix(NA, nrow=V_p, ncol=spectral_rank_k)
          Lambda_original_list[[i]] <- rep(NA, spectral_rank_k)
          qc_metrics$sketch_computed_ok[i] <- FALSE # Already set but good to be explicit
          next
      }
      U_original_list[[i]] <- sketch_result$vectors
      Lambda_original_list[[i]] <- sketch_result$values

      lambdas_i <- Lambda_original_list[[i]]
      if (!is.null(lambdas_i) && length(lambdas_i) > 1) {
        # Eigengap check
        if (spectral_rank_k < length(lambdas_i)) {
            eigengap <- lambdas_i[spectral_rank_k] - lambdas_i[spectral_rank_k + 1]
            if (eigengap < eigengap_tol) {
                qc_metrics$eigengap_sufficient[i] <- FALSE
                if(verbose) message(sprintf("  Geo-HATSA Stage 1: Warning for subject %d: Eigengap (%.2e) at k=%d is below tolerance (%.2e). Rotations might be unstable.", 
                                    i, eigengap, spectral_rank_k, eigengap_tol))
            }
        } else if (length(lambdas_i) == spectral_rank_k && spectral_rank_k > 0) {
            # Cannot compute k+1th eigenvalue, assume gap is sufficient if sketch dim matches request and is >0
            # Or consider it insufficient if strict k vs k+1 gap is required. For now, mark as sufficient.
             qc_metrics$eigengap_sufficient[i] <- TRUE 
        } else {
            qc_metrics$eigengap_sufficient[i] <- FALSE # Not enough eigenvalues for rank k
        }
        
        denominators <- lambdas_i[-length(lambdas_i)]
        denominators[abs(denominators) < .Machine$double.eps^0.5] <- NA # Avoid division by zero
        gaps_i <- (lambdas_i[-1] - lambdas_i[-length(lambdas_i)]) / denominators
        Lambda_original_gaps_list[[i]] <- gaps_i
      } else {
        Lambda_original_gaps_list[[i]] <- numeric(0)
        qc_metrics$eigengap_sufficient[i] <- FALSE # Not enough eigenvalues
      }
      
      if (num_subjects > 10 && i %% floor(num_subjects/5) == 0 && interactive() && verbose) {
          message(sprintf("  Geo-HATSA Stage 1: Processed subject %d/%d", i, num_subjects))
      }
    }
  }
  if (verbose) message_stage("Geo-HATSA Stage 1 complete.", interactive_only = TRUE)

  # --- Stage 2: Performing iterative GEOMETRIC refinement (Geo-GPA) ---
  if (verbose) message_stage("Geo-HATSA Stage 2: Performing iterative geometric refinement (Geo-GPA)...", interactive_only = TRUE)
  
  # Prepare A_originals_list (anchor sketches)
  A_originals_list <- lapply(U_original_list, function(U_orig_subj) {
    if (is.null(U_orig_subj) || !is.matrix(U_orig_subj) || nrow(U_orig_subj) != V_p || ncol(U_orig_subj) != spectral_rank_k || num_anchors == 0) {
      return(matrix(NA, nrow = num_anchors, ncol = spectral_rank_k))
    }
    U_orig_subj[unique_anchor_indices, , drop = FALSE]
  })
  
  # Filter out any NA matrices from A_originals_list that resulted from failed U_original_list entries
  valid_A_indices_for_gpa <- !sapply(A_originals_list, function(A) any(is.na(A)) || nrow(A) != num_anchors || ncol(A) != spectral_rank_k)
  A_originals_for_gpa <- A_originals_list[valid_A_indices_for_gpa]
  
  initial_R_for_gpa <- replicate(sum(valid_A_indices_for_gpa), diag(spectral_rank_k), simplify=FALSE) # Default

  if(length(A_originals_for_gpa) < 2 && verbose){
      message("Geo-HATSA Stage 2: Fewer than 2 subjects have valid anchor sketches for GPA. Skipping refinement.")
      # Create dummy geo_gpa_results if GPA is skipped
      R_final_list_placeholder <- vector("list", num_subjects)
      for(idx in 1:num_subjects) R_final_list_placeholder[[idx]] <- if(qc_metrics$sketch_computed_ok[idx]) diag(spectral_rank_k) else matrix(NA, spectral_rank_k, spectral_rank_k)
      geo_gpa_results <- list(
          R_final_list = R_final_list_placeholder,
          T_anchor_final = matrix(NA, nrow = num_anchors, ncol = spectral_rank_k),
          R_bar_final = diag(spectral_rank_k)
      )
  } else if (length(A_originals_for_gpa) >= 2) {
      geo_gpa_results <- perform_geometric_gpa_refinement(
        A_originals_list = A_originals_for_gpa, 
        n_refine = n_refine,
        k = spectral_rank_k,
        m_rows = num_anchors, 
        tol = gpa_tol,
        rotation_mode = rotation_mode,
        frechet_mean_options = frechet_mean_options,
        verbose = verbose,
        initial_R_list = initial_R_for_gpa
      )
      # Map results back to the full list of subjects
      R_final_list_full <- vector("list", num_subjects)
      gpa_result_idx <- 1
      for(subj_idx in 1:num_subjects){
          if(valid_A_indices_for_gpa[subj_idx]){
              R_final_list_full[[subj_idx]] <- geo_gpa_results$R_final_list[[gpa_result_idx]]
              gpa_result_idx <- gpa_result_idx + 1
          } else {
              R_final_list_full[[subj_idx]] <- matrix(NA, spectral_rank_k, spectral_rank_k)
          }
      }
      geo_gpa_results$R_final_list <- R_final_list_full
  } else { # Should not happen if length(A_originals_for_gpa) < 2 handled above
       geo_gpa_results <- list(R_final_list = replicate(num_subjects, matrix(NA, spectral_rank_k, spectral_rank_k), simplify=FALSE),
                               T_anchor_final = matrix(NA, num_anchors, spectral_rank_k), R_bar_final = diag(spectral_rank_k))
  }
  
  R_final_list <- geo_gpa_results$R_final_list
  T_anchor_final <- geo_gpa_results$T_anchor_final # This is T_anchor_geo
  R_bar_final <- geo_gpa_results$R_bar_final
  
  if (verbose) message_stage("Geo-HATSA Stage 2 complete.", interactive_only = TRUE)

  # --- Stage 3: Applying final rotations (same as run_hatsa_core) ---
  if (verbose) message_stage("Geo-HATSA Stage 3: Applying final rotations...", interactive_only = TRUE)
  U_aligned_list <- vector("list", num_subjects)
  if (num_subjects > 0) {
    for (i in 1:num_subjects) {
      U_orig_i <- U_original_list[[i]]
      R_final_i <- R_final_list[[i]]
      
      if (!is.null(U_orig_i) && is.matrix(U_orig_i) && !any(is.na(U_orig_i)) && 
          nrow(U_orig_i) == V_p && ncol(U_orig_i) == spectral_rank_k &&
          !is.null(R_final_i) && is.matrix(R_final_i) && !any(is.na(R_final_i)) &&
          nrow(R_final_i) == spectral_rank_k && ncol(R_final_i) == spectral_rank_k) {
        U_aligned_list[[i]] <- U_orig_i %*% R_final_i
      } else { 
        U_aligned_list[[i]] <- matrix(NA, nrow = V_p, ncol = spectral_rank_k) 
        if(verbose && (is.null(U_orig_i) || any(is.na(U_orig_i)) || is.null(R_final_i) || any(is.na(R_final_i)))){
            message(sprintf("  Geo-HATSA Stage 3: Subject %d has NA/NULL U_original or R_final. Aligned sketch set to NA.", i))
        }
      }
    }
  }
  if (verbose) message_stage("Geo-HATSA Stage 3 complete. Core Geo-HATSA finished.", interactive_only = TRUE)
  
  # Consolidate results
  geo_hatsa_core_results <- list(
    U_aligned_list = U_aligned_list,
    R_final_list = R_final_list,
    U_original_list = U_original_list,
    Lambda_original_list = Lambda_original_list, 
    Lambda_original_gaps_list = Lambda_original_gaps_list,
    T_anchor_final = T_anchor_final,
    R_bar_final = R_bar_final, 
    qc_metrics = qc_metrics
  )

  # Parameters list (similar to run_hatsa_core, maybe add a geo_hatsa_specific field)
  # parameters <- list(
  #   k = spectral_rank_k,
  #   N_subjects = num_subjects,
  #   V_p = V_p, 
  #   method = "geo_hatsa_core", # Indicate Geo-HATSA was used
  #   anchor_indices = unique_anchor_indices,
  #   k_conn_pos = k_conn_pos,
  #   k_conn_neg = k_conn_neg,
  #   n_refine = n_refine,
  #   use_dtw = use_dtw,
  #   gpa_tol = gpa_tol 
  #   # frechet_mean_options could also be stored if needed for reproducibility
  # )
  
  # The projector expects results from `hatsa_core_results` and parameters.
  # If we use the same `hatsa_projector`, `R_bar_final` will be an extra item.
  # Or, one could define a `geo_hatsa_projector` if R_bar_final needs special handling.
  # For now, just returning the list. The projector constructor would need to know about it.

  return(geo_hatsa_core_results)
}
</file>

<file path="R/gev_helpers.R">
#' Solve Generalized Eigenvalue Problem for Laplacians using PRIMME
#'
#' Solves the generalized eigenvalue problem `A v = λ B v` where A and B are
#' typically sparse graph Laplacians (e.g., `L_task` and `L_conn`). It finds
#' eigenvectors corresponding to the smallest magnitude eigenvalues (`λ`).
#'
#' @param A The sparse, symmetric matrix on the left side (e.g., `L_task`, `dgCMatrix`).
#' @param B The sparse, symmetric, positive semi-definite matrix on the right
#'   side (e.g., `L_conn`, `dgCMatrix`). Will be regularized.
#' @param k_request Integer, the number of eigenvalues/vectors to compute (`NEig`).
#' @param lambda_max_thresh Numeric, the maximum absolute eigenvalue (`|λ|`)
#'   to retain. Eigenpairs with `abs(values) >= lambda_max_thresh` are discarded.
#' @param epsilon_reg_B Numeric, small value to add to the diagonal of B for
#'   regularization (`B_reg = B + epsilon_reg_B * I`). Helps ensure B is
#'   positive definite for the solver. Default 1e-6.
#' @param tol Numeric, tolerance for eigenvalue decomposition convergence.
#'   Default 1e-8 (PRIMME's default is 1e-6, using slightly tighter).
#' @param ... Additional arguments passed to `PRIMME::eigs_sym`.
#'
#' @return A list containing:
#'   \itemize{
#'     \item{\code{vectors}: A dense matrix (`V_p x k_actual`) of filtered eigenvectors.}
#'     \item{\code{values}: A numeric vector of the corresponding filtered eigenvalues.}
#'     \item{\code{n_converged}: The number of eigenpairs PRIMME reports converged.}
#'     \item{\code{n_filtered}: The number of eigenpairs remaining after filtering.}
#'     \item{\code{primme_stats}: The stats list returned by PRIMME.}
#'   }
#'   Throws an error if computation fails. Note: Stability
#'   filtering based on split-half reliability (`r_split`) needs to be applied
#'   separately. Eigenvectors are B-orthogonal.
#'
#' @importFrom Matrix Diagonal t
#' @importFrom PRIMME eigs_sym
#' @importFrom methods is
#' @keywords internal
solve_gev_laplacian_primme <- function(A, B, k_request,
                                         lambda_max_thresh = 0.8,
                                         epsilon_reg_B = 1e-6,
                                         tol = 1e-8,
                                         ...) {

  if (!requireNamespace("PRIMME", quietly = TRUE)) {
      stop("The 'PRIMME' package is required for GEV solving. Please install it.")
  }
    
  if (!is(A, "sparseMatrix") || !is(B, "sparseMatrix")) {
    stop("Inputs A and B must be sparse matrices.")
  }
  V_p <- nrow(A)
  if (V_p == 0) return(list(vectors=matrix(0,0,0), values=numeric(0), n_converged=0, n_filtered=0, primme_stats=list()))
  if (nrow(B) != V_p || ncol(A) != V_p || ncol(B) != V_p) {
    stop("Input matrices A and B must be square and of the same dimension.")
  }

  # Ensure k is valid
  k_solve <- min(k_request, V_p)
  if (k_solve <= 0) {
    warning("k_request must be positive. Cannot solve GEV.")
    return(list(vectors=matrix(0,V_p,0), values=numeric(0), n_converged=0, n_filtered=0, primme_stats=list()))
  }

  # Regularize B: B_reg = B + epsilon * I
  if (epsilon_reg_B > 0) {
    # Safer alternative: explicitly create B_reg without modifying B in the calling scope
    B_reg <- B + epsilon_reg_B * Matrix::Diagonal(V_p)
  } else {
    B_reg <- B # No regularization if epsilon is zero or negative
  }

  # Solve the generalized eigenvalue problem A*v = lambda*B_reg*v using PRIMME
  message_stage(sprintf("Solving generalized eigenvalue problem with PRIMME for %d components...", k_solve), interactive_only = TRUE)
  gev_result <- tryCatch({
    # PRIMME uses 'SA' for Smallest Algebraic, 'SM' isn't directly listed but maps
    # to 'primme_closest_abs' which defaults to targetShift=0. We want smallest lambda magnitude.
    # Using targetShifts=0 and which="primme_closest_abs" seems most appropriate.
    # Let's try which="SM" directly first as it seems intended to map
    PRIMME::eigs_sym(A = A, NEig = k_solve, B = B_reg,
                       which = "SM", 
                       tol = tol,
                       ...)
  }, error = function(e) {
    stop(paste("PRIMME generalized eigenvalue decomposition failed:", e$message))
  })

  n_converged <- length(gev_result$values)
  if (n_converged == 0) {
     warning("PRIMME GEV solver did not converge for any eigenpairs.")
     return(list(vectors = matrix(0, V_p, 0), values = numeric(0), n_converged = 0, n_filtered = 0, primme_stats = gev_result$stats))
  }
  message_stage(sprintf("PRIMME GEV solver converged for %d/%d eigenpairs.", sum(!is.na(gev_result$values)), k_solve), interactive_only = TRUE)

  # Filter based on lambda_max_thresh
  # PRIMME might return NAs for non-converged values
  converged_idx <- !is.na(gev_result$values)
  values_raw <- gev_result$values[converged_idx]
  vectors_raw <- gev_result$vectors[, converged_idx, drop = FALSE] # V_p x n_converged_valid

  # Use absolute value for thresholding magnitude
  filter_idx <- which(abs(values_raw) < lambda_max_thresh)

  if (length(filter_idx) == 0) {
      message_stage(sprintf("No converged eigenvalues passed the lambda_max_thresh < %.3f filter.", lambda_max_thresh), interactive_only = TRUE)
      vectors_filtered <- matrix(0, V_p, 0)
      values_filtered <- numeric(0)
  } else {
      vectors_filtered <- vectors_raw[, filter_idx, drop = FALSE]
      values_filtered <- values_raw[filter_idx]

      # Sort filtered results by eigenvalue magnitude (absolute value) for consistency
      sorted_order <- order(abs(values_filtered))
      values_filtered <- values_filtered[sorted_order]
      vectors_filtered <- vectors_filtered[, sorted_order, drop = FALSE]
      
      message_stage(sprintf("%d converged eigenvalues passed the lambda_max_thresh < %.3f filter.", length(filter_idx), lambda_max_thresh), interactive_only = TRUE)
  }
  
  # Note: Further filtering based on stability (e.g., split-half r > 0.6)
  # and splitting into labGEV/gevShared patches based on value ranges
  # should be done by the calling function.

  return(list(
    vectors = vectors_filtered,
    values = values_filtered,
    n_converged = sum(converged_idx), # Number of non-NA eigenvalues returned
    n_filtered = length(filter_idx),
    primme_stats = gev_result$stats  # Pass along PRIMME stats
  ))
}

#' @title Compute GEV Spectrum Diagnostics
#' @description Computes various diagnostic statistics for a vector of Generalized Eigenvalues (GEV).
#'
#' @param Lambda_GEV A numeric vector of eigenvalues obtained from GEV.
#' @param lambda_max_thresh A numeric threshold used to categorize eigenvalues.
#'        Eigenvalues with absolute value less than this are considered 'retained' or 'stable'.
#'
#' @return A list containing the following diagnostic statistics:
#'   \\itemize{
#'     \\item{n_eigenvalues: Total number of eigenvalues.}
#'     \\item{min_eigenvalue: Minimum eigenvalue.}
#'     \\item{max_eigenvalue: Maximum eigenvalue.}
#'     \\item{mean_eigenvalue: Mean of eigenvalues.}
#'     \\item{median_eigenvalue: Median of eigenvalues.}
#'     \\item{sd_eigenvalue: Standard deviation of eigenvalues.}
#'     \\item{n_below_thresh: Number of eigenvalues with absolute value < lambda_max_thresh.}
#'     \\item{prop_below_thresh: Proportion of eigenvalues with absolute value < lambda_max_thresh.}
#'     \\item{n_above_thresh: Number of eigenvalues with absolute value >= lambda_max_thresh.}
#'     \\item{prop_above_thresh: Proportion of eigenvalues with absolute value >= lambda_max_thresh.}
#'   }
#' @export
#' @examples
#'   Lambda_GEV_sample <- c(0.1, 0.5, 0.85, 0.95, 1.2)
#'   compute_gev_spectrum_diagnostics(Lambda_GEV_sample, lambda_max_thresh = 0.8)
compute_gev_spectrum_diagnostics <- function(Lambda_GEV, lambda_max_thresh) {
  if (!is.numeric(Lambda_GEV) || !is.vector(Lambda_GEV)) {
    stop("Lambda_GEV must be a numeric vector.")
  }
  if (!is.numeric(lambda_max_thresh) || length(lambda_max_thresh) != 1) {
    stop("lambda_max_thresh must be a single numeric value.")
  }
  if (length(Lambda_GEV) == 0) {
    return(
      list(
        n_eigenvalues = 0,
        min_eigenvalue = NA_real_,
        max_eigenvalue = NA_real_,
        mean_eigenvalue = NA_real_,
        median_eigenvalue = NA_real_,
        sd_eigenvalue = NA_real_,
        n_below_thresh = 0,
        prop_below_thresh = NA_real_,
        n_above_thresh = 0,
        prop_above_thresh = NA_real_
      )
    )
  }

  n_eigenvalues <- length(Lambda_GEV)
  abs_Lambda_GEV <- abs(Lambda_GEV)

  n_below_thresh <- sum(abs_Lambda_GEV < lambda_max_thresh)
  prop_below_thresh <- n_below_thresh / n_eigenvalues
  n_above_thresh <- n_eigenvalues - n_below_thresh
  prop_above_thresh <- n_above_thresh / n_eigenvalues

  stats <- list(
    n_eigenvalues = n_eigenvalues,
    min_eigenvalue = min(Lambda_GEV, na.rm = TRUE),
    max_eigenvalue = max(Lambda_GEV, na.rm = TRUE),
    mean_eigenvalue = mean(Lambda_GEV, na.rm = TRUE),
    median_eigenvalue = stats::median(Lambda_GEV, na.rm = TRUE),
    sd_eigenvalue = stats::sd(Lambda_GEV, na.rm = TRUE),
    n_below_thresh = n_below_thresh,
    prop_below_thresh = prop_below_thresh,
    n_above_thresh = n_above_thresh,
    prop_above_thresh = prop_above_thresh
  )

  return(stats)
}
</file>

<file path="R/hatsa_core_algorithm.R">
#' Run the Core HATSA Algorithm
#'
#' Implements the Core HATSA algorithm to align functional connectivity patterns
#' across subjects. This version uses sparse matrices for graph representations,
#' efficient eigendecomposition via `RSpectra`, and incorporates robustness
#' improvements based on detailed audits.
#'
#' @param subject_data_list A list of dense numeric matrices. Each matrix `X_i`
#'   corresponds to a subject, with dimensions `T_i` (time points) x `V_p` (parcels).
#' @param anchor_indices A numeric vector of 1-based indices for the selected
#'   anchor parcels. Duplicate indices will be removed.
#' @param spectral_rank_k An integer specifying the dimensionality (`k`) of the
#'   low-dimensional spectral sketch. Must be non-negative. `k=0` yields empty sketches.
#' @param k_conn_pos An integer. For graph sparsification, number of strongest
#'   positive connections to retain per parcel.
#' @param k_conn_neg An integer. For graph sparsification, number of strongest
#'   negative connections to retain per parcel.
#' @param n_refine An integer, number of GPA refinement iterations.
#' @param use_dtw Logical, defaults to `FALSE`. If `TRUE` (not yet fully implemented),
#'   Dynamic Time Warping would be considered in graph construction similarity.
#'
#' @return A `hatsa_projector` object. This S3 object inherits from
#'   `multiblock_biprojector` (from the `multivarious` package) and contains
#'   the results of the HATSA analysis. Key components include:
#'   \itemize{
#'     \item{\code{v}: The mean aligned sketch (group-level template, V_p x k matrix).}
#'     \item{\code{s}: Stacked aligned sketches for all subjects ((N*V_p) x k matrix).}
#'     \item{\code{sdev}: Component standard deviations (vector of length k, currently defaults to 1s).}
#'     \item{\code{preproc}: Preprocessing object (currently `prep(pass())`).}
#'     \item{\code{block_indices}: List defining subject blocks in the `s` matrix.}
#'     \item{\code{R_final_list}: List of subject-specific rotation matrices (k x k).}
#'     \item{\code{U_original_list}: List of subject-specific original (unaligned) sketch matrices (V_p x k).}
#'     \item{\code{Lambda_original_list}: List of subject-specific original eigenvalues (vector of length k) from the parcel-level decomposition. Essential for Nyström voxel projection.}
#'     \item{\code{T_anchor_final}: The final group anchor template used for alignment (V_a x k matrix, where V_a is number of anchors).}
#'     \item{\code{parameters}: List of input parameters used for the HATSA run (e.g., `k`, `V_p`, `N_subjects`, anchor details, sparsification parameters).}
#'     \item{\code{method}: Character string, "hatsa_core".}
#'     \item{\code{U_aligned_list}: (Note: while used to compute `v` and `s`, direct aligned sketches per subject are also stored if `project_block` needs them or for direct inspection, typically same as `object$s` reshaped per block)}
#'   }
#'   This object can be used with S3 methods like `print`, `summary`, `coef`,
#'   `scores`, `predict` (for new parcel data), and `project_voxels` (for new
#'   voxel data).
#'
#' @examples
#' # Generate example data
#' set.seed(123)
#' N_subjects <- 3 # Small N for quick example
#' V_p_parcels <- 25
#' T_times_avg <- 50
#'
#' # Create a list of matrices (time x parcels)
#' subject_data <- lapply(1:N_subjects, function(i) {
#'   T_i <- T_times_avg + sample(-5:5, 1)
#'   matrix(stats::rnorm(T_i * V_p_parcels), nrow = T_i, ncol = V_p_parcels)
#' })
#'
#' # Assign parcel names (optional but good practice)
#' parcel_names_vec <- paste0("Parcel_", 1:V_p_parcels)
#' subject_data <- lapply(subject_data, function(mat) {
#'   colnames(mat) <- parcel_names_vec
#'   mat
#' })
#'
#' # Define HATSA parameters
#' # Ensure number of anchors >= k for stable Procrustes
#' anchor_idx <- sample(1:V_p_parcels, 7)
#' k_spectral <- 5 # k=5, num_anchors=7 is valid
#' k_pos <- 4
#' k_neg <- 2
#' n_iter_refine <- 2
#'
#' # Run Core HATSA (requires Matrix and RSpectra packages)
#' hatsa_results <- NULL
#' if (requireNamespace("Matrix", quietly = TRUE) &&
#'     requireNamespace("RSpectra", quietly = TRUE)) {
#'   hatsa_results <- tryCatch(
#'     run_hatsa_core(
#'       subject_data_list = subject_data,
#'       anchor_indices = anchor_idx,
#'       spectral_rank_k = k_spectral,
#'       k_conn_pos = k_pos,
#'       k_conn_neg = k_neg,
#'       n_refine = n_iter_refine
#'     ),
#'     error = function(e) {
#'       message("Example run failed: ", e$message)
#'       NULL
#'     }
#'   )
#'
#'   # Inspect the results object
#'   if (!is.null(hatsa_results)) {
#'     print(hatsa_results)
#'     summary_info <- summary(hatsa_results)
#'     print(summary_info)
#'
#'     # Get coefficients (mean aligned sketch)
#'     group_template <- coef(hatsa_results)
#'     # print(dim(group_template)) # Should be V_p x k
#'
#'     # Get stacked scores (aligned sketches for all subjects)
#'     all_scores <- scores(hatsa_results)
#'     # print(dim(all_scores)) # Should be (N*V_p) x k
#'
#'     # Get block indices to map scores to subjects
#'     indices <- block_indices(hatsa_results)
#'     # subject1_scores <- all_scores[indices[[1]], ]
#'     # print(dim(subject1_scores)) # Should be V_p x k
#'   }
#' } else {
#'   if (interactive()) message("Matrix and RSpectra packages needed for this example.")
#' }
#'
#' @seealso \code{\link{hatsa_projector}}, \code{\link{project_voxels.hatsa_projector}}
#' @author Expert R Developer (GPT)
#' @export
#' @importFrom stats setNames rnorm runif sd
#' @importFrom multivarious pass prep
run_hatsa_core <- function(subject_data_list, anchor_indices, spectral_rank_k,
                           k_conn_pos, k_conn_neg, n_refine, use_dtw = FALSE) {

  if (length(subject_data_list) > 0 && !is.null(subject_data_list[[1]])) {
    stopifnot(is.matrix(subject_data_list[[1]]), !inherits(subject_data_list[[1]], "Matrix"))
    V_p <- ncol(subject_data_list[[1]])
    pnames <- colnames(subject_data_list[[1]])
    if (is.null(pnames) || length(pnames) != V_p) pnames <- paste0("P", 1:V_p)
  } else { 
    V_p <- 0
    pnames <- character(0)
  }

  val_results <- validate_hatsa_inputs(subject_data_list, anchor_indices, spectral_rank_k,
                                       k_conn_pos, k_conn_neg, n_refine, V_p)
  unique_anchor_indices <- val_results$unique_anchor_indices
  
  num_subjects <- length(subject_data_list)
  U_original_list <- vector("list", num_subjects)
  Lambda_original_list <- vector("list", num_subjects)
  Lambda_original_gaps_list <- vector("list", num_subjects)
  
  message_stage("Stage 1: Computing initial spectral sketches...", interactive_only = TRUE)
  if (num_subjects > 0) {
    for (i in 1:num_subjects) {
      X_i <- subject_data_list[[i]]
      current_pnames <- colnames(X_i)
      if(is.null(current_pnames) || length(current_pnames) != V_p) current_pnames <- pnames

      W_conn_i_sparse <- compute_subject_connectivity_graph_sparse(X_i, current_pnames, k_conn_pos, k_conn_neg, use_dtw)
      L_conn_i_sparse <- compute_graph_laplacian_sparse(W_conn_i_sparse)
      
      sketch_result <- compute_spectral_sketch_sparse(L_conn_i_sparse, spectral_rank_k)
      U_original_list[[i]] <- sketch_result$vectors
      Lambda_original_list[[i]] <- sketch_result$values

      # Calculate eigengaps for the current subject
      lambdas_i <- Lambda_original_list[[i]]
      if (!is.null(lambdas_i) && length(lambdas_i) > 1) {
        # Denominator for gaps: lambdas_i[-length(lambdas_i)]
        # Ensure denominators are not zero or too small to avoid Inf/NaN
        # compute_spectral_sketch_sparse already filters eigenvalues > tol (e.g., 1e-8)
        # so direct division should be mostly safe, but we can add a small epsilon or check.
        denominators <- lambdas_i[-length(lambdas_i)]
        # Replace zero or very small denominators with NA to result in NA gaps
        # This avoids Inf from division and propagates missingness if a lambda is effectively zero.
        denominators[abs(denominators) < .Machine$double.eps^0.5] <- NA
        gaps_i <- (lambdas_i[-1] - lambdas_i[-length(lambdas_i)]) / denominators
        Lambda_original_gaps_list[[i]] <- gaps_i
      } else {
        Lambda_original_gaps_list[[i]] <- numeric(0)
      }
      
      if (num_subjects > 10 && i %% floor(num_subjects/5) == 0 && interactive()) {
          message(sprintf("  Stage 1: Processed subject %d/%d", i, num_subjects))
      }
    }
  }
  message_stage("Stage 1 complete.", interactive_only = TRUE)

  message_stage("Stage 2: Performing iterative refinement (GPA)...", interactive_only = TRUE)
  A_originals_list <- lapply(U_original_list, function(U_orig_subj) {
    if (is.null(U_orig_subj) || nrow(U_orig_subj) == 0 || ncol(U_orig_subj) == 0 || length(unique_anchor_indices) == 0) {
      return(matrix(0, nrow = length(unique_anchor_indices), ncol = spectral_rank_k))
    }
    U_orig_subj[unique_anchor_indices, , drop = FALSE]
  })
  
  gpa_results <- perform_gpa_refinement(A_originals_list, n_refine, spectral_rank_k)
  R_final_list <- gpa_results$R_final_list
  T_anchor_final <- gpa_results$T_anchor_final
  message_stage("Stage 2 complete.", interactive_only = TRUE)

  message_stage("Stage 3: Applying final rotations...", interactive_only = TRUE)
  U_aligned_list <- vector("list", num_subjects)
  if (num_subjects > 0) {
    for (i in 1:num_subjects) {
      U_orig_i <- U_original_list[[i]]
      R_final_i <- R_final_list[[i]]
      
      if (!is.null(U_orig_i) && nrow(U_orig_i) == V_p && ncol(U_orig_i) == spectral_rank_k &&
          !is.null(R_final_i) && nrow(R_final_i) == spectral_rank_k && ncol(R_final_i) == spectral_rank_k) {
        U_aligned_list[[i]] <- U_orig_i %*% R_final_i
      } else { 
        U_aligned_list[[i]] <- matrix(0, nrow = V_p, ncol = spectral_rank_k) 
      }
    }
  }
  message_stage("Stage 3 complete. Core HATSA finished.", interactive_only = TRUE)
  
  hatsa_core_results <- list(
    U_aligned_list = U_aligned_list,
    R_final_list = R_final_list,
    U_original_list = U_original_list,
    Lambda_original_list = Lambda_original_list, 
    Lambda_original_gaps_list = Lambda_original_gaps_list,
    T_anchor_final = T_anchor_final
  )

  parameters <- list(
    k = spectral_rank_k,
    N_subjects = num_subjects,
    V_p = V_p, 
    method = "hatsa_core",
    anchor_indices = unique_anchor_indices,
    k_conn_pos = k_conn_pos,
    k_conn_neg = k_conn_neg,
    n_refine = n_refine,
    use_dtw = use_dtw
  )
  
  projector_object <- hatsa_projector(hatsa_core_results = hatsa_core_results, 
                                      parameters = parameters)
  
  return(projector_object)
}

#' Helper for printing stage messages, optionally only in interactive sessions
#' @param msg Message to print.
#' @param interactive_only Logical, if TRUE, message only if session is interactive.
#' @keywords internal
message_stage <- function(msg, interactive_only = FALSE) {
  if (interactive_only && !interactive()) {
    return(invisible(NULL))
  }
  message(paste(Sys.time(), "-", msg))
}
</file>

<file path="R/hatsa_projector.R">
#' HATSA Projector Object
#'
#' An S3 object of class \code{hatsa_projector} that stores the results of a
#' Harmonized Tensors SVD Alignment (HATSA) analysis. This object inherits from
#' \code{multiblock_biprojector} (from the `multivarious` package) and is designed
#' to integrate HATSA outputs into a common framework for multiblock data analysis.
#'
#' @field v A numeric matrix (V_p x k) representing the mean aligned sketch,
#'   serving as the group-level template or common loadings.
#' @field s A numeric matrix ((N*V_p) x k) of stacked aligned sketches for all
#'   subjects. These are the subject-specific parcel scores in the common space.
#' @field sdev A numeric vector of length k, representing component-wise standard
#'   deviations (or scales). Currently defaults to a vector of 1s.
#' @field preproc A \code{pre_processor} object (from `multivarious`). For HATSA,
#'   this is typically `prep(pass())` as the input data to `run_hatsa_core` is
#'   already processed up to the point of raw time-series per subject.
#' @field block_indices A list defining which rows in the scores matrix `s` belong
#'   to which subject (block).
#' @field R_final_list A list of subject-specific rotation matrices (k x k) used to
#'   align each subject's original sketch to the common space.
#' @field U_original_list A list of subject-specific original (unaligned) sketch
#'   matrices (V_p x k) derived from their parcel-level graph Laplacians.
#' @field Lambda_original_list A list of numeric vectors, where each vector contains
#'   the k original eigenvalues corresponding to the eigenvectors in `U_original_list`
#'   for that subject. These are crucial for Nyström voxel projection.
#' @field Lambda_original_gaps_list A list of numeric vectors. Each vector contains
#'   the k-1 eigengap ratios `(λ_{j+1} - λ_j) / λ_j` for the corresponding subject's
#'   original eigenvalues. Useful for assessing spectral stability.
#' @field T_anchor_final A numeric matrix (V_a x k, where V_a is the number of
#'   anchors) representing the final group anchor template after Procrustes alignment.
#' @field parameters A list containing the input parameters used for the HATSA run
#'   (e.g., `k`, `V_p`, `N_subjects`, `anchor_indices`, `k_conn_pos`, `k_conn_neg`, `n_refine`).
#' @field method A character string, typically "hatsa_core", indicating the method
#'   used to generate the projector.
#' @field U_aligned_list (Internal) A list of subject-specific aligned sketch matrices (V_p x k).
#'   While `s` provides the stacked version, this list might be retained internally from the
#'   `run_hatsa_core` output passed to the constructor. For user access to aligned sketches per subject,
#'   one would typically use `project_block(object, block = i)` or segment `scores(object)`
#'   using `block_indices(object)`.
#'
#' @return A `hatsa_projector` object.
#'
#' @seealso \code{\link{run_hatsa_core}}, \code{\link{predict.hatsa_projector}}, \code{\link{project_voxels.hatsa_projector}}
#' @name hatsa_projector
NULL # This NULL is important for roxygen2 to generate class documentation

#' Constructor for hatsa_projector S3 class
#'
#' Creates a \code{hatsa_projector} object, which stores the results of the
#' HATSA algorithm and is designed to integrate with the \code{multivarious}
#' package, inheriting from \code{multiblock_biprojector}.
#'
#' @param hatsa_core_results A list containing the core outputs from the HATSA
#'   algorithm. Expected elements include:
#'   \itemize{
#'     \item{\code{U_aligned_list}: List of subject-specific aligned sketch matrices (V_p x k).}
#'     \item{\code{R_final_list}: List of subject-specific rotation matrices (k x k).}
#'     \item{\code{U_original_list}: List of subject-specific original sketch matrices (V_p x k).}
#'     \item{\code{Lambda_original_list}: List of subject-specific original eigenvalues (length k).}
#'     \item{\code{Lambda_original_gaps_list}: List of subject-specific eigengap ratios (length k-1).}
#'     \item{\code{T_anchor_final}: The final group anchor template matrix (N_anchors x k).}
#'   }
#' @param parameters A list of parameters used to run HATSA. Expected elements include:
#'   \itemize{
#'     \item{\code{k}: The number of spectral components (rank).}
#'     \item{\code{N_subjects}: The number of subjects.}
#'     \item{\code{V_p}: The number of parcels/vertices per subject.}
#'     \item{\code{method}: A string, typically \code{hatsa_core}.}
#'   }
#'
#' @return An object of class \code{c("hatsa_projector", "multiblock_biprojector", "projector", "list")}.
#'   This object contains:
#'   \itemize{
#'     \item{\code{v}: The group-level loading matrix (mean aligned sketch, V_p x k).}
#'     \item{\code{s}: The stacked scores matrix (concatenated aligned sketches, (N*V_p) x k).}
#'     \item{\code{sdev}: Component standard deviations (defaulted to 1s, length k).}
#'     \item{\code{preproc}: A preprocessing object, set to \code{multivarious::prep(multivarious::pass())}.}
#'     \item{\code{block_indices}: A list defining rows in \code{s} corresponding to each subject block.}
#'     \item{\code{R_final_list}: Stored from input.}
#'     \item{\code{U_original_list}: Stored from input.}
#'     \item{\code{Lambda_original_list}: Stored from input (crucial for voxel projection).}
#'     \item{\code{Lambda_original_gaps_list}: Stored from input.}
#'     \item{\code{T_anchor_final}: Stored from input.}
#'     \item{\code{parameters}: Stored from input.}
#'     \item{\code{method}: Stored from input parameters, typically \code{hatsa_core}.}
#'   }
#' @export
#' @examples
#' # This is a conceptual example, as real data structures are complex.
#' # Assuming hatsa_results and params are populated from run_hatsa_core:
#' # projector_obj <- hatsa_projector(
#' #   hatsa_core_results = list(
#' #     U_aligned_list = replicate(5, matrix(rnorm(100*10), 100, 10), simplify=FALSE),
#' #     R_final_list = replicate(5, diag(10), simplify=FALSE),
#' #     U_original_list = replicate(5, matrix(rnorm(100*10), 100, 10), simplify=FALSE),
#' #     Lambda_original_list = replicate(5, runif(10, 0.1, 1), simplify=FALSE), # example
#' #     Lambda_original_gaps_list = replicate(5, runif(9, 0.05, 0.5), simplify=FALSE), # example
#' #     T_anchor_final = matrix(rnorm(5*10), 5, 10)
#' #   ),
#' #   parameters = list(
#' #     k=10,
#' #     N_subjects=5,
#' #     V_p=100,
#' #     method="hatsa_core"
#' #   )
#' # )
#' # class(projector_obj)
#' # names(projector_obj)
hatsa_projector <- function(hatsa_core_results, parameters) {
  k <- parameters$k
  N_subjects <- parameters$N_subjects
  V_p <- parameters$V_p # Number of parcels/vertices per subject

  if (is.null(k) || is.null(N_subjects) || is.null(V_p)) {
    stop("Essential parameters (k, N_subjects, V_p) missing from input parameters.")
  }

  # Calculate v: Mean aligned sketch (V_p x k)
  # Robust handling of U_aligned_list, similar to task_hatsa_projector
  valid_aligned_sketches <- list()
  if (is.list(hatsa_core_results$U_aligned_list) && length(hatsa_core_results$U_aligned_list) > 0) {
      valid_aligned_sketches <- Filter(Negate(is.null), hatsa_core_results$U_aligned_list)
  }
  
  if (length(valid_aligned_sketches) > 0) {
      # Check consistency of valid sketches before reducing
      first_sketch_dim <- dim(valid_aligned_sketches[[1]])
      if (!all(sapply(valid_aligned_sketches, function(s) identical(dim(s), first_sketch_dim)))) {
          stop("Valid aligned sketches in U_aligned_list have inconsistent dimensions.")
      }
      if (first_sketch_dim[1] != V_p || first_sketch_dim[2] != k) {
          stop(sprintf("Dimensions of valid aligned sketches [%d x %d] do not match V_p=%d, k=%d.", 
                       first_sketch_dim[1], first_sketch_dim[2], V_p, k))
      }
      v_sum <- Reduce("+", valid_aligned_sketches)
      v <- v_sum / length(valid_aligned_sketches)
  } else {
      v <- matrix(0, nrow = V_p, ncol = k) # Default if no valid sketches
  }

  # Calculate s: Stacked aligned sketches ((N_subjects * V_p) x k)
  # Pre-allocate with NAs and fill, ensuring correct dimensions even with NULLs for some subjects
  s_full <- matrix(NA, nrow = N_subjects * V_p, ncol = k)
  if (N_subjects > 0 && V_p > 0) { # only proceed if dimensions make sense
      current_row_start <- 1
      if (is.list(hatsa_core_results$U_aligned_list) && length(hatsa_core_results$U_aligned_list) == N_subjects) {
          for (subj_idx in 1:N_subjects) {
              if (!is.null(hatsa_core_results$U_aligned_list[[subj_idx]])) {
                  # Additional check for individual sketch dimensions before assignment
                  current_sketch <- hatsa_core_results$U_aligned_list[[subj_idx]]
                  if (is.matrix(current_sketch) && nrow(current_sketch) == V_p && ncol(current_sketch) == k) {
                      s_full[current_row_start:(current_row_start + V_p - 1), ] <- current_sketch
                  } else {
                      warning(sprintf("Subject %d sketch has incorrect dimensions or is not a matrix. Filling with NAs in stacked scores.", subj_idx))
                  }
              }
              current_row_start <- current_row_start + V_p
          }
      } else if (N_subjects > 0) {
          warning("U_aligned_list is not a list of length N_subjects. Stacked scores 's' will be all NAs.")
      }
  }
  s <- s_full

  # sdev: Component standard deviations (length k) - default to 1s for HATSA
  sdev <- rep(1, k)

  # preproc: Set to pass() as HATSA alignment is complex and handled by predict
  preproc_obj <- multivarious::prep(multivarious::pass())

  # block_indices: List defining rows in s for each subject block
  block_indices <- if (N_subjects > 0 && V_p > 0) {
                        split(seq_len(N_subjects * V_p), rep(seq_len(N_subjects), each = V_p))
                   } else {
                        list()
                   }

  # Construct the projector object
  obj <- list(
    v = v,
    s = s,
    sdev = sdev,
    preproc = preproc_obj,
    block_indices = block_indices,
    R_final_list = hatsa_core_results$R_final_list,
    U_original_list = hatsa_core_results$U_original_list,
    Lambda_original_list = hatsa_core_results$Lambda_original_list,
    Lambda_original_gaps_list = hatsa_core_results$Lambda_original_gaps_list,
    T_anchor_final = hatsa_core_results$T_anchor_final,
    parameters = parameters,
    method = parameters$method # Store method, e.g., "hatsa_core"
  )

  # Set the class for S3 dispatch
  # Inherits from multiblock_biprojector, which itself inherits from projector
  class(obj) <- c("hatsa_projector", "multiblock_biprojector", "projector", "list")

  return(obj)
}

#' Print method for hatsa_projector objects
#'
#' Provides a concise summary of the hatsa_projector object.
#'
#' @param x A \code{hatsa_projector} object.
#' @param ... Additional arguments passed to print.
#' @export
print.hatsa_projector <- function(x, ...) {
  cat("HATSA Projector Object\n")
  cat("----------------------\n")
  cat("Method: ", x$parameters$method, "\n")
  cat("Number of Subjects (N): ", x$parameters$N_subjects, "\n")
  cat("Number of Parcels (V_p): ", x$parameters$V_p, "\n")
  cat("Number of Components (k): ", x$parameters$k, "\n")
  cat("\nKey components:\n")
  cat("  Mean Aligned Sketch (v): dimensions [", paste(dim(x$v), collapse = " x "), "]\n")
  cat("  Stacked Aligned Sketches (s): dimensions [", paste(dim(x$s), collapse = " x "), "]\n")
  cat("  Component Standard Deviations (sdev): length [", length(x$sdev), "]\n")
  invisible(x)
}

#' Extract coefficients (mean aligned sketch) from a hatsa_projector object
#'
#' @param object A \code{hatsa_projector} object.
#' @param ... Additional arguments (unused).
#' @return The mean aligned sketch matrix (v).
#' @export
coef.hatsa_projector <- function(object, ...) {
  return(object$v)
}

#' Extract scores (stacked aligned sketches) from a hatsa_projector object
#'
#' @param x A \code{hatsa_projector} object.
#' @param ... Additional arguments (unused).
#' @return The stacked aligned sketches matrix (s).
#' @export
scores.hatsa_projector <- function(x, ...) {
  return(x$s)
}

#' Extract component standard deviations from a hatsa_projector object
#'
#' @param x A \code{hatsa_projector} object.
#' @param ... Additional arguments (unused).
#' @return A vector of component standard deviations (sdev).
#' @export
sdev.hatsa_projector <- function(x, ...) {
  return(x$sdev)
}

#' Extract block indices from a hatsa_projector object
#'
#' These indices map rows of the scores matrix (s) to their respective blocks (subjects).
#'
#' @param x A \code{hatsa_projector} object.
#' @param ... Additional arguments (unused).
#' @return A list of block indices.
#' @export
block_indices.hatsa_projector <- function(x, ...) {
  return(x$block_indices)
}

#' Predict method for hatsa_projector objects
#'
#' Projects new subject-level parcel time-series data into the common space
#' defined by a fitted HATSA model.
#'
#' @param object A fitted \code{hatsa_projector} object.
#' @param newdata_list A list of new subject data. Each element of the list should be a
#'   numeric matrix representing parcel time-series data for one subject
#'   (T_i time points x V_p parcels). Parcel count (V_p) must match the original data.
#' @param ... Additional arguments (currently unused, for S3 compatibility).
#'
#' @return A list of aligned spectral sketch matrices (U_aligned_new, V_p x k),
#'   one for each subject in \code{newdata_list}.
#' @export
#' @importFrom stats predict
predict.hatsa_projector <- function(object, newdata_list, ...) {
  if (!is.list(newdata_list)) {
    stop("`newdata_list` must be a list of matrices.")
  }
  if (length(newdata_list) == 0) {
    return(list())
  }

  # Extract necessary parameters from the fitted object
  k <- object$parameters$k
  V_p_model <- object$parameters$V_p # V_p from the model
  anchor_indices <- object$parameters$anchor_indices
  T_anchor_final <- object$T_anchor_final
  k_conn_pos <- object$parameters$k_conn_pos
  k_conn_neg <- object$parameters$k_conn_neg
  use_dtw_model <- object$parameters$use_dtw

  aligned_sketches_new_list <- vector("list", length(newdata_list))
  for (i in seq_along(newdata_list)) {
    newdata_item <- newdata_list[[i]]
    if (!is.matrix(newdata_item)) {
      warning(sprintf("Item %d in newdata_list is not a matrix. Skipping.", i))
      aligned_sketches_new_list[i] <- list(NULL)
      next
    }
    if (ncol(newdata_item) != V_p_model) {
      warning(sprintf("Item %d in newdata_list has %d parcels, but model expects %d. Skipping.", 
                      i, ncol(newdata_item), V_p_model))
      aligned_sketches_new_list[i] <- list(NULL)
      next
    }

    # Generate parcel names if not present, consistent with run_hatsa_core logic
    current_pnames <- colnames(newdata_item)
    if (is.null(current_pnames) || length(current_pnames) != V_p_model) {
      current_pnames <- paste0("P", 1:V_p_model) 
    }

    # 1. Compute subject connectivity graph
    W_conn_new <- compute_subject_connectivity_graph_sparse(newdata_item, 
                                                            current_pnames, 
                                                            k_conn_pos, 
                                                            k_conn_neg, 
                                                            use_dtw = use_dtw_model)
    # 2. Compute graph Laplacian
    L_conn_new <- compute_graph_laplacian_sparse(W_conn_new)

    # 3. Compute original spectral sketch (eigenvectors)
    # compute_spectral_sketch_sparse returns a list(vectors=U, values=Lambda)
    sketch_result_new <- compute_spectral_sketch_sparse(L_conn_new, k)
    U_orig_new <- sketch_result_new$vectors

    if (is.null(U_orig_new) || nrow(U_orig_new) != V_p_model || ncol(U_orig_new) != k) {
        warning(sprintf("Subject %d: Original sketch computation failed or has incorrect dimensions. Skipping.", i))
        aligned_sketches_new_list[[i]] <- matrix(NA, nrow = V_p_model, ncol = k)
        next
    }

    # 4. Extract anchors from U_orig_new
    if (length(anchor_indices) == 0 && k > 0) {
        warning(sprintf("Subject %d: No anchor indices defined in the model, but k > 0. Cannot perform alignment. Returning original sketch.", i))
        aligned_sketches_new_list[[i]] <- U_orig_new # Or handle as error
        next
    } else if (k == 0) {
        aligned_sketches_new_list[[i]] <- U_orig_new # k=0 sketch
        next
    }
    A_orig_new <- U_orig_new[anchor_indices, , drop = FALSE]

    # 5. Find rotation R_new by aligning A_orig_new to stored T_anchor_final
    # Ensure T_anchor_final is correctly dimensioned
    if (is.null(T_anchor_final) || nrow(T_anchor_final) != length(anchor_indices) || ncol(T_anchor_final) != k) {
        warning(sprintf("Subject %d: T_anchor_final in the model is invalid. Cannot perform alignment. Returning original sketch.", i))
        aligned_sketches_new_list[[i]] <- U_orig_new
        next
    }
    R_new <- solve_procrustes_rotation(A_orig_new, T_anchor_final)

    # 6. Compute aligned sketch U_aligned_new
    U_aligned_new <- U_orig_new %*% R_new
    
    aligned_sketches_new_list[[i]] <- U_aligned_new
  }

  # Directly return
  return(aligned_sketches_new_list)
}

#' Project a specific block (subject) using a hatsa_projector object
#'
#' Retrieves the aligned spectral sketch for a specified subject. If new data
#' for that subject is provided, it projects that new data. Otherwise, it extracts
#' the stored aligned sketch from the original analysis.
#'
#' @param object A fitted \code{hatsa_projector} object.
#' @param newdata Optional. A single numeric matrix of parcel time-series data
#'   (T_i x V_p) for the subject specified by \code{block}. If NULL, the stored
#'   sketch for the block is returned. If provided, this new data is projected.
#' @param block An integer, the index of the subject (block) to project.
#' @param ... Additional arguments passed to \code{predict.hatsa_projector} if
#'   \code{newdata} is provided.
#'
#' @return A matrix (V_p x k) representing the aligned spectral sketch for the
#'   specified subject/block. Returns NULL or raises an error if the block is invalid
#'   or data is inappropriate.
#' @importFrom multivarious project_block
#' @export
project_block.hatsa_projector <- function(object, newdata = NULL, block, ...) {
  V_p <- object$parameters$V_p
  k <- object$parameters$k

  if (is.null(newdata)) {
    # Retrieve stored sketch for the block
    if (!is.numeric(block) || length(block) != 1 || block < 1 || block > object$parameters$N_subjects) {
      stop(sprintf("Invalid block index. Must be an integer between 1 and %d (N_subjects).", object$parameters$N_subjects))
    }
    if (is.null(object$s) || is.null(object$block_indices) || length(object$block_indices) < block) {
        stop("Stored scores 's' or 'block_indices' are missing or incomplete in the object.")
    }

    block_idx_rows <- object$block_indices[[block]]
    if (is.null(block_idx_rows) || length(block_idx_rows) != V_p) {
        stop(sprintf("Block indices for block %d are invalid or do not match V_p.", block))
    }
    
    # Ensure scores matrix has enough rows and correct number of columns
    if (nrow(object$s) < max(block_idx_rows) || ncol(object$s) != k) {
        stop("Scores matrix 's' has incorrect dimensions for the requested block or k.")
    }

    subject_sketch <- object$s[block_idx_rows, , drop = FALSE]
    
    # Reshape to V_p x k if it was extracted as a vector (e.g. k=1)
    # Although drop=FALSE should prevent this for matrices, good to be robust.
    # However, with block_indices, object$s is already (N*V_p) x k, so subsetting rows gives (V_p) x k.
    # So, explicit dim check might be more direct.
    if (nrow(subject_sketch) != V_p || ncol(subject_sketch) != k) {
        stop(sprintf("Extracted sketch for block %d has dimensions [%s] but expected [%d x %d].",
                     block, paste(dim(subject_sketch), collapse="x"), V_p, k))
    }
    return(subject_sketch)

  } else {
    # Project new data for the specified block (newdata is for this one block)
    if (!is.matrix(newdata)) {
      stop("`newdata` must be a numeric matrix (time-series data for one subject).")
    }
    if (ncol(newdata) != V_p) {
      stop(sprintf("`newdata` has %d parcels, but model expects %d.", ncol(newdata), V_p))
    }

    # predict.hatsa_projector expects a list of matrices
    # It returns a list of aligned sketches.
    aligned_sketch_list <- tryCatch(
      predict.hatsa_projector(object, newdata_list = list(newdata), ...),
      error = function(e) {
        warning(sprintf("Prediction for the new data block failed: %s", e$message))
        list(NULL)
      }
    )
    
    if (length(aligned_sketch_list) == 1 && !is.null(aligned_sketch_list[[1]])) {
      return(aligned_sketch_list[[1]])
    } else {
      warning("Prediction for the new data block failed or returned an unexpected result.")
      # Return an empty or NA matrix of appropriate size
      return(matrix(NA, nrow = V_p, ncol = k))
    }
  }
}

#' Summary method for hatsa_projector objects
#'
#' Provides a detailed summary of the hatsa_projector object, including
#' mean anchor alignment error for the original subjects.
#'
#' @param object A \code{hatsa_projector} object.
#' @param ... Additional arguments (unused).
#' @param compute_riemannian_dispersion Logical, if TRUE, computes and includes
#'   Riemannian dispersion metrics for SPD representations (default: FALSE).
#' @param riemannian_dispersion_type Character string, type of SPD representation
#'   for dispersion (e.g., "cov_coeffs"). Passed to `riemannian_dispersion_spd`.
#' @param riemannian_dispersion_options A list of additional arguments passed to
#'   `riemannian_dispersion_spd` (e.g., for `use_geometric_median`, `spd_regularize_epsilon`, `verbose`).
#' @return A list object of class \code{summary.hatsa_projector} containing
#'   summary statistics.
#' @export
#' @importFrom stats predict sd median na.omit
#' @importFrom expm logm expm sqrtm
summary.hatsa_projector <- function(object, ..., 
                                    compute_riemannian_dispersion = FALSE,
                                    riemannian_dispersion_type = "cov_coeffs",
                                    riemannian_dispersion_options = list()) {
  # Basic information
  summary_info <- list(
    method = object$parameters$method,
    N_subjects = object$parameters$N_subjects,
    V_p = object$parameters$V_p,
    k = object$parameters$k,
    num_anchors = length(object$parameters$anchor_indices)
  )

  # Calculate mean anchor alignment error
  if (object$parameters$N_subjects > 0 && object$parameters$k > 0 && summary_info$num_anchors > 0) {
    anchor_errors <- numeric(object$parameters$N_subjects)
    U_original_list <- object$U_original_list
    R_final_list <- object$R_final_list
    T_anchor_final <- object$T_anchor_final
    anchor_indices <- object$parameters$anchor_indices

    all_data_valid_for_error_calc <- TRUE
    if (is.null(U_original_list) || length(U_original_list) != object$parameters$N_subjects ||
        is.null(R_final_list) || length(R_final_list) != object$parameters$N_subjects ||
        is.null(T_anchor_final) || is.null(anchor_indices)) {
      all_data_valid_for_error_calc <- FALSE
    }

    if (all_data_valid_for_error_calc) {
      for (i in 1:object$parameters$N_subjects) {
        U_orig_i <- U_original_list[[i]]
        R_i <- R_final_list[[i]]

        if (is.null(U_orig_i) || is.null(R_i) || 
            nrow(U_orig_i) != object$parameters$V_p || ncol(U_orig_i) != object$parameters$k ||
            nrow(R_i) != object$parameters$k || ncol(R_i) != object$parameters$k || 
            nrow(T_anchor_final) != summary_info$num_anchors || ncol(T_anchor_final) != object$parameters$k) {
          anchor_errors[i] <- NA # Mark as NA if data is inconsistent
          next
        }
        
        A_orig_i_anchors <- U_orig_i[anchor_indices, , drop = FALSE]
        A_aligned_i_anchors <- A_orig_i_anchors %*% R_i
        
        # Frobenius norm of the difference
        # Ensure dimensions match for subtraction
        if (nrow(A_aligned_i_anchors) == nrow(T_anchor_final) && ncol(A_aligned_i_anchors) == ncol(T_anchor_final)) {
            error_matrix <- A_aligned_i_anchors - T_anchor_final
            anchor_errors[i] <- norm(error_matrix, type = "F")
        } else {
            anchor_errors[i] <- NA # Inconsistent dimensions for error calculation
        }
      }
      summary_info$mean_anchor_alignment_error <- mean(anchor_errors, na.rm = TRUE)
      summary_info$anchor_alignment_errors_per_subject <- anchor_errors
    } else {
      summary_info$mean_anchor_alignment_error <- NA
      summary_info$anchor_alignment_errors_per_subject <- rep(NA, object$parameters$N_subjects)
      warning("Could not calculate anchor alignment error due to missing or incomplete data in the object.")
    }
  } else {
    summary_info$mean_anchor_alignment_error <- NA
    summary_info$anchor_alignment_errors_per_subject <- rep(NA, object$parameters$N_subjects)
  }

  # --- HMET-001: Rotation Dispersion (sigma_R) --- 
  if (object$parameters$N_subjects > 0 && object$parameters$k > 0 && !is.null(object$R_final_list)) {
    k_dim <- object$parameters$k
    Rs <- object$R_final_list
    # Filter out any NULLs from Rs, which might occur if a subject had issues
    Rs_valid <- Filter(Negate(is.null), Rs)
    Rs_valid <- Filter(function(R_mat) is.matrix(R_mat) && all(dim(R_mat) == c(k_dim, k_dim)), Rs_valid)
    N_valid_subj_for_rot <- length(Rs_valid)

    if (N_valid_subj_for_rot > 1) {
      # Helper for geodesic distance d(R1, R2) = ||logm(R1^T R2)||_F / sqrt(2)
      geodesic_dist_so_k <- function(R1, R2) {
        if (!requireNamespace("expm", quietly = TRUE)) {
          warning("Package 'expm' needed for geodesic_dist_so_k.")
          return(NA)
        }
        R1t_R2 <- t(R1) %*% R2
        log_R1t_R2 <- tryCatch(expm::logm(R1t_R2, method="Higham08.b"), error = function(e) {
          warning("Error in expm::logm: ", e$message)
          return(matrix(0, k_dim, k_dim))
        })
        return(norm(log_R1t_R2, type = "F") / sqrt(2))
      }

      # Use tryCatch to safely attempt to compute Frechet mean
      R_bar <- tryCatch({
        # Try to use frechet_mean_so_k function if available
        frechet_mean_so_k(R_list = Rs_valid, k_dim = k_dim, max_iter = 50, tol = 1e-7, project_to_SOk = TRUE)
      }, error = function(e) {
        # Fallback to identity matrix and issue warning
        warning("Failed to compute Frechet mean rotation: ", e$message, ". Using identity matrix as fallback.")
        diag(k_dim)
      })
      
      summary_info$R_frechet_mean <- R_bar # Store the mean rotation
      
      geodesic_distances_to_mean <- sapply(Rs_valid, function(R_i) geodesic_dist_so_k(R_i, R_bar))
      
      summary_info$rotation_dispersion_mean_geo_dist <- mean(geodesic_distances_to_mean, na.rm = TRUE)
      summary_info$rotation_dispersion_sd_geo_dist <- sd(geodesic_distances_to_mean, na.rm = TRUE)
      summary_info$rotation_geodesic_distances <- geodesic_distances_to_mean
    } else {
      summary_info$R_frechet_mean <- if (N_valid_subj_for_rot == 1 && k_dim > 0) Rs_valid[[1]] else if (k_dim > 0) diag(k_dim) else matrix(0,0,0)
      summary_info$rotation_dispersion_mean_geo_dist <- NA
      summary_info$rotation_dispersion_sd_geo_dist <- NA
      summary_info$rotation_geodesic_distances <- numeric(0)
    }
  } else {
    summary_info$R_frechet_mean <- if(object$parameters$k > 0) diag(object$parameters$k) else matrix(0,0,0)
    summary_info$rotation_dispersion_mean_geo_dist <- NA
    summary_info$rotation_dispersion_sd_geo_dist <- NA
    summary_info$rotation_geodesic_distances <- numeric(0)
  }

  # --- HMET-001: Variance Explained by k Components (sum of selected eigenvalues) ---
  if (object$parameters$N_subjects > 0 && object$parameters$k > 0 && !is.null(object$Lambda_original_list)) {
    lambdas_orig_valid <- Filter(Negate(is.null), object$Lambda_original_list)
    lambdas_orig_valid <- Filter(function(l) is.numeric(l) && length(l) == object$parameters$k, lambdas_orig_valid)
    
    if (length(lambdas_orig_valid) > 0) {
      sum_k_lambdas_per_subject <- sapply(lambdas_orig_valid, sum)
      summary_info$mean_sum_k_selected_eigenvalues <- mean(sum_k_lambdas_per_subject, na.rm = TRUE)
      summary_info$sd_sum_k_selected_eigenvalues <- sd(sum_k_lambdas_per_subject, na.rm = TRUE)
      summary_info$sum_k_selected_eigenvalues_per_subject <- sum_k_lambdas_per_subject
    } else {
      summary_info$mean_sum_k_selected_eigenvalues <- NA
      summary_info$sd_sum_k_selected_eigenvalues <- NA
      summary_info$sum_k_selected_eigenvalues_per_subject <- numeric(0)
    }
  } else {
    summary_info$mean_sum_k_selected_eigenvalues <- NA
    summary_info$sd_sum_k_selected_eigenvalues <- NA
    summary_info$sum_k_selected_eigenvalues_per_subject <- numeric(0)
  }

  # --- HMET-001: Eigengap Ratios ---
  if (object$parameters$N_subjects > 0 && object$parameters$k > 1 && !is.null(object$Lambda_original_gaps_list)) {
    gaps_list_valid <- Filter(Negate(is.null), object$Lambda_original_gaps_list)
    # Each element of gaps_list_valid should be a numeric vector of length k-1
    gaps_list_valid <- Filter(function(g) is.numeric(g) && length(g) == (object$parameters$k - 1), gaps_list_valid)

    if (length(gaps_list_valid) > 0) {
      # Focus on the (k-1)th gap: (lambda_k - lambda_{k-1}) / lambda_{k-1}
      # This is the last element of each vector in gaps_list_valid
      k_minus_1_gaps <- sapply(gaps_list_valid, function(gaps_subj) gaps_subj[object$parameters$k - 1])
      
      summary_info$median_k_minus_1_eigengap_ratio <- median(k_minus_1_gaps, na.rm = TRUE)
      summary_info$min_k_minus_1_eigengap_ratio <- if(any(!is.na(k_minus_1_gaps))) min(k_minus_1_gaps, na.rm = TRUE) else NA
      summary_info$max_k_minus_1_eigengap_ratio <- if(any(!is.na(k_minus_1_gaps))) max(k_minus_1_gaps, na.rm = TRUE) else NA
      summary_info$k_minus_1_eigengap_ratios_per_subject <- k_minus_1_gaps
    } else {
      summary_info$median_k_minus_1_eigengap_ratio <- NA
      summary_info$min_k_minus_1_eigengap_ratio <- NA
      summary_info$max_k_minus_1_eigengap_ratio <- NA
      summary_info$k_minus_1_eigengap_ratios_per_subject <- numeric(0)
    }
  } else {
    summary_info$median_k_minus_1_eigengap_ratio <- NA
    summary_info$min_k_minus_1_eigengap_ratio <- NA
    summary_info$max_k_minus_1_eigengap_ratio <- NA
    summary_info$k_minus_1_eigengap_ratios_per_subject <- numeric(0)
  }

  # --- RGEOM-006: Optionally compute Riemannian Dispersion ---
  if (compute_riemannian_dispersion) {
    # Use tryCatch to safely attempt to compute Riemannian dispersion
    verbose <- riemannian_dispersion_options$verbose %||% FALSE
    if (verbose && interactive()) { 
        message(sprintf("Computing Riemannian dispersion for type: %s...", riemannian_dispersion_type))
    }
    
    # Prepare arguments for riemannian_dispersion_spd
    disp_args <- list(
      object = object,
      type = riemannian_dispersion_type
    )
    
    # Add options from riemannian_dispersion_options
    if (length(riemannian_dispersion_options) > 0) {
        for(opt_name in names(riemannian_dispersion_options)) {
            disp_args[[opt_name]] <- riemannian_dispersion_options[[opt_name]]
        }
    }
    
    dispersion_results <- tryCatch({
      # Try to call riemannian_dispersion_spd function
      do.call(riemannian_dispersion_spd, disp_args)
    }, error = function(e) {
      # Handle missing function or other errors
      warning("Failed to compute Riemannian dispersion: ", e$message)
      NULL
    })
    
    if (!is.null(dispersion_results)) {
      summary_info$riemannian_dispersion_type_used <- riemannian_dispersion_type
      summary_info$riemannian_mean_dispersion <- dispersion_results$mean_dispersion
      summary_info$riemannian_median_dispersion <- dispersion_results$median_dispersion
      summary_info$riemannian_num_valid_subjects_for_disp <- dispersion_results$num_valid_subjects
    } else {
      summary_info$riemannian_dispersion_type_used <- riemannian_dispersion_type
      summary_info$riemannian_mean_dispersion <- NA_real_
      summary_info$riemannian_median_dispersion <- NA_real_
      summary_info$riemannian_num_valid_subjects_for_disp <- 0
    }
  } else {
    # Ensure these fields are NULL if not computed, for consistent print checks
    summary_info$riemannian_dispersion_type_used <- NULL
    summary_info$riemannian_mean_dispersion <- NULL
    summary_info$riemannian_median_dispersion <- NULL
    summary_info$riemannian_num_valid_subjects_for_disp <- NULL
  }

  class(summary_info) <- "summary.hatsa_projector"
  return(summary_info)
}

# Define %||% operator if it doesn't exist
`%||%` <- function(x, y) if (is.null(x)) y else x

#' Print method for summary.hatsa_projector objects
#'
#' @param x A \code{summary.hatsa_projector} object.
#' @param ... Additional arguments (unused).
#' @export
print.summary.hatsa_projector <- function(x, ...) {
  cat("HATSA Projector Summary\n")
  cat("-----------------------\n")
  cat("Method: ", x$method, "\n")
  cat("Number of Subjects (N): ", x$N_subjects, "\n")
  cat("Number of Parcels (V_p): ", x$V_p, "\n")
  cat("Number of Components (k): ", x$k, "\n")
  cat("Number of Anchors: ", x$num_anchors, "\n")
  cat("\n")
  if (!is.null(x$mean_anchor_alignment_error) && !is.na(x$mean_anchor_alignment_error)) {
    cat("Mean Anchor Alignment Error (Frobenius Norm): ", sprintf("%.4f", x$mean_anchor_alignment_error), "\n")
  } else if (x$k > 0 && x$num_anchors > 0 && x$N_subjects > 0) {
    cat("Mean Anchor Alignment Error: Not calculable (data missing or inconsistent)\n")
  }

  # Display Rotation Dispersion
  if (!is.null(x$rotation_dispersion_mean_geo_dist) && !is.na(x$rotation_dispersion_mean_geo_dist)) {
    cat("Rotation Dispersion (Mean Geodesic Distance to R_bar): ", sprintf("%.4f", x$rotation_dispersion_mean_geo_dist), "\n")
    cat("Rotation Dispersion (SD Geodesic Distance to R_bar): ", sprintf("%.4f", x$rotation_dispersion_sd_geo_dist), "\n")
  }

  # Display Variance Explained (Sum of k eigenvalues)
  if (!is.null(x$mean_sum_k_selected_eigenvalues) && !is.na(x$mean_sum_k_selected_eigenvalues)) {
    cat("Mean Sum of k Selected Eigenvalues (Energy): ", sprintf("%.4f", x$mean_sum_k_selected_eigenvalues), "\n")
    cat("SD Sum of k Selected Eigenvalues: ", sprintf("%.4f", x$sd_sum_k_selected_eigenvalues), "\n")
  }

  # Display Eigengap Ratios
  if (!is.null(x$median_k_minus_1_eigengap_ratio) && !is.na(x$median_k_minus_1_eigengap_ratio)) {
    cat(sprintf("Median (λ_k - λ_{k-1})/λ_{k-1} Ratio: %.4f (Min: %.4f, Max: %.4f)\n", 
                x$median_k_minus_1_eigengap_ratio, 
                x$min_k_minus_1_eigengap_ratio, 
                x$max_k_minus_1_eigengap_ratio))
  }

  # Display Riemannian Dispersion if computed
  if (!is.null(x$riemannian_dispersion_type_used)) {
    cat("\n--- Riemannian Dispersion ---\n")
    cat("SPD Representation Type: ", x$riemannian_dispersion_type_used, "\n")
    if (!is.na(x$riemannian_mean_dispersion)) {
      cat("  Mean Riemannian Dispersion: ", sprintf("%.4f", x$riemannian_mean_dispersion), "\n")
      cat("  Median Riemannian Dispersion: ", sprintf("%.4f", x$riemannian_median_dispersion), "\n")
      cat("  (Based on ", x$riemannian_num_valid_subjects_for_disp, " valid subjects)\n")
    } else {
      cat("  Riemannian Dispersion: Not calculable or computation failed.\n")
    }
  }

  invisible(x)
}

# --- HMET-002: Reconstruction Error S3 Method ---

#' Calculate Reconstruction Error for HATSA Projector Objects
#'
#' Computes various types of reconstruction errors based on a fitted HATSA model.
#' This helps quantify alignment quality, sanity check model components, or assess
#' how well non-anchor information is captured.
#'
#' @param object A fitted \code{hatsa_projector} object.
#' @param type A character string specifying the type of reconstruction error to compute.
#'   One of:
#'   \itemize{
#'     \item{\code{"anchors"} (Default): Error between subject-specific aligned anchor sketches
#'           and the group anchor template (`T_anchor_final`).}
#'     \item{\code{"all_parcels"}: Error between original subject sketches (`U_original_list`)
#'           and sketches reconstructed from aligned versions via inverse rotation.
#'           (Sanity check, should be near zero if rotations are orthogonal).}
#'     \item{\code{"non_anchors"}: Error in predicting non-anchor parcel sketches from
#'           anchor parcel sketches in the aligned space (Anchor Residual `ε̄`).}
#'   }
#' @param ... Additional arguments, potentially passed to internal helper functions
#'   (e.g., for non-anchor prediction methods).
#'
#' @return A named list containing reconstruction error metrics. The structure depends
#'   on the `type` requested, typically including `mean_error` and `per_subject_error`.
#'   For `type="non_anchors"`, it may also include `per_parcel_error` if implemented.
#' @export
reconstruction_error <- function(object, type = "anchors", ...) {
  UseMethod("reconstruction_error")
}

#' @rdname reconstruction_error
#' @export
reconstruction_error.hatsa_projector <- function(object, type = "anchors", ...) {
  # Parameter extraction from object
  N_subjects <- object$parameters$N_subjects
  V_p <- object$parameters$V_p
  k <- object$parameters$k
  anchor_indices <- object$parameters$anchor_indices
  num_anchors <- length(anchor_indices)
  
  U_original_list <- object$U_original_list
  R_final_list <- object$R_final_list
  T_anchor_final <- object$T_anchor_final

  # Initialize results list
  results <- list(
    type = type,
    mean_error = NA,
    per_subject_error = rep(NA, N_subjects),
    notes = character(0)
  )

  # --- Helper to get U_aligned_list (reconstruct from s if not directly stored) ---
  .get_U_aligned_list <- function(obj) {
    # Check if U_aligned_list was explicitly stored by constructor (current constructor does not)
    # if (!is.null(obj$._cache$U_aligned_list)) return(obj$._cache$U_aligned_list)
    # if (!is.null(obj$U_aligned_list_stored)) return(obj$U_aligned_list_stored) # If we decide to store it

    # Reconstruct from obj$s and obj$block_indices
    if (is.null(obj$s) || is.null(obj$block_indices) || obj$parameters$N_subjects == 0) {
      warning("Cannot reconstruct U_aligned_list: object$s or object$block_indices missing, or N_subjects is 0.")
      return(vector("list", obj$parameters$N_subjects)) # List of NULLs
    }
    
    U_aligned_list_recon <- vector("list", obj$parameters$N_subjects)
    for (i in 1:obj$parameters$N_subjects) {
      if (length(obj$block_indices) >= i && !is.null(obj$block_indices[[i]])) {
        rows_subj_i <- obj$block_indices[[i]]
        if (max(rows_subj_i) <= nrow(obj$s) && ncol(obj$s) == obj$parameters$k) {
          U_aligned_list_recon[[i]] <- obj$s[rows_subj_i, , drop = FALSE]
        } else {
          U_aligned_list_recon[[i]] <- NULL # Mark as NULL if dimensions mismatch
        }
      } else {
        U_aligned_list_recon[[i]] <- NULL
      }
    }
    # Optional: Cache this reconstructed list if it were to be used multiple times within this function call
    # obj$._cache$U_aligned_list <- U_aligned_list_recon 
    return(U_aligned_list_recon)
  }

  # Type dispatch
  if (type == "anchors") {
    if (N_subjects == 0 || k == 0 || num_anchors == 0) {
      results$notes <- "Skipped: N_subjects, k, or num_anchors is 0."
      return(results)
    }
    if (is.null(U_original_list) || is.null(R_final_list) || is.null(T_anchor_final)) {
      results$notes <- "Skipped: U_original_list, R_final_list, or T_anchor_final is missing."
      return(results)
    }

    subject_errors <- numeric(N_subjects)
    for (i in 1:N_subjects) {
      U_orig_i <- U_original_list[[i]]
      R_i <- R_final_list[[i]]

      if (is.null(U_orig_i) || !is.matrix(U_orig_i) || nrow(U_orig_i) != V_p || ncol(U_orig_i) != k ||
          is.null(R_i) || !is.matrix(R_i) || any(dim(R_i) != c(k, k)) || 
          !is.matrix(T_anchor_final) || nrow(T_anchor_final) != num_anchors || ncol(T_anchor_final) != k) {
        subject_errors[i] <- NA
        next
      }
      
      A_orig_i_anchors <- U_orig_i[anchor_indices, , drop = FALSE]
      if(any(is.na(A_orig_i_anchors))) {subject_errors[i] <- NA; next} # Check after subsetting

      A_aligned_i_anchors <- A_orig_i_anchors %*% R_i
      
      if (nrow(A_aligned_i_anchors) == num_anchors && ncol(A_aligned_i_anchors) == k) {
        error_matrix <- A_aligned_i_anchors - T_anchor_final
        subject_errors[i] <- norm(error_matrix, type = "F")
      } else {
        subject_errors[i] <- NA
      }
    }
    results$per_subject_error <- subject_errors
    results$mean_error <- mean(subject_errors, na.rm = TRUE)
    
  } else if (type == "all_parcels") {
    # To be implemented
    results$notes <- "Type 'all_parcels' not yet fully implemented."
    # Logic: U_reconstructed_i = U_aligned_i %*% t(R_i)
    # Error: ||U_orig_i - U_reconstructed_i||_F
    # U_aligned_list_local <- .get_U_aligned_list(object)

  } else if (type == "non_anchors") {
    # To be implemented
    results$notes <- "Type 'non_anchors' not yet fully implemented."
    # Logic: Predict U_NA_aligned_i from U_A_aligned_i
    # U_aligned_list_local <- .get_U_aligned_list(object)

  } else {
    stop(sprintf("Unknown reconstruction error type: '%s'. Must be 'anchors', 'all_parcels', or 'non_anchors'.", type))
  }

  return(results)
}
</file>

<file path="R/hatsa_qc_plots.R">
#' HATSA Quality Control and Stability Plots
#'
#' This file contains functions for generating QC plots, such as k-stability plots,
#' for HATSA results.
#'
#' @importFrom ggplot2 ggplot aes geom_line geom_point facet_wrap theme_bw labs scale_x_continuous ggtitle
#' @importFrom stats sd na.omit

# Ensure stats is available for sd, na.omit if not base
# Ensure ggplot2 is available for plotting functions

#' Plot HATSA k-Stability Metrics
#'
#' Generates plots of stability metrics versus the number of components (k)
#' from a list of HATSA results obtained with varying k. This helps in selecting
#' an optimal k.
#'
#' @param projector_list_over_k A list of \code{hatsa_projector} objects.
#'   Each object should be the result of a HATSA run with a different 'k'.
#'   The list can be named with k values, or k will be extracted from each object.
#' @param metrics_to_plot A character vector specifying which metrics to plot.
#'   Possible values are "Hk" (Alignment Homogeneity based on Riemannian dispersion
#'   of covariance of aligned coefficients) and/or "CV_eigen_cov" (Mean Coefficient
#'   of Variation of eigenvalues of covariance of aligned coefficients).
#'   Default is \code{c("Hk", "CV_eigen_cov")}.
#' @param Hk_dispersion_stat Character, either "mean" (default) or "median".
#'   Specifies which statistic from \code{riemannian_dispersion_spd} to use for Hk.
#' @param spd_cov_coeffs_options A list of additional arguments to pass to
#'   \code{hatsa::get_spd_representations(..., type = "cov_coeffs")}.
#'   For example, \code{list(spd_regularize_epsilon = 1e-6)}.
#' @param Hk_options A list of additional arguments to pass to
#'   \code{hatsa::riemannian_dispersion_spd(...)}.
#'   For example, \code{list(tangent_metric = "logeuclidean", use_geometric_median = FALSE)}.
#'   Note that `use_geometric_median` will be overridden by `Hk_dispersion_stat == "median"`.
#' @param verbose Logical, if TRUE, prints progress messages. Default is FALSE.
#'
#' @return A \code{ggplot} object containing the stability plots, or NULL if
#'   no valid metrics can be computed.
#'
#' @export
#' @examples
#' # Conceptual example:
#' # Assume projector_k3, projector_k4, projector_k5 are hatsa_projector objects
#' # k_list <- list("3" = projector_k3, "4" = projector_k4, "5" = projector_k5)
#' # plot_k_stability_hatsa(k_list)
#'
#' # To run if example data were available:
#' # if (requireNamespace("ggplot2", quietly = TRUE) &&
#' #     exists("generate_synthetic_hatsa_output_list")) {
#' #   example_projector_list <- generate_synthetic_hatsa_output_list(
#' #      k_values = c(3, 4, 5, 6), N_subjects = 5, V_p = 20
#' #   )
#' #   plot_k_stability_hatsa(example_projector_list)
#' # }
plot_k_stability_hatsa <- function(projector_list_over_k,
                                   metrics_to_plot = c("Hk", "CV_eigen_cov"),
                                   Hk_dispersion_stat = "mean",
                                   spd_cov_coeffs_options = list(),
                                   Hk_options = list(),
                                   verbose = FALSE) {

  if (!requireNamespace("ggplot2", quietly = TRUE)) {
    stop("Package 'ggplot2' is required for this function.")
  }
  if (!is.list(projector_list_over_k) || length(projector_list_over_k) == 0) {
    stop("`projector_list_over_k` must be a non-empty list of hatsa_projector objects.")
  }
  if (!Hk_dispersion_stat %in% c("mean", "median")) {
    stop("`Hk_dispersion_stat` must be either 'mean' or 'median'.")
  }

  results_df_list <- list()

  for (i in seq_along(projector_list_over_k)) {
    proj_obj <- projector_list_over_k[[i]]
    k_name <- names(projector_list_over_k)[i]

    if (!inherits(proj_obj, "hatsa_projector")) {
      warning(sprintf("Item %d in projector_list_over_k is not a hatsa_projector object. Skipping.", i))
      next
    }
    current_k <- proj_obj$parameters$k
    if (is.null(current_k)) {
      warning(sprintf("Could not determine k for item %d. Skipping.", i))
      next
    }
    if (verbose) message_stage(sprintf("Processing for k = %d...", current_k))

    # --- Calculate Hk (Alignment Homogeneity) ---
    if ("Hk" %in% metrics_to_plot) {
      if (verbose) message_stage("  Calculating Hk (Riemannian Dispersion)...")
      
      current_Hk_options <- Hk_options
      current_Hk_options$object <- proj_obj
      current_Hk_options$type <- "cov_coeffs" # Hk is specifically for cov_coeffs
      current_Hk_options$use_geometric_median <- (Hk_dispersion_stat == "median")
      # Merge get_spd_representations options if needed by riemannian_dispersion_spd indirectly
      # `riemannian_dispersion_spd` itself calls `get_spd_representations`
      # So, spd_cov_coeffs_options should be part of Hk_options if they affect SPD matrices
      # for dispersion (e.g. k_conn_params if type was fc_conn, but here it's cov_coeffs)
      # For cov_coeffs, only spd_regularize_epsilon from spd_cov_coeffs_options might be relevant via `...`
      # to `get_spd_representations` call within `riemannian_dispersion_spd`
      # It might be cleaner if riemannian_dispersion_spd explicitly takes spd_options
      if (!is.null(spd_cov_coeffs_options$spd_regularize_epsilon)) {
           current_Hk_options$spd_regularize_epsilon <- spd_cov_coeffs_options$spd_regularize_epsilon
      }


      disp_results <- tryCatch(
        do.call(hatsa::riemannian_dispersion_spd, current_Hk_options),
        error = function(e) {
          warning(sprintf("Hk calculation failed for k = %d: %s", current_k, e$message))
          NULL
        }
      )
      
      Hk_value <- NA_real_
      if (!is.null(disp_results)) {
        Hk_value <- disp_results[[paste0(Hk_dispersion_stat, "_dispersion")]]
      }
      results_df_list[[length(results_df_list) + 1]] <- data.frame(
        k = current_k,
        metric_name = paste0("Hk (", Hk_dispersion_stat, " disp.)"),
        value = Hk_value
      )
    }

    # --- Calculate CV_eigen_cov (Mean CV of eigenvalues of Cov_coeffs) ---
    if ("CV_eigen_cov" %in% metrics_to_plot) {
      if (verbose) message_stage("  Calculating CV_eigen_cov...")
      
      current_spd_options <- spd_cov_coeffs_options
      current_spd_options$object <- proj_obj
      current_spd_options$type <- "cov_coeffs"
      
      spd_list <- tryCatch(
        do.call(hatsa::get_spd_representations, current_spd_options),
        error = function(e) {
          warning(sprintf("Failed to get SPD representations for k = %d for CV_eigen_cov: %s", current_k, e$message))
          NULL
        }
      )

      mean_cv_value <- NA_real_
      if (!is.null(spd_list) && length(Filter(Negate(is.null), spd_list)) > 0) {
        valid_spd_list <- Filter(Negate(is.null), spd_list)
        
        cvs_subject <- sapply(valid_spd_list, function(spd_matrix) {
          if (is.null(spd_matrix) || !is.matrix(spd_matrix) || any(dim(spd_matrix) == 0)) return(NA_real_)
          # Eigenvalues of a kxk covariance matrix. k here is proj_obj$parameters$k.
          if (proj_obj$parameters$k == 0) return(NA_real_)
          
          eig_vals <- tryCatch(
            eigen(spd_matrix, symmetric = TRUE, only.values = TRUE)$values,
            error = function(e_eig) {warning(sprintf("Eigen decomposition failed for a cov_coeff matrix (k=%d).", current_k)); NULL}
          )
          
          if (is.null(eig_vals) || length(eig_vals) == 0) return(NA_real_)
          
          # Remove numerically zero or negative eigenvalues before CV calculation for stability
          # Though eigenvalues of a (regularized) SPD matrix should be positive.
          eig_vals_clean <- eig_vals[eig_vals > .Machine$double.eps^0.75] # A bit more than double.eps

          if (length(eig_vals_clean) < 1) return(NA_real_) # Needs at least one positive eigenvalue
          if (length(eig_vals_clean) == 1) return(NA_real_) # SD of 1 value is NA, so CV is NA. Correct.
                                                         # Or, define CV as 0 if only one eigenvalue? No, NA is better.

          mean_e <- mean(eig_vals_clean, na.rm = TRUE)
          sd_e <- stats::sd(eig_vals_clean, na.rm = TRUE)

          if (is.na(sd_e) || is.na(mean_e) || abs(mean_e) < .Machine$double.eps) { # check mean_e for near zero
            return(NA_real_)
          }
          return(sd_e / mean_e)
        })
        mean_cv_value <- mean(stats::na.omit(cvs_subject), na.rm = TRUE)
        if (is.nan(mean_cv_value)) mean_cv_value <- NA_real_ # Handle case where all cvs_subject are NA
      }
      results_df_list[[length(results_df_list) + 1]] <- data.frame(
        k = current_k,
        metric_name = "CV_eigen_cov",
        value = mean_cv_value
      )
    }
  }

  if (length(results_df_list) == 0) {
    if (verbose) message_stage("No metrics could be computed.")
    return(NULL)
  }

  plot_data <- do.call(rbind, results_df_list)
  
  if (nrow(plot_data) == 0 || all(is.na(plot_data$value))) {
    if (verbose) message_stage("No valid data points to plot.")
    return(NULL)
  }
  
  # Ensure k is numeric for plotting scale
  plot_data$k <- as.numeric(plot_data$k)

  p <- ggplot2::ggplot(plot_data, ggplot2::aes(x = k, y = value, group = metric_name, color = metric_name)) +
    ggplot2::geom_line(linewidth = 1) +
    ggplot2::geom_point(size = 2) +
    ggplot2::facet_wrap(~ metric_name, scales = "free_y", ncol = 1) +
    ggplot2::scale_x_continuous(breaks = unique(plot_data$k)) + # Ensure all k values are ticks
    ggplot2::labs(
      title = "HATSA k-Stability Analysis",
      x = "Number of Components (k)",
      y = "Metric Value"
    ) +
    ggplot2::theme_bw() +
    ggplot2::theme(legend.position = "none") # Facets make legend redundant

  return(p)
}

# Helper function for generating example data (conceptual)
# To make the example runnable, one might need a synthetic data generator.
# This is a placeholder to illustrate how projector_list_over_k would look.
# generate_synthetic_hatsa_output_list <- function(k_values = c(3,4,5), N_subjects = 5, V_p = 20) {
#   # This function would need to create valid hatsa_projector objects
#   # using synthetic data and calls to run_hatsa_core and hatsa_projector constructor.
#   # For now, it's a conceptual placeholder.
#   # For proper testing, one would mock hatsa_projector objects or use actual test data.
#   
#   # Example structure:
#   # k_list <- lapply(k_values, function(k_val) {
#   #   # ... generate or load a hatsa_projector object for k_val ...
#   #   # For example:
#   #   params <- list(k = k_val, N_subjects = N_subjects, V_p = V_p, method = "hatsa_core", anchor_indices = 1:5)
#   #   core_res <- list(
#   #     U_aligned_list = replicate(N_subjects, matrix(rnorm(V_p*k_val), V_p, k_val), simplify=FALSE),
#   #     R_final_list = replicate(N_subjects, diag(k_val), simplify=FALSE),
#   #     U_original_list = replicate(N_subjects, matrix(rnorm(V_p*k_val), V_p, k_val), simplify=FALSE),
#   #     Lambda_original_list = replicate(N_subjects, sort(runif(k_val, 0.1, 1), decreasing=TRUE), simplify=FALSE),
#   #     Lambda_original_gaps_list = replicate(N_subjects, {
#   #        lams <- sort(runif(k_val, 0.1, 1), decreasing=TRUE); if(k_val>1) (lams[1:(k_val-1)] - lams[2:k_val])/lams[2:k_val] else numeric(0)
#   #        }, simplify=FALSE),
#   #     T_anchor_final = matrix(rnorm(min(5,V_p)*k_val), min(5,V_p), k_val)
#   #   )
#   #   hatsa::hatsa_projector(core_res, params)
#   # })
#   # names(k_list) <- as.character(k_values)
#   # return(k_list)
#   warning("generate_synthetic_hatsa_output_list is a conceptual placeholder and does not produce real data.")
#   return(list())
# }


#' Plot Multidimensional Scaling (MDS) of Subjects based on SPD Matrix Distances
#'
#' Computes Riemannian distances between subjects based on their SPD matrix
#' representations, performs classical MDS, and plots the subjects in a
#' low-dimensional space (typically 2D).
#'
#' @param projector_object A \code{hatsa_projector} or \code{task_hatsa_projector} object.
#' @param k_mds Integer, the number of MDS dimensions to compute (default: 2).
#' @param spd_representation_type Character string, the type of SPD representation
#'   to use for distance calculation (e.g., "cov_coeffs", "fc_conn").
#'   This is passed to \code{hatsa::riemannian_distance_matrix_spd}.
#' @param dist_mat_options A list of additional arguments to pass to
#'   \code{hatsa::riemannian_distance_matrix_spd}. This can include arguments like
#'   \code{spd_metric}, \code{subject_data_list} (if needed for the chosen type),
#'   \code{k_conn_params}, \code{spd_regularize_epsilon}, \code{verbose}, etc.
#' @param subject_info Optional. A data frame with \code{N_subjects} rows.
#'   If provided, it can contain a column named \code{subject_label} for text labels on
#'   the plot, and other columns that can be mapped to ggplot aesthetics (e.g.,
#'   a column named by \code{color_by_column} or \code{shape_by_column}).
#'   Row names of \code{subject_info} should correspond to subject indices (1 to N) or
#'   be actual subject IDs if the distance matrix has them.
#' @param color_by_column Character string. If \code{subject_info} is provided, the name
#'   of the column in \code{subject_info} to use for coloring points.
#' @param shape_by_column Character string. If \code{subject_info} is provided, the name
#'   of the column in \code{subject_info} to use for point shapes.
#' @param plot_labels Logical, whether to plot subject labels near points. Requires
#'   a \code{subject_label} column in \code{subject_info} or uses default labels.
#'   Consider using the \code{ggrepel} package for better label placement if many
#'   points overlap (not directly implemented here to reduce dependencies).
#' @param cmdscale_add Logical, the \code{add} argument for \code{stats::cmdscale}
#'   (default: TRUE). Useful if distances are not perfectly Euclidean.
#' @param verbose Logical, if TRUE, prints progress messages. Default is FALSE.
#'
#' @return A list containing:
#'   \itemize{
#'     \item{\code{plot}: A \code{ggplot} object for the MDS plot.}
#'     \item{\code{mds_results}: The output from \code{stats::cmdscale}, including
#'           coordinates (\code{points}), eigenvalues (\code{eig}), etc.}
#'     \item{\code{distance_matrix}: The computed Riemannian distance matrix.}
#'     \item{\code{valid_subject_indices}: Indices of subjects included in MDS (after NA removal).}
#'   }
#'   Returns NULL if critical steps fail (e.g., distance matrix computation).
#'
#' @export
#' @importFrom stats cmdscale
#' @importFrom ggplot2 ggplot aes geom_point geom_text geom_hline geom_vline labs theme_bw
#' @examples
#' # Conceptual example, assuming 'proj_obj' is a hatsa_projector object
#' # and subject_covariates is a data frame with N_subjects rows
#' # and columns "ID" (for labels), "Group" (for color).
#'
#' # if (requireNamespace("ggplot2", quietly = TRUE) &&
#' #     exists("generate_synthetic_hatsa_output") && # Assuming a single projector gen
#' #     exists("hatsa_projector")) { # Ensure constructor is available
#' #
#' #   # Generate a single projector object
#' #   N_subj_example <- 10
#' #   V_p_example <- 30
#' #   k_example <- 5
#' #   proj_params <- list(k = k_example, N_subjects = N_subj_example, V_p = V_p_example,
#' #                       method="hatsa_core", anchor_indices = 1:5)
#' #   proj_core_res <- list(
#' #      U_aligned_list = replicate(N_subj_example, matrix(rnorm(V_p_example*k_example), V_p_example, k_example), simplify=FALSE),
#' #      R_final_list = replicate(N_subj_example, diag(k_example), simplify=FALSE),
#' #      U_original_list = replicate(N_subj_example, matrix(rnorm(V_p_example*k_example), V_p_example, k_example), simplify=FALSE),
#' #      Lambda_original_list = replicate(N_subj_example, sort(runif(k_example, 0.1, 1), decreasing=TRUE), simplify=FALSE),
#' #      Lambda_original_gaps_list = replicate(N_subj_example, {
#' #         lams <- sort(runif(k_example, 0.1, 1), decreasing=TRUE); if(k_example>1) (lams[1:(k_example-1)] - lams[2:k_example])/lams[2:k_example] else numeric(0)
#' #         }, simplify=FALSE),
#' #      T_anchor_final = matrix(rnorm(min(5,V_p_example)*k_example), min(5,V_p_example), k_example)
#' #    )
#' #   proj_obj <- hatsa::hatsa_projector(proj_core_res, proj_params)
#' #
#' #   # Example subject_info
#' #   subject_covariates <- data.frame(
#' #     subject_label = paste0("S", 1:N_subj_example),
#' #     Group = factor(rep(c("A", "B"), each = N_subj_example / 2)),
#' #     stringsAsFactors = FALSE
#' #   )
#' #
#' #   # Run MDS plot
#' #   mds_plot_result <- plot_mds_spd_subjects(
#' #     projector_object = proj_obj,
#' #     spd_representation_type = "cov_coeffs",
#' #     dist_mat_options = list(spd_metric = "logeuclidean", spd_regularize_epsilon = 1e-6),
#' #     subject_info = subject_covariates,
#' #     color_by_column = "Group",
#' #     plot_labels = TRUE,
#' #     verbose = TRUE
#' #   )
#' #   if (!is.null(mds_plot_result)) print(mds_plot_result$plot)
#' # }
plot_mds_spd_subjects <- function(projector_object,
                                  k_mds = 2,
                                  spd_representation_type = "cov_coeffs",
                                  dist_mat_options = list(),
                                  subject_info = NULL,
                                  color_by_column = NULL,
                                  shape_by_column = NULL,
                                  plot_labels = FALSE,
                                  cmdscale_add = TRUE,
                                  verbose = FALSE) {

  if (!requireNamespace("ggplot2", quietly = TRUE)) {
    stop("Package 'ggplot2' is required for this function.")
  }
  if (!inherits(projector_object, "hatsa_projector") && !inherits(projector_object, "task_hatsa_projector")) {
    stop("`projector_object` must be a hatsa_projector or task_hatsa_projector object.")
  }
  if (projector_object$parameters$N_subjects == 0) {
    warning("No subjects in the projector object. Cannot create MDS plot.")
    return(NULL)
  }

  if (verbose) message_stage(sprintf("Computing Riemannian distance matrix (type: %s)...", spd_representation_type))

  dist_args <- dist_mat_options
  dist_args$object <- projector_object
  dist_args$type <- spd_representation_type
  # Ensure verbose from main function is passed if not in dist_mat_options
  if (is.null(dist_args$verbose)) dist_args$verbose <- verbose 

  dist_matrix_full <- tryCatch(
    do.call(hatsa::riemannian_distance_matrix_spd, dist_args),
    error = function(e) {
      warning(sprintf("Riemannian distance matrix computation failed: %s", e$message))
      NULL
    }
  )

  if (is.null(dist_matrix_full)) {
    return(NULL)
  }
  
  N_total_subjects <- projector_object$parameters$N_subjects
  
  # Identify subjects with all NA distances (completely failed) or rows/cols that are all NA
  # These subjects cannot be included in MDS.
  valid_subjects_mask <- apply(dist_matrix_full, 1, function(row) !all(is.na(row))) &
                         apply(dist_matrix_full, 2, function(col) !all(is.na(col)))
  
  num_valid_subjects <- sum(valid_subjects_mask)

  if (num_valid_subjects < 2) {
    warning("Less than 2 subjects have valid distance data. Cannot perform MDS.")
    return(list(plot = NULL, mds_results = NULL, distance_matrix = dist_matrix_full, valid_subject_indices = which(valid_subjects_mask)))
  }
  if (num_valid_subjects < N_total_subjects) {
     warning(sprintf("%d subjects out of %d had NA distances and were excluded from MDS.", 
                     N_total_subjects - num_valid_subjects, N_total_subjects))
  }
  
  dist_matrix_valid <- dist_matrix_full[valid_subjects_mask, valid_subjects_mask, drop = FALSE]
  valid_subject_indices <- which(valid_subjects_mask)


  if (verbose) message_stage(sprintf("Performing classical MDS on %d subjects for %d dimensions...", num_valid_subjects, k_mds))
  
  # cmdscale expects a dist object or a full symmetric matrix with zeros on diagonal
  # Our riemannian_distance_matrix_spd should return this.
  # However, if there are any remaining NAs after subsetting (should not happen if subsetting is correct),
  # cmdscale will fail. We check for this implicitly by tryCatch.
  mds_fit <- tryCatch(
    stats::cmdscale(as.dist(dist_matrix_valid), k = k_mds, eig = TRUE, add = cmdscale_add),
    error = function(e) {
      warning(sprintf("MDS computation (cmdscale) failed: %s", e$message))
      NULL
    }
  )

  if (is.null(mds_fit)) {
    return(list(plot = NULL, mds_results = NULL, distance_matrix = dist_matrix_full, valid_subject_indices = valid_subject_indices))
  }

  mds_coords <- as.data.frame(mds_fit$points)
  colnames(mds_coords) <- paste0("Dim", 1:ncol(mds_coords))
  mds_coords$subject_idx_original <- valid_subject_indices # Store original index

  # Prepare plotting data by merging with subject_info if provided
  plot_df <- mds_coords
  plot_df$subject_label_plot <- paste0("S", plot_df$subject_idx_original) # Default labels

  if (!is.null(subject_info) && inherits(subject_info, "data.frame")) {
      if(nrow(subject_info) == N_total_subjects) {
          subject_info_valid <- subject_info[valid_subjects_mask, , drop = FALSE]
          subject_info_valid$subject_idx_original <- valid_subject_indices # for merging with mds_coords
          
          # Attempt to merge. If subject_info has rownames that are subject IDs from dist_matrix,
          # and dist_matrix had interpretable rownames/colnames, this logic would need adjustment.
          # Current riemannian_distance_matrix_spd aims for 1:N_subjects in rownames/colnames.
          plot_df <- merge(plot_df, subject_info_valid, by = "subject_idx_original", all.x = TRUE)

          if ("subject_label" %in% colnames(plot_df) && plot_labels) {
             plot_df$subject_label_plot <- plot_df$subject_label
          }
      } else {
          warning("`subject_info` provided but row count does not match N_subjects. It will be ignored for merging specifics, but used for column name checks.")
      }
  }
  
  # Set up aesthetics
  plot_aes <- ggplot2::aes(x = Dim1, y = Dim2)
  if (!is.null(color_by_column) && color_by_column %in% colnames(plot_df)) {
    plot_aes$colour <- ggplot2::aes_string(colour = color_by_column)$colour
  }
  if (!is.null(shape_by_column) && shape_by_column %in% colnames(plot_df)) {
    # Ensure the shaping variable is a factor for discrete shapes
    if(!is.factor(plot_df[[shape_by_column]])) plot_df[[shape_by_column]] <- as.factor(plot_df[[shape_by_column]])
    plot_aes$shape <- ggplot2::aes_string(shape = shape_by_column)$shape
  }


  p <- ggplot2::ggplot(plot_df, plot_aes) +
    ggplot2::geom_point(size = 3, alpha = 0.8) +
    ggplot2::geom_hline(yintercept = 0, linetype = "dashed", color = "grey50") +
    ggplot2::geom_vline(xintercept = 0, linetype = "dashed", color = "grey50") +
    ggplot2::labs(
      title = paste("MDS of Subjects based on", spd_representation_type),
      subtitle = paste0("Riemannian Distances (", dist_mat_options$spd_metric %||% "logeuclidean", 
                       "), MDS k=", k_mds, ", GOF=", sprintf("%.2f", mds_fit$GOF[1])),
      x = "MDS Dimension 1",
      y = "MDS Dimension 2",
      caption = if (num_valid_subjects < N_total_subjects) {
                  sprintf("%d/%d subjects plotted due to NA distances.", num_valid_subjects, N_total_subjects)
                } else { "" }
    ) +
    ggplot2::theme_bw()

  if (plot_labels) {
      # Use ggrepel if available, otherwise geom_text
      if (requireNamespace("ggrepel", quietly = TRUE)) {
          p <- p + ggrepel::geom_text_repel(ggplot2::aes(label = subject_label_plot), size = 3, 
                                            max.overlaps = Inf, # try to plot all
                                            box.padding = 0.4) 
      } else {
          p <- p + ggplot2::geom_text(ggplot2::aes(label = subject_label_plot), size = 3, nudge_y = 0.05 * diff(range(plot_df$Dim2, na.rm=TRUE)), check_overlap = TRUE)
      }
  }
  
  # Add % variance explained by axes if k_mds=2 and eig available
  if (k_mds >= 2 && !is.null(mds_fit$eig)) {
      total_pos_eig_variance <- sum(mds_fit$eig[mds_fit$eig > 0])
      if (total_pos_eig_variance > 0) {
          var_dim1 <- (mds_fit$eig[1] / total_pos_eig_variance) * 100
          var_dim2 <- (mds_fit$eig[2] / total_pos_eig_variance) * 100
          p <- p + ggplot2::xlab(sprintf("MDS Dimension 1 (%.1f%%)", var_dim1)) +
                   ggplot2::ylab(sprintf("MDS Dimension 2 (%.1f%%)", var_dim2))
      }
  }


  if (verbose) message_stage("MDS plot generated.")

  return(list(
    plot = p,
    mds_results = mds_fit,
    distance_matrix_computed = dist_matrix_full, # The one before NA removal for MDS
    valid_subject_indices_in_mds = valid_subject_indices
  ))
}

# Helper for null or default
'%||%' <- function(a, b) if (is.null(a)) b else a

# Make sure ggplot2 related imports are at the top of the file or in NAMESPACE
# @importFrom ggplot2 ggplot aes geom_point geom_text geom_hline geom_vline labs theme_bw ggtitle
# @importFrom stats cmdscale
# @importFrom ggrepel geom_text_repel # Optional, but nice for labels


#' Plot Multidimensional Scaling (MDS) of Subjects based on SPD Matrix Distances
#'
#' Computes Riemannian distances between subjects based on their SPD matrix
#' representations, performs classical MDS, and plots the subjects in a
#' low-dimensional space (typically 2D).
#'
#' @param projector_object A \code{hatsa_projector} or \code{task_hatsa_projector} object.
#' @param k_mds Integer, the number of MDS dimensions to compute (default: 2).
#' @param spd_representation_type Character string, the type of SPD representation
#'   to use for distance calculation (e.g., "cov_coeffs", "fc_conn").
#'   This is passed to \code{hatsa::riemannian_distance_matrix_spd}.
#' @param dist_mat_options A list of additional arguments to pass to
#'   \code{hatsa::riemannian_distance_matrix_spd}. This can include arguments like
#'   \code{spd_metric}, \code{subject_data_list} (if needed for the chosen type),
#'   \code{k_conn_params}, \code{spd_regularize_epsilon}, \code{verbose}, etc.
#' @param subject_info Optional. A data frame with \code{N_subjects} rows.
#'   If provided, it can contain a column named \code{subject_label} for text labels on
#'   the plot, and other columns that can be mapped to ggplot aesthetics (e.g.,
#'   a column named by \code{color_by_column} or \code{shape_by_column}).
#'   Row names of \code{subject_info} should correspond to subject indices (1 to N) or
#'   be actual subject IDs if the distance matrix has them.
#' @param color_by_column Character string. If \code{subject_info} is provided, the name
#'   of the column in \code{subject_info} to use for coloring points.
#' @param shape_by_column Character string. If \code{subject_info} is provided, the name
#'   of the column in \code{subject_info} to use for point shapes.
#' @param plot_labels Logical, whether to plot subject labels near points. Requires
#'   a \code{subject_label} column in \code{subject_info} or uses default labels.
#'   Consider using the \code{ggrepel} package for better label placement if many
#'   points overlap (not directly implemented here to reduce dependencies).
#' @param cmdscale_add Logical, the \code{add} argument for \code{stats::cmdscale}
#'   (default: TRUE). Useful if distances are not perfectly Euclidean.
#' @param verbose Logical, if TRUE, prints progress messages. Default is FALSE.
#'
#' @return A list containing:
#'   \itemize{
#'     \item{\code{plot}: A \code{ggplot} object for the MDS plot.}
#'     \item{\code{mds_results}: The output from \code{stats::cmdscale}, including
#'           coordinates (\code{points}), eigenvalues (\code{eig}), etc.}
#'     \item{\code{distance_matrix}: The computed Riemannian distance matrix.}
#'     \item{\code{valid_subject_indices}: Indices of subjects included in MDS (after NA removal).}
#'   }
#'   Returns NULL if critical steps fail (e.g., distance matrix computation).
#'
#' @export
#' @importFrom stats cmdscale
#' @importFrom ggplot2 ggplot aes geom_point geom_text geom_hline geom_vline labs theme_bw
#' @examples
#' # Conceptual example, assuming 'proj_obj' is a hatsa_projector object
#' # and subject_covariates is a data frame with N_subjects rows
#' # and columns "ID" (for labels), "Group" (for color).
#'
#' # if (requireNamespace("ggplot2", quietly = TRUE) &&
#' #     exists("generate_synthetic_hatsa_output") && # Assuming a single projector gen
#' #     exists("hatsa_projector")) { # Ensure constructor is available
#' #
#' #   # Generate a single projector object
#' #   N_subj_example <- 10
#' #   V_p_example <- 30
#' #   k_example <- 5
#' #   proj_params <- list(k = k_example, N_subjects = N_subj_example, V_p = V_p_example,
#' #                       method="hatsa_core", anchor_indices = 1:5)
#' #   proj_core_res <- list(
#' #      U_aligned_list = replicate(N_subj_example, matrix(rnorm(V_p_example*k_example), V_p_example, k_example), simplify=FALSE),
#' #      R_final_list = replicate(N_subj_example, diag(k_example), simplify=FALSE),
#' #      U_original_list = replicate(N_subj_example, matrix(rnorm(V_p_example*k_example), V_p_example, k_example), simplify=FALSE),
#' #      Lambda_original_list = replicate(N_subj_example, sort(runif(k_example, 0.1, 1), decreasing=TRUE), simplify=FALSE),
#' #      Lambda_original_gaps_list = replicate(N_subj_example, {
#' #         lams <- sort(runif(k_example, 0.1, 1), decreasing=TRUE); if(k_example>1) (lams[1:(k_example-1)] - lams[2:k_example])/lams[2:k_example] else numeric(0)
#' #         }, simplify=FALSE),
#' #      T_anchor_final = matrix(rnorm(min(5,V_p_example)*k_example), min(5,V_p_example), k_example)
#' #    )
#' #   proj_obj <- hatsa::hatsa_projector(proj_core_res, proj_params)
#' #
#' #   # Example subject_info
#' #   subject_covariates <- data.frame(
#' #     subject_label = paste0("S", 1:N_subj_example),
#' #     Group = factor(rep(c("A", "B"), each = N_subj_example / 2)),
#' #     stringsAsFactors = FALSE
#' #   )
#' #
#' #   # Run MDS plot
#' #   mds_plot_result <- plot_mds_spd_subjects(
#' #     projector_object = proj_obj,
#' #     spd_representation_type = "cov_coeffs",
#' #     dist_mat_options = list(spd_metric = "logeuclidean", spd_regularize_epsilon = 1e-6),
#' #     subject_info = subject_covariates,
#' #     color_by_column = "Group",
#' #     plot_labels = TRUE,
#' #     verbose = TRUE
#' #   )
#' #   if (!is.null(mds_plot_result)) print(mds_plot_result$plot)
#' # }
</file>

<file path="R/hatsa_validation_metrics.R">
#' @file hatsa_validation_metrics.R
#' @title Validation Metrics for HATSA
#' @description Functions to compute various validation metrics for assessing
#' HATSA performance, typically using toy data with known ground truth.
#'
#' @importFrom stats cor
#' @importFrom vegan procrustes
NULL

#' Compute Group Template (v) Recovery Metrics
#'
#' Evaluates how well the estimated group-level spectral template (`object$v`
#' from a `hatsa_projector` object) matches a true ground-truth spectral basis.
#'
#' @param hatsa_object A fitted `hatsa_projector` object.
#' @param U_true The ground-truth spectral basis (matrix, e.g., Vp x k).
#' @param ... Additional arguments passed to `vegan::procrustes`.
#'
#' @return A list containing:
#'   \item{correlation}{Pearson correlation between the vectorized aligned estimated
#'     template and the vectorized true template.}
#'   \item{frobenius_norm_diff}{Frobenius norm of the difference between the
#'     aligned estimated template and the true template.}
#'   \item{procrustes_result}{The result of the `vegan::procrustes` call.}
#'   \item{v_aligned}{The Procrustes-aligned estimated group template.}
#'
#' @export
#' @examples
#' # See core-hatsa-toy-example.Rmd for usage.
compute_v_recovery <- function(hatsa_object, U_true, ...) {
  if (!inherits(hatsa_object, "hatsa_projector")) {
    stop("`hatsa_object` must be of class 'hatsa_projector'.")
  }
  if (!is.matrix(U_true) || !is.numeric(U_true)) {
    stop("`U_true` must be a numeric matrix.")
  }
  if (is.null(hatsa_object$v)) {
      stop("`hatsa_object$v` is NULL. Cannot compute recovery.")
  }
  if (nrow(hatsa_object$v) != nrow(U_true) || ncol(hatsa_object$v) != ncol(U_true)) {
    stop("Dimensions of `hatsa_object$v` and `U_true` must match.")
  }

  v_estimated <- hatsa_object$v

  # Align estimated v to U_true
  procr_fit <- vegan::procrustes(X = v_estimated, Y = U_true, symmetric = FALSE, ...)
  v_aligned <- procr_fit$Yrot

  # Metrics
  correlation <- stats::cor(as.vector(v_aligned), as.vector(U_true))
  frobenius_norm_diff <- norm(v_aligned - U_true, type = "F")

  return(list(
    correlation = correlation,
    frobenius_norm_diff = frobenius_norm_diff,
    procrustes_result = procr_fit,
    v_aligned = v_aligned
  ))
}

#' Compute Anchor Template (T_anchor_final) Recovery Metrics
#'
#' Evaluates how well the estimated group anchor template (`object$T_anchor_final`
#' from a `hatsa_projector` object) matches the true spectral basis at the
#' anchor locations.
#'
#' @param hatsa_object A fitted `hatsa_projector` object.
#' @param U_true The ground-truth spectral basis (matrix, Vp x k) for all parcels.
#' @param anchor_indices_true A numeric vector of indices specifying which rows of
#'   `U_true` correspond to the anchor parcels.
#' @param ... Additional arguments passed to `vegan::procrustes`.
#'
#' @return A list containing:
#'   \item{correlation}{Pearson correlation between the vectorized aligned estimated
#'     anchor template and the vectorized true anchor template.}
#'   \item{frobenius_norm_diff}{Frobenius norm of the difference between the
#'     aligned estimated anchor template and the true anchor template.}
#'   \item{procrustes_result}{The result of the `vegan::procrustes` call.}
#'   \item{T_anchor_aligned}{The Procrustes-aligned estimated anchor template.}
#'
#' @export
#' @examples
#' # See core-hatsa-toy-example.Rmd for usage.
compute_anchor_template_recovery <- function(hatsa_object, U_true, anchor_indices_true, ...) {
  if (!inherits(hatsa_object, "hatsa_projector")) {
    stop("`hatsa_object` must be of class 'hatsa_projector'.")
  }
  if (!is.matrix(U_true) || !is.numeric(U_true)) {
    stop("`U_true` must be a numeric matrix.")
  }
  if (!is.numeric(anchor_indices_true) || !is.vector(anchor_indices_true)) {
    stop("`anchor_indices_true` must be a numeric vector.")
  }
  if (any(anchor_indices_true > nrow(U_true)) || any(anchor_indices_true <= 0)) {
    stop("Invalid `anchor_indices_true`.")
  }
  if (is.null(hatsa_object$T_anchor_final)) {
    stop("`hatsa_object$T_anchor_final` is NULL. Cannot compute recovery.")
  }

  T_anchor_estimated <- hatsa_object$T_anchor_final
  U_true_anchors <- U_true[anchor_indices_true, , drop = FALSE]

  if (nrow(T_anchor_estimated) != nrow(U_true_anchors) || ncol(T_anchor_estimated) != ncol(U_true_anchors)) {
    stop(paste(
      "Dimensions of `hatsa_object$T_anchor_final` (", 
      nrow(T_anchor_estimated), "x", ncol(T_anchor_estimated),
      ") and `U_true` at anchor_indices_true (",
      nrow(U_true_anchors), "x", ncol(U_true_anchors),
      ") must match.", sep=""
    ))
  }

  # Align estimated T_anchor_final to U_true_anchors
  procr_fit <- vegan::procrustes(X = T_anchor_estimated, Y = U_true_anchors, symmetric = FALSE, ...)
  T_anchor_aligned <- procr_fit$Yrot

  # Metrics
  correlation <- stats::cor(as.vector(T_anchor_aligned), as.vector(U_true_anchors))
  frobenius_norm_diff <- norm(T_anchor_aligned - U_true_anchors, type = "F")

  return(list(
    correlation = correlation,
    frobenius_norm_diff = frobenius_norm_diff,
    procrustes_result = procr_fit,
    T_anchor_aligned = T_anchor_aligned
  ))
}

#' Compute Rotation Recovery (SO(k) Alignment) Metrics
#'
#' Assesses how accurately the subject-specific rotation matrices (`R_i`)
#' estimated by HATSA match the true rotations.
#'
#' @param hatsa_object A fitted `hatsa_projector` object, which contains
#'   `R_final_list` (list of estimated k x k rotation matrices).
#' @param R_true_list A list of true k x k rotation matrices, corresponding to
#'   each subject in `hatsa_object`.
#' @param ... Additional arguments passed to `misalign_deg` function.
#'
#' @return A numeric vector of misalignment angles in degrees, one for each
#'   subject. Returns `NA` for subjects where either the estimated or true
#'   rotation is missing or invalid for comparison.
#'
#' @export
#' @examples
#' # See core-hatsa-toy-example.Rmd for usage.
compute_rotation_recovery <- function(hatsa_object, R_true_list, ...) {
  if (!inherits(hatsa_object, "hatsa_projector")) {
    stop("`hatsa_object` must be of class 'hatsa_projector'.")
  }
  if (is.null(hatsa_object$R_final_list)) {
    stop("`hatsa_object$R_final_list` is NULL. Cannot compute rotation recovery.")
  }
  if (!is.list(R_true_list)) {
    stop("`R_true_list` must be a list of matrices.")
  }

  # Replace any NULL elements in R_final_list with NA placeholder
  R_est_list <- hatsa_object$R_final_list
  for (i in seq_along(R_est_list)) {
    if (is.null(R_est_list[[i]])) {
      R_est_list[[i]] <- NA
    }
  }
  
  # Find common length for comparison
  num_subjects <- min(length(R_est_list), length(R_true_list))
  
  if (num_subjects == 0) {
    # No subjects to compare
    warning("No subjects available for rotation recovery comparison.")
    return(numeric(0))
  }
  
  # Match lengths - truncate longer list
  R_est_list <- R_est_list[1:num_subjects]
  R_true_list <- R_true_list[1:num_subjects]
  
  misalignment_degrees <- numeric(num_subjects)

  for (i in seq_len(num_subjects)) {
    R_est <- R_est_list[[i]]
    R_true <- R_true_list[[i]]

    if (length(R_est) == 1 && is.na(R_est) || is.null(R_est) || is.null(R_true) || 
        !is.matrix(R_est) || !is.matrix(R_true)) {
      misalignment_degrees[i] <- NA_real_
      warning(sprintf("Missing or non-matrix rotation for subject %d. Setting misalignment to NA.", i), call. = FALSE)
      next
    }
    
    if(!all(dim(R_est) == dim(R_true))) {
      misalignment_degrees[i] <- NA_real_
      warning(sprintf("Dimension mismatch for rotation matrices for subject %d (Est: %s, True: %s). Setting misalignment to NA.", 
                      i, paste(dim(R_est), collapse="x"), paste(dim(R_true), collapse="x")), call. = FALSE)
      next
    }
    
    # Use misalign_deg with namespace to avoid export dependency
    misalignment_degrees[i] <- hatsa::misalign_deg(R_est = R_est, R_true = R_true, ...)
  }

  return(misalignment_degrees)
}

#' Compute Eigenvalue Spectrum Fidelity Metrics
#'
#' Compares subject-specific eigenvalues from HATSA (`Lambda_original_list`)
#' with true eigenvalues (e.g., from a toy data generator).
#'
#' @param hatsa_object A fitted `hatsa_projector` object, which contains
#'   `Lambda_original_list` (list of numeric vectors of eigenvalues).
#' @param true_eigenvalues_list A list of numeric vectors, where each vector contains
#'   the true eigenvalues for the corresponding subject. If a single vector is provided,
#'   it is assumed to be the common true eigenvalues for all subjects.
#' @param k_to_compare Integer or NULL. The number of top eigenvalues to compare.
#'   If NULL (default), all available eigenvalues are compared (up to the length of
#'   the shorter of the estimated or true eigenvalue vectors for that subject).
#'
#' @return A list of lists, one for each subject. Each inner list contains:
#'   \item{correlation}{Pearson correlation between estimated and true eigenvalues.}
#'   \item{mse}{Mean Squared Error between estimated and true eigenvalues.}
#'   \item{num_compared}{The number of eigenvalue pairs actually compared.}
#'   Returns NULL for a subject if inputs are invalid for that subject.
#'
#' @export
#' @examples
#' # See core-hatsa-toy-example.Rmd for usage.
compute_eigenvalue_fidelity <- function(hatsa_object, true_eigenvalues_list, k_to_compare = NULL) {
  if (!inherits(hatsa_object, "hatsa_projector")) {
    stop("`hatsa_object` must be of class 'hatsa_projector'.")
  }
  if (is.null(hatsa_object$Lambda_original_list)) {
    stop("`hatsa_object$Lambda_original_list` is NULL. Cannot compute eigenvalue fidelity.")
  }
  if (!is.list(hatsa_object$Lambda_original_list)) {
    stop("`hatsa_object$Lambda_original_list` must be a list.")
  }
  if (!is.list(true_eigenvalues_list) && !is.numeric(true_eigenvalues_list)){
    stop("`true_eigenvalues_list` must be a list of numeric vectors or a single numeric vector.")
  }
  
  # If true_eigenvalues_list is a single vector, replicate it for all subjects
  if (is.numeric(true_eigenvalues_list) && !is.list(true_eigenvalues_list)) {
    true_eigenvalues_list <- rep(list(true_eigenvalues_list), length(hatsa_object$Lambda_original_list))
  }

  if (length(hatsa_object$Lambda_original_list) != length(true_eigenvalues_list)) {
    stop("Length of `hatsa_object$Lambda_original_list` and `true_eigenvalues_list` must be equal.")
  }
  if (!is.null(k_to_compare) && (!is.numeric(k_to_compare) || length(k_to_compare) != 1 || k_to_compare < 1)){
    stop("`k_to_compare` must be a single positive integer or NULL.")
  }

  num_subjects <- length(hatsa_object$Lambda_original_list)
  results_list <- vector("list", num_subjects)
  names(results_list) <- names(hatsa_object$Lambda_original_list) # Preserve names if any
  if (is.null(names(results_list)) && num_subjects > 0) {
    # Use subject index if no names provided
    names(results_list) <- if(!is.null(names(true_eigenvalues_list))) {
      names(true_eigenvalues_list)
    } else {
      paste0("Subject_", seq_len(num_subjects))
    }
  }

  for (i in seq_len(num_subjects)) {
    est_lambdas <- hatsa_object$Lambda_original_list[[i]]
    true_lambdas <- true_eigenvalues_list[[i]]

    if (is.null(est_lambdas) || !is.numeric(est_lambdas) || 
        is.null(true_lambdas) || !is.numeric(true_lambdas)) {
      warning(sprintf("Missing or non-numeric eigenvalues for subject %d. Skipping.", i), call. = FALSE)
      results_list[[i]] <- list(correlation = NA_real_, mse = NA_real_, num_compared = 0)
      next
    }
    
    len_est <- length(est_lambdas)
    len_true <- length(true_lambdas)
    
    if (len_est == 0 || len_true == 0) {
        warning(sprintf("Empty eigenvalue vector(s) for subject %d. Skipping.", i), call. = FALSE)
        results_list[[i]] <- list(correlation = NA_real_, mse = NA_real_, num_compared = 0)
        next
    }

    current_k <- min(len_est, len_true)
    if (!is.null(k_to_compare)) {
      if (k_to_compare > current_k) {
        warning(sprintf("k_to_compare (%d) is greater than available eigenvalues for subject %d (min_len=%d). Using min_len.", 
                        k_to_compare, i, current_k), call. = FALSE)
      } else {
        current_k <- k_to_compare
      }
    }
    
    if (current_k == 0) { # Should be caught by len_est/len_true checks but as safeguard
        results_list[[i]] <- list(correlation = NA_real_, mse = NA_real_, num_compared = 0)
        next
    }

    est_lambdas_comp <- est_lambdas[1:current_k]
    true_lambdas_comp <- true_lambdas[1:current_k]
    
    # Ensure no NAs that would break cor or mean, replace with warning and NA result
    if(anyNA(est_lambdas_comp) || anyNA(true_lambdas_comp)){
        warning(sprintf("NA values found in eigenvalues for comparison for subject %d. Metrics will be NA.", i), call. = FALSE)
        results_list[[i]] <- list(correlation = NA_real_, mse = NA_real_, num_compared = current_k)
        next
    }
    
    # Avoid correlation with zero variance vectors
    cor_val <- NA_real_
    if (stats::var(est_lambdas_comp) > 1e-9 && stats::var(true_lambdas_comp) > 1e-9) {
        cor_val <- tryCatch(stats::cor(est_lambdas_comp, true_lambdas_comp),
                              error = function(e) NA_real_)
    } else if (identical(est_lambdas_comp, true_lambdas_comp)) {
        cor_val <- 1.0 # Perfect correlation if both are constant and identical
    }

    mse_val <- mean((est_lambdas_comp - true_lambdas_comp)^2)

    results_list[[i]] <- list(
      correlation = cor_val,
      mse = mse_val,
      num_compared = current_k
    )
  }

  return(results_list)
}
</file>

<file path="R/hatsa-package.R">
#' @keywords internal
"_PACKAGE"

#' hatsa: Core HATSA for Functional Connectivity Alignment
#'
#' The `hatsa` package implements the Core HATSA (Hybrid Anchor-based
#' Time-Series Alignment) algorithm. This algorithm is designed to align
#' functional connectivity patterns, represented as spectral sketches derived
#' from graph Laplacians, across multiple subjects. It leverages anchor parcels
#' and Generalized Procrustes Analysis for robust alignment, employing sparse
#' matrix operations for computational efficiency.
#'
#' @section Core HATSA Algorithm:
#' The primary function is \code{\link{run_hatsa_core}}. It processes
#' subject-specific time-series data and alignment parameters to produce:
#' \itemize{
#'   \item Original spectral sketches for each subject.
#'   \item Aligned spectral sketches for each subject.
#'   \item The corresponding rotation matrices (in SO(k)) used for alignment.
#' }
#'
#' The algorithm follows three main stages:
#' 1.  **Initial Spectral Sketching (per subject):**
#'     \itemize{
#'       \item A correlation matrix is computed from the time-series.
#'       \item This graph is sparsified based on strongest positive/negative
#'             connections per parcel.
#'       \item The sparsified graph is symmetrized using a weighted averaging scheme.
#'       \item Non-zero edge weights of the symmetrized graph are z-scored,
#'             ensuring symmetry is preserved.
#'       \item A sparse graph Laplacian (`L = D - W`) is constructed.
#'       \item The `k` eigenvectors corresponding to the smallest, non-zero
#'             eigenvalues of `L` form the subject's original spectral sketch.
#'             Eigenvectors for eigenvalues numerically close to zero are discarded.
#'             This step uses efficient methods for sparse matrices.
#'     }
#' 2.  **Iterative Refinement (Generalized Procrustes Analysis - GPA):**
#'     \itemize{
#'       \item Rows corresponding to pre-defined anchor parcels are extracted from
#'             each subject's original spectral sketch.
#'       \item A group anchor template is initialized (typically as the mean of
#'             subjects' anchor sketches).
#'       \item Iteratively:
#'             \itemize{
#'               \item Subject-specific orthogonal rotation matrices (`R_i \in SO(k)`)
#'                     are computed to best align each subject's anchor sketch to the
#'                     current group template (Orthogonal Procrustes Problem).
#'               \item The group anchor template is updated as the mean of the
#'                     subjects' rotated anchor sketches.
#'             }
#'     }
#' 3.  **Apply Final Rotations:**
#'     \itemize{
#'       \item The final rotation matrices from the GPA are applied to each
#'             subject's full original spectral sketch to obtain the aligned
#'             spectral sketches.
#'     }
#'
#' @docType _PACKAGE
#' @name hatsa-package
#' @aliases hatsa hatsaR
NULL
</file>

<file path="R/metrics.R">
# Helper for safe matrix logarithm, returning NULL on error
# Not exported, used internally by misalign_deg
safe_logm_internal <- function(M) {
  res <- tryCatch(
    expm::logm(M),
    error = function(e) {
      # warning(paste("Matrix logarithm failed:", e$message, 
      #               ". Will use fallback for misalignment calculation."))
      return(NULL)
    }
  )
  return(res)
}

#' Calculate Misalignment Angle Between Two Rotation Matrices
#'
#' Computes the geodesic distance on SO(k) between two k x k rotation matrices,
#' representing the angle of rotation needed to align one to the other.
#' The result is given in degrees.
#'
#' @param R_est A k x k estimated rotation matrix (should be orthogonal with determinant +1).
#' @param R_true A k x k true rotation matrix (should be orthogonal with determinant +1).
#' @param method Character string, the method to use. Default is "geodesic".
#'   Currently, "geodesic" uses the matrix logarithm. 
#'   A fallback to a simpler trace-based formula (most accurate for SO(3)) is used 
#'   if `expm::logm` fails or the `expm` package is not installed.
#'
#' @return The misalignment angle in degrees. Returns 0 for identical matrices.
#'   Returns NA if dimensions are mismatched or matrices are not square.
#' 
#' @details
#' The primary method ("geodesic") calculates the angle as:
#' `||logm(R_true^T R_est)||_F / sqrt(2)`,
#' where `logm` is the matrix logarithm and `||.||_F` is the Frobenius norm.
#' This is a standard geodesic distance on the special orthogonal group SO(k).
#'
#' If the `expm` package is not available, or if `expm::logm` fails (e.g., due to
#' numerical issues if matrices are not perfectly orthogonal), the function
#' falls back to a simpler formula derived from the trace:
#' `acos((trace(R_true^T R_est) - 1) / 2)`.
#' This fallback formula is exact for SO(3) and provides a dissimilarity measure
#' for other k, being 0 for perfect alignment.
#'
#' @examples
#' if (requireNamespace("expm", quietly = TRUE)) {
#'   R1 <- diag(3)
#'   theta <- pi/4
#'   R2 <- matrix(c(cos(theta), -sin(theta), 0,
#'                  sin(theta),  cos(theta), 0,
#'                  0,           0,          1), nrow=3, byrow=TRUE)
#'   misalign_deg(R2, R1) # Should be 45 degrees
#'
#'   # Example for k=2
#'   R_true_2d <- matrix(c(1,0,0,1),2,2)
#'   angle_rad_2d <- pi/6 # 30 degrees
#'   R_est_2d <- matrix(c(cos(angle_rad_2d), -sin(angle_rad_2d),
#'                        sin(angle_rad_2d), cos(angle_rad_2d)), 2, 2)
#'   misalign_deg(R_est_2d, R_true_2d)
#' }
#' 
#' @export
#' @importFrom expm logm
misalign_deg <- function(R_est, R_true, method = "geodesic") {
  if (!is.matrix(R_est) || !is.matrix(R_true)) {
    warning("Inputs must be matrices.")
    return(NA_real_)
  }
  if (!all(dim(R_est) == dim(R_true))) {
    warning("Rotation matrices must have the same dimensions.")
    return(NA_real_)
  }
  if (nrow(R_est) != ncol(R_est)) {
    warning("Matrices must be square.")
    return(NA_real_)
  }
  k_dim <- nrow(R_est)

  # Calculate relative rotation: M_rel = R_true^T %*% R_est
  # This measures how R_est is rotated relative to R_true.
  M_rel <- crossprod(R_true, R_est) 

  # Check for perfect alignment first to avoid numerical issues with logm or acos
  if (isTRUE(all.equal(M_rel, diag(k_dim), tolerance = 1e-7))) {
    return(0)
  }

  angle_rad <- NA_real_
  use_fallback <- TRUE

  if (method == "geodesic") {
    if (requireNamespace("expm", quietly = TRUE)) {
      log_M_rel <- safe_logm_internal(M_rel)
      if (!is.null(log_M_rel)) {
        # Geodesic distance on SO(k) is ||logm(M_rel)||_F / sqrt(2)
        angle_rad <- norm(log_M_rel, type = "F") / sqrt(2)
        use_fallback <- FALSE
      } else {
        warning("Matrix logarithm failed for geodesic method. Using fallback trace-based calculation.", call. = FALSE)
      }
    } else {
      warning("Package 'expm' not available for geodesic method. Using fallback trace-based calculation.", call. = FALSE)
    }
  }

  if (use_fallback) {
    # Fallback: Original formula from user snippet, best for SO(3) or as general dissimilarity
    # acos((trace(M_rel) - 1) / 2)
    # This formula is for SO(3). For general k, a more appropriate trace-based one might be:
    # For SO(k), angle = acos( ( tr(M_rel) - (k_dim - 2*floor(k_dim/2)) ) / (2*floor(k_dim/2)) )
    # if k is even and det(M_rel)=1 (det should be 1 if R_true, R_est are SO(k)).
    # However, the (tr(M)-1)/2 is simpler and was the user's original. Let's stick to it as a fallback
    # and clearly state its limitations / interpretation. The pmin/pmax is crucial.
    trace_val <- sum(diag(M_rel))
    
    # Heuristic value for acos, from user's original formula in the toy example context.
    # This value is (cos(theta_1) + ... + cos(theta_p) + (k-2p))/2 where theta_i are principal angles
    # For SO(3), it simplifies to cos(rotation_angle)
    # For SO(2), trace is 2*cos(theta), so (trace-0)/2 = cos(theta)
    # If k_dim = 2, (trace_val - 0)/2 = trace_val/2 = cos(angle)
    # If k_dim = 3, (trace_val - 1)/2 = cos(angle)
    # Let's use the k-dependent formula that generalizes for SO(k) based on principal angles
    # cos_theta = ( trace(R) - (k - 2*floor(k/2) - mod(k,2)*sign(det(R)-1) ) ) / (2*floor(k/2)) is complex.
    # The one suggested in review: (sum(diag(M)) - ncol(M)) / 2 IS NOT QUITE RIGHT.
    # It should be (sum(diag(M)) - (k_dim - 2)) / 2 for SO(k) if k-2 eigenvalues are 1.
    # Let's use the simplest one from the original prompt, which is common for SO(3)
    # and is at least a bounded value for acos. Warn that it's SO(3) specific for accuracy.
    if (k_dim != 3) {
        warning("Fallback trace-based angle is most accurate for 3x3 rotations (SO(3)). ",
                "For k != 3, it serves as a dissimilarity measure.", call. = FALSE)
    }
    raw_value_for_acos <- (trace_val - 1) / 2
    if (k_dim == 2) { # For SO(2), trace(R) = 2*cos(theta), so cos(theta) = trace(R)/2
        raw_value_for_acos <- trace_val / 2
    }

    angle_rad <- acos(pmin(1, pmax(-1, raw_value_for_acos)))
  }

  return(angle_rad * 180 / pi)
}
</file>

<file path="R/procrustes_alignment.R">
#' Solve Orthogonal Procrustes Problem for `R_i` in SO(k)
#'
#' Finds `R_i in SO(k)` that best aligns `A_orig_subj_anchor` to `T_anchor_group`.
#' This is the standard unweighted version.
#'
#' @param A_orig_subj_anchor Numeric matrix (`m x k`), subject's anchor sketch.
#' @param T_anchor_group Numeric matrix (`m x k`), group anchor template.
#' @return Orthogonal rotation matrix `R_i` (`k x k`) with `det(R_i) = 1`.
#' @keywords internal
solve_procrustes_rotation <- function(A_orig_subj_anchor, T_anchor_group) {
  k_dim <- ncol(A_orig_subj_anchor) 
  if (k_dim == 0) return(matrix(0,0,0)) 

  M <- crossprod(A_orig_subj_anchor, T_anchor_group) 
  
  if (all(abs(M) < 1e-14)) {
      warning("Cross-product matrix M in solve_procrustes_rotation is near zero; rotation is ill-defined. Returning identity.")
      return(diag(k_dim))
  }

  svd_M <- svd(M) 
  U_svd <- svd_M$u
  V_svd <- svd_M$v 
  
  R_raw <- V_svd %*% base::t(U_svd) 
  
  # Audit patch: Fast reflection fix using QR decomp sign check
  # More robust than det() for large k and handles reflection correctly
  sign_det <- sign(prod(diag(qr(R_raw)$qr))) # O(k^2) check
  
  R_i <- R_raw
  if (sign_det < 0) {
      # Audit patch: Flip column corresponding to the *smallest* singular value
      j_min_sv <- which.min(svd_M$d)
      V_svd_corrected <- V_svd
      V_svd_corrected[, j_min_sv] <- -V_svd_corrected[, j_min_sv]
      R_i <- V_svd_corrected %*% base::t(U_svd)
      
      # Optional sanity check (can be removed in production)
      # if (abs(det(R_i) - 1.0) > 1e-6) {
      #   warning("Determinant correction failed?")
      # }
  }
  
  # The previous check for det != +/- 1 is removed as the QR sign check is sufficient
  # and handles the reflection properly. Issues with the matrix M itself 
  # (e.g. near singularity) might still lead to unstable rotations, but the 
  # rotation matrix R_i returned will have det = +1.
  
  return(R_i)
}

#' Perform Generalized Procrustes Analysis (GPA) refinement iterations
#'
#' Iteratively refines subject rotations and a group anchor template.
#' Can use weighted Procrustes if task rows are present and omega_mode is specified.
#' If n_refine = 0, computes initial rotations against the mean template but does not update the template.
#'
#' @param A_originals_list List of `(m_parcels + m_tasks) x k` anchor matrices for `N` subjects.
#' @param n_refine Integer, number of refinement iterations.
#' @param k Integer, spectral rank (number of columns in anchor matrices).
#' @param m_parcel_rows Integer, number of rows corresponding to parcel anchors.
#'   Required if `m_task_rows > 0` for weighted Procrustes.
#' @param m_task_rows Integer, number of rows corresponding to task anchors. Default 0.
#'   If > 0, `solve_procrustes_rotation_weighted` is used.
#' @param omega_mode Character string, mode for `solve_procrustes_rotation_weighted`.
#'   Default `"fixed"`. Ignored if `m_task_rows == 0`.
#' @param fixed_omega_weights List, weights for `"fixed"` mode. Passed to weighted solver.
#'   Default `list(parcel = 1.0, condition = 0.5)` is handled by the weighted solver if NULL.
#' @param reliability_scores_list List (parallel to `A_originals_list`), each element a
#'   numeric vector of reliability scores for task anchors. Used if `omega_mode == "adaptive"`
#'   and `m_task_rows > 0`. If `NULL`, adaptive mode in solver may use default behavior.
#' @param scale_omega_trace Logical, passed to weighted solver. Default `TRUE`.
#'
#' @return List: `R_final_list` (list of `k x k` rotation matrices),
#'         `T_anchor_final` (final `(m_parcels+m_tasks) x k` group anchor template).
#' @keywords internal
perform_gpa_refinement <- function(A_originals_list, n_refine, k, 
                                   m_parcel_rows = NULL, m_task_rows = 0,
                                   omega_mode = "fixed", 
                                   fixed_omega_weights = NULL,
                                   reliability_scores_list = NULL,
                                   scale_omega_trace = TRUE) {
  num_subjects <- length(A_originals_list)
  
  if (is.null(m_parcel_rows)) {
    first_valid_A <- NULL
    for (subj_A in A_originals_list) {
        if (!is.null(subj_A) && is.matrix(subj_A) && ncol(subj_A) == k) {
            first_valid_A <- subj_A
            break
        }
    }
    if (!is.null(first_valid_A)) {
        m_total_rows_inferred <- nrow(first_valid_A)
    } else {
        m_total_rows_inferred <- 0 
    }
    if (m_task_rows > 0) stop("m_parcel_rows must be specified if m_task_rows > 0.")
    m_parcel_rows <- m_total_rows_inferred 
  }
  
  m_total_rows <- m_parcel_rows + m_task_rows

  if (k == 0) { 
      T_anchor <- matrix(0, nrow = m_total_rows, ncol = 0)
      R_final_list <- replicate(num_subjects, matrix(0,0,0), simplify=FALSE)
      return(list(R_final_list = R_final_list, T_anchor_final = T_anchor))
  }
  if (m_total_rows == 0 && k > 0){
      T_anchor <- matrix(0, nrow = 0, ncol = k)
      R_final_list <- replicate(num_subjects, diag(k), simplify = FALSE)
      return(list(R_final_list = R_final_list, T_anchor_final = T_anchor))
  }

  valid_A_originals_indices <- sapply(A_originals_list, function(A) {
    !is.null(A) && is.matrix(A) && nrow(A) == m_total_rows && ncol(A) == k
  })
  
  valid_A_originals <- A_originals_list[valid_A_originals_indices]

  if (length(valid_A_originals) > 0) {
      T_anchor <- Reduce(`+`, valid_A_originals) / length(valid_A_originals)
  } else { 
      T_anchor <- matrix(0, nrow = m_total_rows, ncol = k)
      warning("No valid anchor matrices found for initialization. GPA template starts at zero.")
  }

  R_iter_list <- replicate(num_subjects, diag(k), simplify = FALSE)

  # Audit patch: Handle n_refine == 0 by computing initial rotations only
  if (n_refine == 0L) {
      for (i in seq_len(num_subjects)) {
          Ai <- A_originals_list[[i]]
          # Check validity again, although indices were computed above
          if (!valid_A_originals_indices[i]) next 
          
          subj_reliability_scores <- if (!is.null(reliability_scores_list) && length(reliability_scores_list) >= i) reliability_scores_list[[i]] else NULL
          
          if (m_task_rows > 0) {
                 R_iter_list[[i]] <- solve_procrustes_rotation_weighted(
                                       Ai, T_anchor,
                                       m_parcel_rows, m_task_rows,
                                       omega_mode, fixed_omega_weights,
                                       subj_reliability_scores, # Use looked-up value
                                       scale_omega_trace)
          } else {
                 R_iter_list[[i]] <- solve_procrustes_rotation(Ai, T_anchor)
          }
      }
      return(list(R_final_list = R_iter_list, T_anchor_final = T_anchor))
  }
  
  # --- Main Refinement Loop (n_refine > 0) --- 
  for (iter_num in 1:n_refine) {
    rotated_anchors_sum_for_update <- matrix(0, nrow = m_total_rows, ncol = k)
    active_subjects_count_for_update <- 0

    for (i in 1:num_subjects) {
      A_orig_subj <- A_originals_list[[i]]
      
      if (valid_A_originals_indices[i]) { # Use pre-calculated validity
        current_R_i <- NULL
        subj_reliability_scores <- if (!is.null(reliability_scores_list) && length(reliability_scores_list) >= i) reliability_scores_list[[i]] else NULL
          
        if (m_task_rows > 0) {
          current_R_i <- solve_procrustes_rotation_weighted(
                                A_source = A_orig_subj, 
                                T_target = T_anchor, 
                                m_parcel_rows = m_parcel_rows, 
                                m_task_rows = m_task_rows, 
                                omega_mode = omega_mode, 
                                fixed_omega_weights = fixed_omega_weights,
                                reliability_scores = subj_reliability_scores,
                                scale_omega_trace = scale_omega_trace
                              )
        } else {
          current_R_i <- solve_procrustes_rotation(A_orig_subj, T_anchor)
        }
        R_iter_list[[i]] <- current_R_i
        
        rotated_anchors_sum_for_update <- rotated_anchors_sum_for_update + (A_orig_subj %*% current_R_i)
        active_subjects_count_for_update <- active_subjects_count_for_update + 1
      } else {
        # Subject data invalid or missing, R_iter_list[[i]] remains (identity from init or prev iter)
      }
    }
    
    if (active_subjects_count_for_update > 0) {
        T_anchor <- rotated_anchors_sum_for_update / active_subjects_count_for_update
    } else if (m_total_rows > 0 || k > 0) { 
        T_anchor <- matrix(0, nrow = m_total_rows, ncol = k) 
        warning("No valid subjects contributed to GPA template update in iteration %d. Template reset to zeros.", iter_num)
    }
  }
  
  return(list(R_final_list = R_iter_list, T_anchor_final = T_anchor))
}

#' Perform Geometric Generalized Procrustes Analysis (Geo-GPA) on SO(k)
#'
#' Refines a set of rotation matrices and a group anchor template using either
#' an SVD-based iterative Procrustes approach (default) or a variant where the
#' template update is oriented by the Frechet mean of rotations.
#' The SVD mode minimizes the Frobenius distance between rotated subject anchors
#' and the template. The Riemannian mode aims to make the template's orientation
#' consistent with the Frechet mean of subject rotations, which implicitly relates
#' to minimizing intrinsic SO(k) geodesic distances for the rotations' central tendency.
#'
#' @param A_originals_list A list of subject-specific anchor matrices (m_rows x k).
#'   Each matrix `A_i` contains the rows from `U_original_list[[i]]` corresponding
#'   to the `unique_anchor_indices`.
#' @param n_refine Integer, number of GPA refinement iterations. Default: 10.
#' @param k Integer, the dimensionality of the sketch space (number of columns in A_i and R_i).
#' @param m_rows Integer, number of anchor features (number of rows in A_i).
#' @param tol Numeric, tolerance for convergence based on the relative Frobenius norm
#'   change in the template `T_template`. Default: 1e-7.
#' @param rotation_mode Character string, one of `"svd"` (default) or `"riemannian"`.
#'   - `"svd"`: Uses SVD to find the closest SO(k) rotation for each subject to align
#'     `A_i` with `T_template`. `T_template` is the Euclidean mean of `A_i R_i`.
#'     This mode minimizes the sum of squared Frobenius distances `sum(||A_i R_i - T||_F^2)`.
#'   - `"riemannian"`: Individual rotations `R_i` are updated as in `"svd"` mode.
#'     However, `T_template` is updated by first computing the Frechet mean (`R_bar`)
#'     of the current `R_list`, then averaging `A_i R_i R_bar^T` (configurations aligned
#'     to `R_bar`'s frame), and finally rotating this average back by `R_bar`.
#'     This mode seeks to make the template orientation consistent with the intrinsic
#'     mean of the rotation ensemble.
#' @param frechet_mean_options A list of options to pass to `hatsa::frechet_mean_so_k`,
#'   used if `rotation_mode = "riemannian"` or for the final `R_bar_final` computation.
#'   Example: `list(max_iter = 20, tol = 1e-5)`. Defaults are used if not provided.
#' @param verbose Logical, if TRUE, prints progress messages. Default TRUE.
#' @param initial_R_list Optional list of initial k x k rotation matrices. If NULL,
#'   identity matrices are used.
#' @return A list containing:
#'   - `R_final_list`: List of final subject-specific rotation matrices (k x k).
#'   - `T_anchor_final`: The final group anchor template matrix (m_rows x k).
#'   - `R_bar_final`: The Fréchet mean of the final `R_final_list`.
#' @importFrom stats svd det
#' @keywords internal
perform_geometric_gpa_refinement <- function(A_originals_list,
                                             n_refine = 10,
                                             k,
                                             m_rows,
                                             tol = 1e-7,
                                             rotation_mode = c("svd", "riemannian"),
                                             frechet_mean_options = list(),
                                             verbose = TRUE,
                                             initial_R_list = NULL) {

  rotation_mode <- match.arg(rotation_mode)
  N <- length(A_originals_list)

  if (N == 0) {
    warning("A_originals_list is empty. Cannot perform GPA.")
    return(list(R_final_list = list(), T_anchor_final = matrix(NA, nrow = m_rows, ncol = k), R_bar_final = diag(k)))
  }

  # Initialize R_list
  R_list <- if (!is.null(initial_R_list) && length(initial_R_list) == N) {
    initial_R_list
  } else {
    replicate(N, diag(k), simplify = FALSE)
  }
  
  # Ensure initial R_list contains SO(k) matrices
  for(i in 1:N){
    if (!is.null(R_list[[i]]) && all(dim(R_list[[i]]) == c(k,k))) {
        svd_R <- svd(R_list[[i]])
        R_proj <- svd_R$u %*% t(svd_R$v)
        if (det(R_proj) < 0) {
            V_prime <- svd_R$v
            V_prime[,k] <- -V_prime[,k]
            R_proj <- svd_R$u %*% t(V_prime)
        }
        R_list[[i]] <- R_proj
    } else {
        R_list[[i]] <- diag(k) # Fallback if invalid initial matrix
    }
  }

  # Initial template T_template
  T_sum_initial <- matrix(0, nrow = m_rows, ncol = k)
  valid_configs_count_initial <- 0
  for (i in 1:N) {
    if (!is.null(A_originals_list[[i]]) && !is.null(R_list[[i]])) {
      T_sum_initial <- T_sum_initial + (A_originals_list[[i]] %*% R_list[[i]])
      valid_configs_count_initial <- valid_configs_count_initial + 1
    }
  }
  if (valid_configs_count_initial == 0) stop("No valid initial configurations (A_i R_i) to compute initial template.")
  T_template <- T_sum_initial / valid_configs_count_initial
  
  R_bar_current_iter <- NULL # Used for Riemannian mode's initial_mean for frechet_mean_so_k

  for (iter in seq_len(n_refine)) {
    T_old <- T_template

    # --- 1. Update R_i for each subject --- 
    for (i in 1:N) {
      Ai <- A_originals_list[[i]]
      if (is.null(Ai) || !is.matrix(Ai) || nrow(Ai) != m_rows || ncol(Ai) != k) {
        if(verbose) message(sprintf("Skipping rotation update for subject %d due to invalid A_i.", i))
        R_list[[i]] <- diag(k) # Keep as identity or previous if A_i is problematic
        next
      }
      M <- crossprod(Ai, T_template) # t(Ai) %*% T_template (k x k matrix)
      svd_M <- svd(M)
      R_new_i <- svd_M$u %*% t(svd_M$v)
      
      # Ensure R_new_i is in SO(k) (det(R) = 1)
      if (det(R_new_i) < 0) {
        V_prime <- svd_M$v
        V_prime[,k] <- -V_prime[,k] # Flip the sign of the last column of V
        R_new_i <- svd_M$u %*% t(V_prime)
      }
      R_list[[i]] <- R_new_i
    }

    # --- 2. Update T_template --- 
    if (rotation_mode == "svd") {
      T_sum <- matrix(0, nrow = m_rows, ncol = k)
      valid_configs_count <- 0
      for (i in 1:N) {
          if (!is.null(A_originals_list[[i]]) && !is.null(R_list[[i]])) {
            T_sum <- T_sum + (A_originals_list[[i]] %*% R_list[[i]])
            valid_configs_count <- valid_configs_count + 1
          }
      }
      if (valid_configs_count == 0) {
          warning("No valid configurations to update template in iteration ", iter, ". Using previous template.")
          T_template <- T_old
      } else {
          T_template <- T_sum / valid_configs_count
      }
    } else { # rotation_mode == "riemannian"
      # Compute R_bar = FrechetMean(R_list)
      current_frechet_opts <- list(
          k_dim = k,
          max_iter = frechet_mean_options$max_iter %||% 20, 
          tol = frechet_mean_options$tol %||% 1e-5, 
          initial_mean = frechet_mean_options$initial_mean %||% R_bar_current_iter %||% R_list[[1]],
          project_to_SOk = TRUE
      )
      if(is.null(current_frechet_opts$initial_mean)) current_frechet_opts$initial_mean <- diag(k)

      R_bar <- do.call(frechet_mean_so_k, c(list(R_list = R_list), current_frechet_opts))
      R_bar_current_iter <- R_bar # Save for next iteration's initial_mean hint
      
      T_sum_aligned <- matrix(0, nrow = m_rows, ncol = k)
      valid_configs_count_riem <- 0
      if (!is.null(R_bar) && all(dim(R_bar) == c(k,k))) {
            R_bar_t <- t(R_bar)
            for (i in 1:N) {
                if (!is.null(A_originals_list[[i]]) && !is.null(R_list[[i]])) {
                    T_sum_aligned <- T_sum_aligned + (A_originals_list[[i]] %*% R_list[[i]] %*% R_bar_t)
                    valid_configs_count_riem <- valid_configs_count_riem + 1
                }
            }
            if (valid_configs_count_riem == 0) {
                warning("No valid configurations for Riemannian template update in iter ", iter, ". Using previous template.")
                T_template <- T_old
            } else {
                T_template <- (T_sum_aligned / valid_configs_count_riem) %*% R_bar
            }
      } else {
          warning("R_bar computation failed or invalid in Riemannian mode iter ", iter, ". Using previous template.")
          T_template <- T_old
      }
    }

    # --- 3. Convergence Check --- 
    delta <- norm(T_template - T_old, type = "F") / max(1e-9, norm(T_old, type = "F"))
    if (verbose) {
      message(sprintf("Geo-GPA iter %d/%d (%s mode): Template change (Delta) = %.2e", 
                      iter, n_refine, rotation_mode, delta))
    }
    if (delta < tol) {
      if (verbose) message("Geo-GPA converged.")
      break
    }
    if (iter == n_refine && verbose) {
        message("Geo-GPA reached max iterations without specified convergence.")
    }
  }

  # --- Final R_bar (Fréchet mean of final R_list) --- 
  final_frechet_opts <- list(
      k_dim = k,
      max_iter = frechet_mean_options$max_iter %||% 50, # More iter for final
      tol = frechet_mean_options$tol %||% 1e-7,
      initial_mean = frechet_mean_options$initial_mean %||% R_bar_current_iter %||% (if(length(R_list)>0) R_list[[1]] else diag(k)),
      project_to_SOk = TRUE
  )
   if(is.null(final_frechet_opts$initial_mean)) final_frechet_opts$initial_mean <- diag(k)

  R_bar_final <- if (length(R_list) > 0) {
      do.call(frechet_mean_so_k, c(list(R_list = R_list), final_frechet_opts))
  } else {
      diag(k)
  }
  if (is.null(R_bar_final)) R_bar_final <- diag(k) # Fallback if frechet_mean_so_k fails

  return(list(R_final_list = R_list, 
              T_anchor_final = T_template, 
              R_bar_final = R_bar_final))
}

# Helper for Frechet mean options default (borrowed from purrr::`%||%`)
`%||%` <- function(x, y) if (is.null(x)) y else x
</file>

<file path="R/projection_helpers.R">
#' @title Project Features onto a Spectral Basis
#' @description Projects a feature matrix onto a given spectral basis. Handles potential
#'   transposition of the feature matrix and uses the appropriate orthogonal projection
#'   formula based on whether the basis is orthonormal.
#'
#' @param feature_matrix A numeric matrix representing features. It can be in
#'   `V_p x C` format (parcels x features/conditions) or `C x V_p` format.
#'   The function will attempt to orient it correctly based on `U_basis`.
#'   If `feature_matrix` is a sparse matrix (from `Matrix` package) and large
#'   (e.g., `prod(dim(.)) > 1e7`), the function will stop; otherwise, it's coerced to dense.
#' @param U_basis A numeric matrix representing the spectral basis, typically with
#'   dimensions `V_p x k_dims_basis` (parcels x basis dimensions).
#'   If `U_basis` is a sparse matrix and large, the function will stop; otherwise, it's coerced to dense.
#' @param tol_orthonormal A numeric tolerance to check for orthonormality of `U_basis`.
#'   `max(abs(crossprod(U_basis) - I)) / k_dims_basis` is compared against this tolerance.
#'   Default is `sqrt(.Machine$double.eps)`.
#' @param assume_orthonormal Logical. If `TRUE`, `U_basis` is assumed to be orthonormal,
#'   and the check (including rank check and `UtU` calculation) is skipped, 
#'   directly using the faster projection `crossprod(U_basis, features)`.
#'   Default is `FALSE`.
#'
#' @return A numeric (dense) matrix containing the projected features.
#'   If `feature_matrix` was oriented to `V_p x C` (parcels x features/conditions),
#'   the output will be `k_dims_basis x C` (basis dimensions x features/conditions).
#'   If `feature_matrix` was `C x V_p` and was transposed for projection, the output
#'   will be `C x k_dims_basis` to maintain the original feature orientation as rows.
#'
#' @export
#' @examples
#' V_p <- 10
#' k_dims <- 3
#' C <- 5
#' U_basis_ortho <- svd(matrix(rnorm(V_p * k_dims), V_p, k_dims))$u
#' U_basis_non_ortho <- matrix(rnorm(V_p * k_dims), V_p, k_dims)
#'
#' # Feature matrix: V_p x C (parcels x conditions)
#' features1 <- matrix(rnorm(V_p * C), V_p, C)
#' proj1_ortho <- project_features_to_spectral_space(features1, U_basis_ortho)
#' print(dim(proj1_ortho))
#' proj1_non_ortho <- project_features_to_spectral_space(features1, U_basis_non_ortho)
#' print(dim(proj1_non_ortho))
#'
#' # Ambiguous square matrix error
#' V_p_sq <- 7
#' U_basis_sq <- matrix(rnorm(V_p_sq*k_dims), V_p_sq, k_dims)
#' features_sq_ambiguous <- matrix(rnorm(V_p_sq*V_p_sq), V_p_sq, V_p_sq)
#' try(project_features_to_spectral_space(features_sq_ambiguous, U_basis_sq))
#' 
#' # Rank deficient U_basis warning
#' U_rank_def <- matrix(rnorm(V_p * k_dims), V_p, k_dims)
#' if (k_dims > 1) U_rank_def[, k_dims] <- U_rank_def[, 1] # Make last col same as first
#' try(project_features_to_spectral_space(features1, U_rank_def))
#'
project_features_to_spectral_space <- function(feature_matrix, U_basis, 
                                             tol_orthonormal = sqrt(.Machine$double.eps),
                                             assume_orthonormal = FALSE) {
  # Sparse matrix handling with size guard
  sparse_size_threshold <- 1e7 # Heuristic for "large"
  if (inherits(feature_matrix, "sparseMatrix")) {
    if (prod(dim(feature_matrix)) > sparse_size_threshold) {
      stop("feature_matrix is a large sparse Matrix; please convert to dense explicitly if memory allows, or use a sparse-aware projection method.")
    }
    feature_matrix <- as.matrix(feature_matrix)
  }
  if (inherits(U_basis, "sparseMatrix")) {
    if (prod(dim(U_basis)) > sparse_size_threshold) {
      stop("U_basis is a large sparse Matrix; please convert to dense explicitly if memory allows, or use a sparse-aware projection method.")
    }
    U_basis <- as.matrix(U_basis)
  }

  if (!is.matrix(feature_matrix) || !is.numeric(feature_matrix)) {
    stop("feature_matrix must be a numeric matrix.")
  }
  if (!is.matrix(U_basis) || !is.numeric(U_basis)) {
    stop("U_basis must be a numeric matrix.")
  }

  V_p_basis <- nrow(U_basis)
  k_dims_basis <- ncol(U_basis)

  if (V_p_basis == 0) {
    warning("U_basis has 0 rows (V_p = 0). Projection is ill-defined or trivial.")
    if (nrow(feature_matrix) == 0) {
        return(matrix(0, nrow = k_dims_basis, ncol = ncol(feature_matrix)))
    } else if (ncol(feature_matrix) == 0) {
        return(matrix(0, nrow = nrow(feature_matrix), ncol = k_dims_basis))
    } else {
        return(matrix(0,0,0))
    }
  }
  
  if (k_dims_basis == 0) {
    warning("U_basis has 0 columns (k_dims_basis = 0). Resulting projection will have 0 dimensions.")
    if (nrow(feature_matrix) == V_p_basis) {
        return(matrix(0, nrow = 0, ncol = ncol(feature_matrix)))
    } else if (ncol(feature_matrix) == V_p_basis) {
        return(matrix(0, nrow = nrow(feature_matrix), ncol = 0))
    } else { 
        stop(sprintf("Dimensions of feature_matrix (%d x %d) are incompatible with U_basis (%d x %d) when k_dims_basis is 0.", 
                     nrow(feature_matrix), ncol(feature_matrix), V_p_basis, k_dims_basis))
    }
  }

  fm_oriented <- NULL
  input_was_transposed <- FALSE
  
  is_rows_match_Vp <- nrow(feature_matrix) == V_p_basis
  is_cols_match_Vp <- ncol(feature_matrix) == V_p_basis

  if (is_rows_match_Vp && is_cols_match_Vp) {
    stop(sprintf("feature_matrix dimensions (%d x %d) are ambiguous as both match V_p_basis (%d). Please ensure feature_matrix is oriented as V_p x C (parcels x features/conditions). If unsure, or if it is C x V_p where C == V_p, please transpose it first before calling.",
                 nrow(feature_matrix), ncol(feature_matrix), V_p_basis))
  } else if (is_rows_match_Vp) {
    fm_oriented <- feature_matrix 
    input_was_transposed <- FALSE
  } else if (is_cols_match_Vp) {
    fm_oriented <- t(feature_matrix) 
    input_was_transposed <- TRUE
  } else {
    stop(sprintf("feature_matrix dimensions (%d x %d) are incompatible with U_basis dimensions (%d x %d). One dimension of feature_matrix must match nrow(U_basis).",
                 nrow(feature_matrix), ncol(feature_matrix), V_p_basis, k_dims_basis))
  }
  
  if (ncol(fm_oriented) == 0) { 
      warning("Oriented feature matrix has 0 columns (C=0). Resulting projection will have 0 columns.")
      if (input_was_transposed) {
          return(matrix(0, nrow = 0, ncol = k_dims_basis))
      } else {
          return(matrix(0, nrow = k_dims_basis, ncol = 0))
      }
  }

  is_orthonormal <- FALSE
  UtU <- NULL # Define UtU outside so it's available for qr.solve if needed

  if (assume_orthonormal) {
    is_orthonormal <- TRUE
  } else {
    # Check U_basis rank first
    qr_U <- qr(U_basis)
    if (qr_U$rank < k_dims_basis) {
        warning(sprintf("U_basis appears rank deficient (rank %d / %d dimensions). Projection results may be unreliable or reflect a lower-dimensional space.",
                        qr_U$rank, k_dims_basis))
    }
    UtU <- crossprod(U_basis)
    Id_k <- diag(k_dims_basis)
    max_dev <- max(abs(UtU - Id_k))
    # Audit suggestion: Scale tolerance by k_dims_basis
    is_orthonormal <- max_dev < (tol_orthonormal * k_dims_basis)
  }

  projected_features_internal <- NULL 
  if (is_orthonormal) {
    projected_features_internal <- crossprod(U_basis, fm_oriented)
  } else {
    if (is.null(UtU)) UtU <- crossprod(U_basis) # Should be calculated if !assume_orthonormal
    
    projected_features_internal <- tryCatch({
      # Using UtU in qr.solve for (U^T U)^-1 U^T X form
      qr.solve(UtU, crossprod(U_basis, fm_oriented), tol = tol_orthonormal * k_dims_basis) 
    }, error = function(e) {
      stop(paste("Error in qr.solve, likely UtU is singular or ill-conditioned (U_basis may be rank-deficient, or k_dims_basis > V_p_basis and U_basis not full rank). Original error:", e$message))
    })
  }

  if (input_was_transposed) {
    return(t(projected_features_internal)) 
  } else {
    return(projected_features_internal) 
  }
}

#' @title Residualize Matrix on Subspace
#' @description Residualizes the rows of `matrix_to_residualize` with respect to
#'   the row space of `subspace_basis_matrix`. This is equivalent to projecting
#'   each column of `t(matrix_to_residualize)` onto the column space of
#'   `t(subspace_basis_matrix)` and taking the residuals.
#'
#' @param matrix_to_residualize A numeric matrix (e.g., `C x k`) whose rows
#'   are to be residualized. If sparse (from `Matrix` package), will be coerced to dense.
#'   A warning is issued if the matrix is very large (`C*k > 1e7`).
#' @param subspace_basis_matrix A numeric matrix (e.g., `m x k`) whose rows
#'   span the subspace to project out. If sparse, will be coerced to dense.
#'
#' @return A numeric matrix with the same dimensions as `matrix_to_residualize`,
#'   containing the residualized rows.
#'
#' @export
#' @examples
#' # Z_i: 3 conditions, 5-dimensional spectral space (3x5)
#' Z_i <- matrix(rnorm(15), nrow = 3, ncol = 5)
#' # A_parc_i: 2 anchor parcels, 5-dimensional spectral space (2x5)
#' A_parc_i <- matrix(rnorm(10), nrow = 2, ncol = 5)
#' Z_i_res <- residualize_matrix_on_subspace(Z_i, A_parc_i)
#' print(dim(Z_i_res))
#' if (nrow(A_parc_i) > 0 && ncol(A_parc_i) > 0 && qr(t(A_parc_i))$rank > 0) {
#'   # Test orthogonality: Z_i_res %*% t(A_parc_i) should be near zero
#'   print(round(Z_i_res %*% t(A_parc_i), 10))
#' }
#'
residualize_matrix_on_subspace <- function(matrix_to_residualize, subspace_basis_matrix) {
  large_matrix_threshold <- 1e7 # Heuristic for C*k

  if (inherits(matrix_to_residualize, "sparseMatrix")) {
    matrix_to_residualize <- as.matrix(matrix_to_residualize)
  }
  if (inherits(subspace_basis_matrix, "sparseMatrix")) {
    subspace_basis_matrix <- as.matrix(subspace_basis_matrix)
  }

  if (!is.matrix(matrix_to_residualize) || !is.numeric(matrix_to_residualize)) {
    stop("matrix_to_residualize must be a numeric matrix.")
  }
  if (!is.matrix(subspace_basis_matrix) || !is.numeric(subspace_basis_matrix)) {
    stop("subspace_basis_matrix must be a numeric matrix.")
  }

  k_dim_Y <- ncol(matrix_to_residualize)
  k_dim_X <- ncol(subspace_basis_matrix)
  C_dim_Y <- nrow(matrix_to_residualize)
  m_dim_X <- nrow(subspace_basis_matrix)
  
  if (prod(dim(matrix_to_residualize)) > large_matrix_threshold) {
      warning("matrix_to_residualize is large; residualization involves transposes and may be memory intensive.")
  }

  if (k_dim_Y != k_dim_X) {
    stop(sprintf("Number of columns must match: matrix_to_residualize has %d, subspace_basis_matrix has %d.",
                 k_dim_Y, k_dim_X))
  }

  if (k_dim_Y == 0) { 
    return(matrix(0, nrow = C_dim_Y, ncol = 0))
  }
  if (m_dim_X == 0) { 
    return(matrix_to_residualize)
  }
  if (C_dim_Y == 0) { 
    return(matrix(0, nrow = 0, ncol = k_dim_Y))
  }

  Y_eff <- t(matrix_to_residualize)
  X_eff <- t(subspace_basis_matrix) 
  
  # Audit suggestion: NA/Inf check
  if (any(!is.finite(X_eff))) stop("subspace_basis_matrix (after transpose) contains non-finite values.")
  if (any(!is.finite(Y_eff))) stop("matrix_to_residualize (after transpose) contains non-finite values.")
  
  qr_X_eff <- qr(X_eff)
  
  if (qr_X_eff$rank == 0) {
      return(matrix_to_residualize) 
  }
  
  if (qr_X_eff$rank < ncol(X_eff)) {
      warning(sprintf("subspace_basis_matrix (after transpose, %d x %d) appears rank deficient (rank %d < %d columns). Projection is onto a lower-dimensional subspace.",
              nrow(X_eff), ncol(X_eff), qr_X_eff$rank, ncol(X_eff)))
  }
  
  # TODO: Consider orientation switch for big k_dim_Y > C_dim_Y if performance critical
  residuals_eff <- qr.resid(qr_X_eff, Y_eff)

  return(t(residuals_eff))
}

#' @title Build Augmented Anchor Matrix
#' @description Combines parcel anchors and projected task features into a single
#'   augmented anchor matrix for Procrustes alignment.
#'   If inputs have different numeric types (e.g. integer and double), the result
#'   is coerced per R's default rules (usually to the more general type).
#'
#' @param A_parcel_anchors A numeric matrix representing parcel anchors,
#'   typically with dimensions `m_parcels x k_dims`.
#' @param Z_task_features_projected A numeric matrix representing projected task
#'   features (e.g., condition means in the spectral space), typically with
#'   dimensions `m_task_features x k_dims`. Can be `NULL` or have 0 rows if
#'   no task features are to be added.
#'
#' @return An augmented numeric matrix with dimensions
#'   `(m_parcels + m_task_features) x k_dims`. 
#'   Row names are combined from inputs (made unique with `make.unique`). 
#'   Column names are taken from `A_parcel_anchors` if it has them and rows; 
#'   otherwise from `Z_task_features_projected`. An error is thrown if both have
#'   differing non-NULL column names.
#'   If `Z_task_features_projected` is `NULL` or has 0 rows, `A_parcel_anchors` is returned.
#'   If `A_parcel_anchors` has 0 rows and `Z_task_features_projected` is valid,
#'   `Z_task_features_projected` is returned.
#'
#' @export
#' @examples
#' m_parcels <- 5; m_task <- 3; k_dims <- 4
#' A_p <- matrix(rnorm(m_parcels*k_dims), m_parcels, k_dims, dimnames=list(paste0("p",1:m_parcels), paste0("k",1:k_dims)))
#' Z_t <- matrix(rnorm(m_task*k_dims), m_task, k_dims, dimnames=list(paste0("t",1:m_task), paste0("k",1:k_dims)))
#' A_aug <- build_augmented_anchor_matrix(A_p, Z_t)
#' print(dimnames(A_aug))
#'
#' # Differing colnames should error
#' Z_t_bad_colnames <- Z_t; colnames(Z_t_bad_colnames) <- paste0("dim",1:k_dims)
#' try(build_augmented_anchor_matrix(A_p, Z_t_bad_colnames))
#'
build_augmented_anchor_matrix <- function(A_parcel_anchors, Z_task_features_projected) {

  if (!is.matrix(A_parcel_anchors) || !is.numeric(A_parcel_anchors)) {
    stop("A_parcel_anchors must be a numeric matrix.")
  }

  has_A_parc_rows <- nrow(A_parcel_anchors) > 0
  has_Z_task_rows <- !is.null(Z_task_features_projected) && 
                       is.matrix(Z_task_features_projected) && 
                       nrow(Z_task_features_projected) > 0

  A_parc_rownames <- if (has_A_parc_rows) rownames(A_parcel_anchors) else NULL
  A_parc_colnames <- if (has_A_parc_rows) colnames(A_parcel_anchors) else NULL # Could be NULL even if rows exist
  
  Z_task_rownames <- NULL
  Z_task_colnames <- NULL
  if (has_Z_task_rows) {
    if (!is.numeric(Z_task_features_projected)) {
      stop("Z_task_features_projected, if provided and non-empty, must be a numeric matrix.")
    }
    Z_task_rownames <- rownames(Z_task_features_projected)
    Z_task_colnames <- colnames(Z_task_features_projected) # Could be NULL
    
    if (has_A_parc_rows && (ncol(A_parcel_anchors) != ncol(Z_task_features_projected))) {
        stop(sprintf("Number of columns must match: A_parcel_anchors has %d, Z_task_features_projected has %d.",
                   ncol(A_parcel_anchors), ncol(Z_task_features_projected)))
    }
    # Audit suggestion: Strict colname check
    if (!is.null(A_parc_colnames) && !is.null(Z_task_colnames) && !identical(A_parc_colnames, Z_task_colnames)) {
         stop("Column names/order differ between A_parcel_anchors and Z_task_features_projected; please align before binding.")
    }
  }

  if (!has_Z_task_rows) {
    return(A_parcel_anchors) 
  }
  if (!has_A_parc_rows) {
    return(Z_task_features_projected) 
  }
  
  result <- rbind(A_parcel_anchors, Z_task_features_projected)
  
  # Ensure A_parc_rownames and Z_task_rownames are character vectors
  if (is.null(A_parc_rownames)) {
    A_parc_rownames <- paste0("P", 1:nrow(A_parcel_anchors))
  }
  
  if (is.null(Z_task_rownames)) {
    Z_task_rownames <- paste0("T", 1:nrow(Z_task_features_projected))
  }
  
  # Combine row names and make unique
  final_rownames <- make.unique(c(A_parc_rownames, Z_task_rownames), sep = "_")
  if (length(final_rownames) == nrow(result)) { # make.unique can return shorter if inputs were NULL
    rownames(result) <- final_rownames
  }
  
  final_colnames <- A_parc_colnames 
  if (is.null(final_colnames) && !is.null(Z_task_colnames)) {
      final_colnames <- Z_task_colnames
  }
  if (!is.null(final_colnames) && length(final_colnames) == ncol(result)) {
      colnames(result) <- final_colnames
  }
  
  return(result)
}
</file>

<file path="R/riemannian_geometry.R">
#' @keywords internal
.regularize_spd <- function(S, epsilon_spd = 1e-6) {
  if (!is.matrix(S) || !is.numeric(S) || !isSymmetric.matrix(S, tol = 1e-9)) { # Check symmetry
    stop("Input S must be a numeric, symmetric matrix.")
  }
  if (nrow(S) == 0) return(S)

  eig <- eigen(S, symmetric = TRUE)
  vals <- eig$values
  vals_reg <- ifelse(vals < epsilon_spd, epsilon_spd, vals)
  S_reg <- eig$vectors %*% diag(vals_reg) %*% t(eig$vectors)
  S_reg_symm <- (S_reg + t(S_reg)) / 2
  return(S_reg_symm)
}

#' Compute Riemannian Distance Between SPD Matrices (Log-Euclidean Metric)
#'
#' Calculates the Riemannian distance between two symmetric positive-definite (SPD)
#' matrices S1 and S2, using the Log-Euclidean metric as described by
#' Mitteroecker & Bookstein (2008) and Arsigny et al. (2006).
#' `d(S1, S2) = ||logm(S1) - logm(S2)||_F` which, for the M&B formulation, is
#' effectively `sqrt(sum(log(lambda_j(S2^-1 S1))^2))`.
#' This function is a core component of RGEOM-001.
#'
#' @param S1 A numeric, symmetric matrix (p x p).
#' @param S2 A numeric, symmetric matrix (p x p).
#' @param regularize_epsilon A small positive value used for regularization to
#'   ensure matrices are SPD before inversion and eigenvalue computation. Default 1e-6.
#' @param eigenvalue_floor A small positive value to threshold relative eigenvalues
#'   before taking the logarithm, preventing issues with `log(0)`. Default 1e-9.
#'
#' @return The Riemannian distance (a scalar). Returns `NA` if computation fails.
#' @export
#' @examples
#' S1 <- matrix(c(2.3, -0.3, -0.3, 3.6), 2, 2)
#' S2 <- matrix(c(3.7, 1.9, 1.9, 2.8), 2, 2)
#' # riemannian_distance_spd(S1, S2) # Expected: ~1.24156 (Need to import MASS for ginv)
#'
#' # Identical matrices
#' # riemannian_distance_spd(S1, S1) # Expected: 0
riemannian_distance_spd <- function(S1, S2, regularize_epsilon = 1e-6, eigenvalue_floor = 1e-9) {
  if (!is.matrix(S1) || !is.matrix(S2) ||
      !is.numeric(S1) || !is.numeric(S2) ||
      !isSymmetric.matrix(S1, tol = 1e-9) || !isSymmetric.matrix(S2, tol = 1e-9) ||
      nrow(S1) != ncol(S1) || nrow(S2) != ncol(S2) ||
      nrow(S1) != nrow(S2)) {
    stop("S1 and S2 must be numeric, square, symmetric matrices of the same dimension.")
  }
  p <- nrow(S1)
  if (p == 0) return(0)

  S1_reg <- .regularize_spd(S1, regularize_epsilon)
  S2_reg <- .regularize_spd(S2, regularize_epsilon)

  # Use the generalized eigenvalue problem: eigen(S1, S2)
  # This is guaranteed to yield real, positive eigenvalues for SPD matrices
  eig_result <- tryCatch({
    eigen(S1_reg, S2_reg, symmetric = TRUE, only.values = TRUE)
  }, error = function(e) {
    warning(sprintf("Generalized eigenvalue problem failed: %s", e$message))
    return(NULL)
  })

  if (is.null(eig_result)) {
    warning("Generalized eigenvalue computation failed. Returning NA.")
    return(NA_real_)
  }

  relative_eigenvalues <- eig_result$values
  # Filter for positive eigenvalues above the floor
  valid_eigenvalues <- relative_eigenvalues[relative_eigenvalues > eigenvalue_floor]

  if (length(valid_eigenvalues) == 0) {
    if (all.equal(S1_reg, S2_reg, tolerance = regularize_epsilon * 10)) return(0.0)
    warning("No valid positive relative eigenvalues found after flooring. Distance is NA.")
    return(NA_real_)
  }

  if (length(valid_eigenvalues) < p && interactive()) {
    message(sprintf("RiemannianDist: Only %d / %d relative eigenvalues > %.2e used for distance.",
                    length(valid_eigenvalues), p, eigenvalue_floor))
  }

  log_relative_eigenvalues <- log(valid_eigenvalues)
  distance <- sqrt(sum(log_relative_eigenvalues^2))

  return(distance)
}

#' Matrix Logarithm of an SPD Matrix
#'
#' Computes the principal matrix logarithm of a symmetric positive-definite (SPD)
#' matrix. The input matrix is first regularized to ensure positive definiteness.
#'
#' @param S A numeric, symmetric matrix.
#' @param regularize_epsilon Epsilon for regularization. Default 1e-6.
#' @return The matrix logarithm of S (a symmetric matrix).
#' @export
#' @importFrom expm logm
#' @examples
#' S1 <- matrix(c(2.3, -0.3, -0.3, 3.6), 2, 2)
#' logS1 <- matrix_logm_spd(S1)
#' print(logS1)
matrix_logm_spd <- function(S, regularize_epsilon = 1e-6) {
  if (!is.matrix(S) || !is.numeric(S) || !isSymmetric.matrix(S, tol = 1e-9)) {
    stop("Input S must be a numeric, symmetric matrix.")
  }
  S_reg <- .regularize_spd(S, regularize_epsilon)
  eig <- eigen(S_reg, symmetric = TRUE)
  vals <- pmax(eig$values, regularize_epsilon)
  log_S <- eig$vectors %*% diag(log(vals)) %*% t(eig$vectors)
  log_S_symm <- (log_S + t(log_S)) / 2
  return(log_S_symm)
}

#' Matrix Exponential of a Symmetric Matrix
#'
#' Computes the matrix exponential of a symmetric matrix. The result is an SPD matrix.
#'
#' @param S_symm A numeric, symmetric matrix (e.g., a matrix in the tangent space).
#' @return The matrix exponential of S_symm (an SPD matrix).
#' @export
#' @importFrom expm expm
#' @examples
#' S1 <- matrix(c(2.3, -0.3, -0.3, 3.6), 2, 2)
#' logS1 <- matrix_logm_spd(S1)
#' S1_reconstructed <- matrix_expm_spd(logS1)
#' # all.equal(S1, S1_reconstructed) # Should be TRUE, up to numerical precision
matrix_expm_spd <- function(S_symm) {
  if (!is.matrix(S_symm) || !is.numeric(S_symm) || !isSymmetric.matrix(S_symm, tol = 1e-9)) {
    stop("Input S_symm must be a numeric, symmetric matrix.")
  }
  eig <- eigen(S_symm, symmetric = TRUE)
  exp_S <- eig$vectors %*% diag(exp(eig$values)) %*% t(eig$vectors)
  exp_S_symm <- (exp_S + t(exp_S)) / 2
  return(.regularize_spd(exp_S_symm))
}

#' Matrix Square Root of an SPD Matrix
#'
#' Computes the principal matrix square root of a symmetric positive-definite (SPD)
#' matrix. The input matrix is first regularized to ensure positive definiteness.
#' The matrix square root S_sqrt is such that S_sqrt %*% S_sqrt = S.
#'
#' @param S A numeric, symmetric matrix.
#' @param regularize_epsilon Epsilon for regularization. Default 1e-6.
#' @return The matrix square root of S (an SPD matrix).
#' @export
#' @importFrom expm sqrtm
#' @examples
#' S1 <- matrix(c(2.3, -0.3, -0.3, 3.6), 2, 2)
#' S1_sqrt <- matrix_sqrt_spd(S1)
#' # S1_reconstructed_from_sqrt <- S1_sqrt %*% S1_sqrt
#' # all.equal(S1, S1_reconstructed_from_sqrt) # Should be TRUE
matrix_sqrt_spd <- function(S, regularize_epsilon = 1e-6) {
  if (!is.matrix(S) || !is.numeric(S) || !isSymmetric.matrix(S, tol = 1e-9)) {
    stop("Input S must be a numeric, symmetric matrix.")
  }
  S_reg <- .regularize_spd(S, regularize_epsilon)
  eig <- eigen(S_reg, symmetric = TRUE)
  vals <- pmax(eig$values, regularize_epsilon)
  sqrt_S <- eig$vectors %*% diag(sqrt(vals)) %*% t(eig$vectors)
  sqrt_S_symm <- (Re(sqrt_S) + t(Re(sqrt_S))) / 2
  return(.regularize_spd(sqrt_S_symm, regularize_epsilon))
}

#' Affine-Invariant Riemannian Metric (AIRM) Distance
#'
#' Computes the Affine-Invariant Riemannian Metric (AIRM) distance between two
#' symmetric positive-definite (SPD) matrices S1 and S2.
#' The distance is defined as: `||logm(S1^(-1/2) %*% S2 %*% S1^(-1/2))||_F`,
#' where `||.||_F` is the Frobenius norm.
#'
#' @param S1 A numeric, symmetric positive-definite matrix.
#' @param S2 A numeric, symmetric positive-definite matrix.
#' @param regularize_epsilon Epsilon for regularization. Default 1e-6.
#' @return The AIRM distance (a non-negative scalar).
#' @export
#' @examples
#' S1 <- matrix(c(2.3, -0.3, -0.3, 3.6), 2, 2)
#' S2 <- matrix(c(3.7, 1.9, 1.9, 2.8), 2, 2)
#' # dist_airm <- airm_distance(S1, S2)
#' # print(dist_airm)
#' # dist_logeuclidean <- riemannian_distance_spd(S1, S2)
#' # print(dist_logeuclidean) # Note: AIRM and LogEuclidean are different metrics.
airm_distance <- function(S1, S2, regularize_epsilon = 1e-6) {
  if (!is.matrix(S1) || !is.numeric(S1) || !isSymmetric.matrix(S1, tol = 1e-9) ||
      !is.matrix(S2) || !is.numeric(S2) || !isSymmetric.matrix(S2, tol = 1e-9) ||
      nrow(S1) != ncol(S1) || nrow(S2) != ncol(S2) || nrow(S1) != nrow(S2)) {
    stop("S1 and S2 must be numeric, square, symmetric matrices of the same dimension.")
  }

  # Regularize inputs to ensure they are SPD
  S1_reg <- .regularize_spd(S1, regularize_epsilon)
  S2_reg <- .regularize_spd(S2, regularize_epsilon)

  # Compute S1_reg^(-1/2)
  # S1_sqrt_inv <- inv(S1_reg^(1/2))
  S1_sqrt <- matrix_sqrt_spd(S1_reg, regularize_epsilon)
  
  S1_sqrt_inv <- tryCatch(solve(S1_sqrt),
                          error = function(e) {
                            warning("solve(S1_sqrt) failed, trying pseudo-inverse. AIRM distance may be less stable. Error: ", e$message)
                            if (!requireNamespace("MASS", quietly = TRUE)) {
                                stop("MASS package needed for ginv fallback but not available. Original error for solve(): ", e$message)
                            }
                            MASS::ginv(S1_sqrt)
                          })

  # Compute the argument of logm: S1_sqrt_inv %*% S2_reg %*% S1_sqrt_inv
  # This intermediate matrix should also be SPD.
  inner_matrix <- S1_sqrt_inv %*% S2_reg %*% S1_sqrt_inv
  inner_matrix_reg <- .regularize_spd(inner_matrix, regularize_epsilon) # Regularize before logm
  
  # Compute logm of the inner matrix
  log_inner_matrix <- matrix_logm_spd(inner_matrix_reg, regularize_epsilon)
  
  # Frobenius norm of the result: sqrt(sum(diag(t(X) %*% X))) or sqrt(sum(X^2)) for symmetric X
  # Since log_inner_matrix is symmetric:
  distance <- sqrt(sum(log_inner_matrix^2))
  
  return(distance)
}

#' Log Map for SPD Matrices (Log-Euclidean Metric)
#'
#' Projects SPD matrix S2 to the tangent space of SPD matrix S1
#' using the Log-Euclidean metric.
#' The tangent vector is `logm(S2) - logm(S1)`.
#'
#' @param S1 Reference SPD matrix (point on the manifold where the tangent space is anchored).
#' @param S2 SPD matrix to project to the tangent space at S1.
#' @param regularize_epsilon Epsilon for regularization. Default 1e-6.
#' @return A symmetric matrix representing S2 in the tangent space at S1.
#' @export
logmap_spd_logeuclidean <- function(S1, S2, regularize_epsilon = 1e-6) {
  if (!is.matrix(S1) || !isSymmetric.matrix(S1, tol = 1e-9) ||
      !is.matrix(S2) || !isSymmetric.matrix(S2, tol = 1e-9)) {
    stop("S1 and S2 must be symmetric matrices.")
  }
  logS1 <- matrix_logm_spd(S1, regularize_epsilon)
  logS2 <- matrix_logm_spd(S2, regularize_epsilon)
  return(logS2 - logS1)
}

#' Exp Map for SPD Matrices (Log-Euclidean Metric)
#'
#' Maps a tangent vector V (symmetric matrix) from the tangent space at SPD matrix S1
#' back to the SPD manifold using the Log-Euclidean metric.
#' Result is `expm(logm(S1) + V)`.
#'
#' @param S1 SPD matrix (point on manifold where tangent space is anchored).
#' @param V_at_S1 Symmetric matrix (tangent vector at S1).
#' @param regularize_epsilon Epsilon for regularization of S1. Default 1e-6.
#' @return An SPD matrix on the manifold.
#' @export
expmap_spd_logeuclidean <- function(S1, V_at_S1, regularize_epsilon = 1e-6) {
  if (!is.matrix(S1) || !isSymmetric.matrix(S1, tol = 1e-9) ||
      !is.matrix(V_at_S1) || !isSymmetric.matrix(V_at_S1, tol = 1e-9)) {
    stop("S1 and V_at_S1 must be symmetric matrices.")
  }
  logS1 <- matrix_logm_spd(S1, regularize_epsilon)
  # The argument to expm is logm(S1) + V_at_S1, which must be symmetric.
  # logS1 is symmetric, V_at_S1 is assumed symmetric by input validation.
  return(matrix_expm_spd(logS1 + V_at_S1))
}

#' Log Map for SPD Matrices (AIRM Metric)
#'
#' Projects SPD matrix S2 to the tangent space of SPD matrix S1
#' using the Affine-Invariant Riemannian Metric (AIRM).
#' Tangent vector is `S1^(1/2) %*% logm(S1^(-1/2) %*% S2 %*% S1^(-1/2)) %*% S1^(1/2)`.
#'
#' @param S1 Reference SPD matrix.
#' @param S2 SPD matrix to project.
#' @param regularize_epsilon Epsilon for regularization. Default 1e-6.
#' @return A symmetric matrix (tangent vector at S1).
#' @export
logmap_spd_airm <- function(S1, S2, regularize_epsilon = 1e-6) {
   if (!is.matrix(S1) || !isSymmetric.matrix(S1, tol = 1e-9) ||
      !is.matrix(S2) || !isSymmetric.matrix(S2, tol = 1e-9)) {
    stop("S1 and S2 must be symmetric matrices.")
  }
  S1_reg <- .regularize_spd(S1, regularize_epsilon)
  S2_reg <- .regularize_spd(S2, regularize_epsilon)
  
  S1_sqrt <- matrix_sqrt_spd(S1_reg, regularize_epsilon)
  S1_sqrt_inv <- tryCatch(solve(S1_sqrt), error = function(e) {
      warning("solve(S1_sqrt) failed in logmap_spd_airm, trying pseudo-inverse. Error: ", e$message)
      if(!requireNamespace("MASS", quietly=TRUE)) stop("MASS needed for ginv.")
      MASS::ginv(S1_sqrt)
  })
  
  inner_logm_arg <- S1_sqrt_inv %*% S2_reg %*% S1_sqrt_inv
  log_inner <- matrix_logm_spd(inner_logm_arg, regularize_epsilon)
  
  tangent_vector <- S1_sqrt %*% log_inner %*% S1_sqrt
  # Ensure symmetry for the tangent vector
  return((tangent_vector + t(tangent_vector)) / 2)
}

#' Exp Map for SPD Matrices (AIRM Metric)
#'
#' Maps a tangent vector V (symmetric matrix) from the tangent space at SPD matrix S1
#' back to the SPD manifold using the AIRM.
#' Result is `S1^(1/2) %*% expm(S1^(-1/2) %*% V %*% S1^(-1/2)) %*% S1^(1/2)`.
#'
#' @param S1 SPD matrix (point on manifold).
#' @param V_at_S1 Symmetric matrix (tangent vector at S1).
#' @param regularize_epsilon Epsilon for regularization. Default 1e-6.
#' @return An SPD matrix on the manifold.
#' @export
expmap_spd_airm <- function(S1, V_at_S1, regularize_epsilon = 1e-6) {
  if (!is.matrix(S1) || !isSymmetric.matrix(S1, tol = 1e-9) ||
      !is.matrix(V_at_S1) || !isSymmetric.matrix(V_at_S1, tol = 1e-9)) {
    stop("S1 and V_at_S1 must be symmetric matrices.")
  }
  S1_reg <- .regularize_spd(S1, regularize_epsilon)
  V_at_S1_symm <- (V_at_S1 + t(V_at_S1)) / 2 # Ensure V is symmetric
  
  S1_sqrt <- matrix_sqrt_spd(S1_reg, regularize_epsilon)
  S1_sqrt_inv <- tryCatch(solve(S1_sqrt), error = function(e) {
      warning("solve(S1_sqrt) failed in expmap_spd_airm, trying pseudo-inverse. Error: ", e$message)
      if(!requireNamespace("MASS", quietly=TRUE)) stop("MASS needed for ginv.")
      MASS::ginv(S1_sqrt)
  })
  
  inner_expm_arg <- S1_sqrt_inv %*% V_at_S1_symm %*% S1_sqrt_inv
  # The argument to expm must be symmetric for matrix_expm_spd to guarantee SPD output.
  inner_expm_arg_symm <- (inner_expm_arg + t(inner_expm_arg)) / 2
  
  exp_inner <- matrix_expm_spd(inner_expm_arg_symm)
  
  mapped_matrix <- S1_sqrt %*% exp_inner %*% S1_sqrt
  # Ensure the final result is SPD
  return(.regularize_spd((mapped_matrix + t(mapped_matrix)) / 2, regularize_epsilon))
}

#' Fréchet Mean of SPD Matrices
#'
#' Computes the Fréchet mean (Karcher mean) of a list of Symmetric 
#' Positive-Definite (SPD) matrices.
#' This function can use either the Affine-Invariant Riemannian Metric (AIRM)
#' via the `shapes` package (if available and `metric="airm"`), or an iterative
#' algorithm based on the Log-Euclidean metric (`metric="logeuclidean"`).
#'
#' @param S_list A list of SPD matrices.
#' @param metric Character string, either `"logeuclidean"` (default) or `"airm"`.
#' @param regularize_epsilon Epsilon for regularization. Default 1e-6.
#' @param max_iter Maximum number of iterations for Log-Euclidean algorithm. Default 50.
#' @param tol Tolerance for convergence for Log-Euclidean algorithm (Frobenius norm
#'   of the mean tangent vector). Default 1e-5.
#' @param step_size Step size for the gradient descent in Log-Euclidean. Default 0.5.
#' @param init_method For Log-Euclidean, method to initialize the mean: 
#'  `"euclidean"` (Euclidean mean of matrices) or `"first"` (first matrix in list).
#'  Default `"euclidean"`.
#' @param verbose Logical, if TRUE, prints iteration info for Log-Euclidean. Default FALSE.
#' @return The Fréchet mean (an SPD matrix).
#' @export
#' @importFrom stats median
frechet_mean_spd <- function(S_list,
                             metric = c("logeuclidean", "airm"),
                             regularize_epsilon = 1e-6,
                             max_iter = 50, 
                             tol = 1e-5,
                             step_size = 0.5,
                             init_method = c("euclidean", "first"),
                             verbose = FALSE) {
  metric <- match.arg(metric)
  init_method <- match.arg(init_method)

  if (!is.list(S_list) || length(S_list) == 0) {
    stop("S_list must be a non-empty list of matrices.")
  }
  if (!all(sapply(S_list, function(m) is.matrix(m) && isSymmetric.matrix(m, tol = 1e-9)))) {
    stop("All elements in S_list must be symmetric matrices.")
  }
  # Further ensure all matrices are of the same dimension
  p <- nrow(S_list[[1]])
  if (!all(sapply(S_list, function(m) nrow(m) == p && ncol(m) == p))) {
    stop("All matrices in S_list must have the same dimensions.")
  }
  if (p == 0) return(matrix(0,0,0))

  # Regularize all input matrices first
  S_list_reg <- lapply(S_list, function(S) .regularize_spd(S, regularize_epsilon))
  n_matrices <- length(S_list_reg)

  if (metric == "airm") {
    if (!requireNamespace("shapes", quietly = TRUE)) {
      stop("Package 'shapes' needed for AIRM Fréchet mean. Please install it or use metric='logeuclidean'.")
    }
    if (verbose) message("Using AIRM Fréchet mean via shapes::mediancov().")
    # shapes::mediancov expects an array (p, p, N)
    spd_array <- array(unlist(S_list_reg), dim = c(p, p, n_matrices))
    # shapes::mediancov with type="riemann" calculates the Karcher mean using AIRM
    mean_matrix <- shapes::mediancov(spd_array, type = "riemann")$median
    return(.regularize_spd(mean_matrix, regularize_epsilon)) # Ensure final strict SPD
  
  } else if (metric == "logeuclidean") {
    if (verbose) message("Using iterative Log-Euclidean Fréchet mean.")
    
    # Initialization
    current_mean <- if(init_method == "first") {
      S_list_reg[[1]]
    } else { # euclidean mean
      Reduce("+", S_list_reg) / n_matrices
    }
    current_mean <- .regularize_spd(current_mean, regularize_epsilon)

    for (iter in 1:max_iter) {
      tangent_vectors <- lapply(S_list_reg, function(S_i) {
        logmap_spd_logeuclidean(current_mean, S_i, regularize_epsilon)
      })
      
      mean_tangent_vector <- Reduce("+", tangent_vectors) / n_matrices
      norm_mean_tangent <- sqrt(sum(mean_tangent_vector^2)) # Frobenius norm

      if (verbose) {
        message(sprintf("Iter %d: Norm of mean tangent vector = %.2e", iter, norm_mean_tangent))
      }

      if (norm_mean_tangent < tol) {
        if (verbose) message("Converged.")
        break
      }
      
      # Update mean by moving along the mean tangent vector direction
      current_mean <- expmap_spd_logeuclidean(current_mean, mean_tangent_vector * step_size, regularize_epsilon)
      current_mean <- .regularize_spd(current_mean, regularize_epsilon) # Ensure it stays SPD
      
      if (iter == max_iter && verbose) {
        message("Max iterations reached without convergence.")
      }
    }
    return(current_mean)
  }
}

#' Grassmann Distance between Subspaces
#'
#' Computes the Grassmann distance between two k-dimensional subspaces in R^p,
#' represented by orthonormal basis matrices U and V (both p x k).
#' The distance is `sqrt(sum(theta_i^2))`, where `theta_i` are the principal angles
#' between the subspaces.
#' Principal angles `theta_i = acos(s_i)`, where `s_i` are the singular values of `U^T V`.
#'
#' @param U A numeric matrix (p x k) with orthonormal columns.
#' @param V A numeric matrix (p x k) with orthonormal columns.
#' @param tol Tolerance for checking orthonormality. Default 1e-6.
#' @param sv_floor Smallest value for singular values before acos to avoid `acos(>1)`
#'   due to numerical precision. Values are clamped to `[-1+sv_floor, 1-sv_floor]`.
#'   Default 1e-7.
#' @return The Grassmann distance (a non-negative scalar).
#' @export
#' @examples
#' # Example from P. Edelman, T. A. Arias, and A. S. Edelman. "Geometry of algorithms
#' # with orthogonality constraints." SIAM Journal on Matrix Analysis and Applications, 1998.
#' p <- 3; k <- 2
#' U <- qr.Q(qr(matrix(rnorm(p*k), p, k)))
#' V <- qr.Q(qr(matrix(rnorm(p*k), p, k)))
#' # grassmann_distance(U, V)
#'
#' # Identical subspaces
#' # grassmann_distance(U, U) # Should be 0
#'
#' # Orthogonal subspaces (if k <= p/2)
#' if (k <= p/2) {
#'   # V_ortho <- qr.Q(qr(matrix(rnorm(p*k), p, k)), complete=TRUE)[, (k+1):(2*k), drop=FALSE]
#'   # if (ncol(V_ortho) == k) grassmann_distance(U, V_ortho) # Should be sqrt(k * (pi/2)^2)
#' }
grassmann_distance <- function(U, V, tol = 1e-6, sv_floor = 1e-7) {
  if (!is.matrix(U) || !is.numeric(U) || !is.matrix(V) || !is.numeric(V)) {
    stop("U and V must be numeric matrices.")
  }
  if (nrow(U) != nrow(V) || ncol(U) != ncol(V)) {
    stop("U and V must have the same dimensions.")
  }
  p <- nrow(U)
  k <- ncol(U)

  if (k == 0) return(0) # Distance between 0-dim subspaces is 0
  if (k > p) stop("k (number of columns) cannot be greater than p (number of rows).")

  # Check orthonormality (optional, but good for ensuring valid input)
  # U^T U should be I_k
  utu <- crossprod(U)
  vtv <- crossprod(V)
  if (!all.equal(utu, diag(k), tolerance = tol) || 
      !all.equal(vtv, diag(k), tolerance = tol)) {
    warning("Columns of U and/or V may not be perfectly orthonormal. Results might be approximate.")
    # Optional: Orthonormalize them? e.g., U <- qr.Q(qr(U))
  }

  # Singular values of U^T V are cos(theta_i)
  # Ensure U^T V is k x k
  UtV <- crossprod(U, V) 
  s <- svd(UtV, nu = 0, nv = 0)$d

  # Clamp singular values to avoid acos domain errors due to precision
  # s_i should be <= 1. We clamp to [-(1-floor), 1-floor] to be safe with acos.
  s_clamped <- pmin(pmax(s, -(1-sv_floor)), (1-sv_floor))
  
  # Principal angles theta_i = acos(s_i)
  # If any s_i are very close to 1 (e.g. > 1 - sv_floor^2), theta_i is ~0.
  # If any s_i are very close to 0, theta_i is ~pi/2.
  principal_angles <- acos(s_clamped)
  
  # Grassmann distance = sqrt(sum(theta_i^2))
  distance <- sqrt(sum(principal_angles^2))
  
  return(distance)
}

#' Compute Fréchet Mean of Rotation Matrices on SO(k)
#'
#' Calculates the Fréchet mean (geometric mean) of a list of k x k rotation
#' matrices. The Fréchet mean is the rotation matrix R_bar that minimizes the
#' sum of squared geodesic distances to all rotation matrices in the list:
#' \code{R_bar = argmin_R sum_i d(R, R_i)^2}.
#' This implementation uses an iterative algorithm involving logarithmic and
#' exponential maps on SO(k).
#'
#' @param R_list A list of k x k matrices, expected to be rotation matrices (in SO(k) or O(k)).
#'   The function attempts to work with matrices close to SO(k).
#' @param k_dim Integer, the dimension of the rotation matrices (e.g., 3 for SO(3)).
#'   If NULL (default), it's inferred from the first valid matrix in `R_list`.
#' @param max_iter Integer, the maximum number of iterations for the algorithm (default: 50).
#' @param tol Numeric, the tolerance for convergence. The algorithm stops when the
#'   Frobenius norm of the mean tangent vector is below this tolerance (default: 1e-7).
#' @param initial_mean Optional. A k x k matrix to use as the initial estimate for the mean.
#'   If NULL, the first valid matrix in `R_list` is used, or an identity matrix if none are valid.
#' @param project_to_SOk Logical. If TRUE (default), after each update, the new mean estimate
#'  is projected to the closest matrix in SO(k) using SVD. This helps maintain numerical stability
#'  and ensures the result is indeed in SO(k).
#'
#' @return A k x k matrix representing the Fréchet mean of the input rotation matrices.
#'   Returns an identity matrix of appropriate dimension if `R_list` is empty, contains no valid
#'   matrices, or if `k_dim` cannot be determined.
#'
#' @export
#' @importFrom expm logm expm
#' @examples
#' # Example for SO(3)
#' if (requireNamespace("expm", quietly = TRUE)) {
#'   R1 <- matrix(c(1,0,0, 0,cos(0.1),-sin(0.1), 0,sin(0.1),cos(0.1)), 3, 3)
#'   R2 <- matrix(c(cos(0.2),-sin(0.2),0, sin(0.2),cos(0.2),0, 0,0,1), 3, 3)
#'   R_list_so3 <- list(R1, R2)
#'   # frechet_mean_so_k(R_list_so3) # k_dim inferred
#'   # frechet_mean_so_k(R_list_so3, k_dim = 3)
#' }
#'
#' # Example for SO(2)
#' if (requireNamespace("expm", quietly = TRUE)) {
#'   theta1 <- pi/4
#'   R1_so2 <- matrix(c(cos(theta1), -sin(theta1), sin(theta1), cos(theta1)), 2, 2)
#'   theta2 <- pi/3
#'   R2_so2 <- matrix(c(cos(theta2), -sin(theta2), sin(theta2), cos(theta2)), 2, 2)
#'   # frechet_mean_so_k(list(R1_so2, R2_so2))
#' }
.so_logm_closed_form <- function(R) {
  k <- nrow(R)
  if (k == 2) {
    # SO(2): logm(R) = theta * [0 -1; 1 0], theta = atan2(R[2,1], R[1,1])
    theta <- atan2(R[2,1], R[1,1])
    return(matrix(c(0, -theta, theta, 0), 2, 2))
  } else if (k == 3) {
    # SO(3): logm(R) = (theta/(2*sin(theta))) * (R - t(R)), theta = acos((tr(R)-1)/2)
    trR <- sum(diag(R))
    theta <- acos(pmin(pmax((trR - 1) / 2, -1), 1))
    if (abs(theta) < 1e-10) return(matrix(0, 3, 3))
    logR <- (theta / (2 * sin(theta))) * (R - t(R))
    return(logR)
  } else {
    stop("Closed-form logm only implemented for SO(2) and SO(3)")
  }
}

frechet_mean_so_k <- function(R_list, k_dim = NULL, max_iter = 50, tol = 1e-7, initial_mean = NULL, project_to_SOk = TRUE) {
  if (!requireNamespace("expm", quietly = TRUE)) {
    stop("Package 'expm' is required for frechet_mean_so_k.")
  }

  # Filter out NULLs and non-matrices
  valid_Rs <- Filter(function(R) is.matrix(R) && !is.null(R), R_list)

  if (length(valid_Rs) == 0) {
    if (!is.null(k_dim) && k_dim > 0) return(diag(k_dim))
    warning("R_list is empty or contains no valid matrices, and k_dim not specified. Returning NULL.")
    return(NULL)
  }

  # Determine k_dim if not provided
  if (is.null(k_dim)) {
    k_dim <- nrow(valid_Rs[[1]])
    if (ncol(valid_Rs[[1]]) != k_dim) {
      warning("First valid matrix in R_list is not square. Cannot infer k_dim. Returning NULL.")
      return(NULL)
    }
  }
  if (k_dim == 0) {
      warning("k_dim is 0. Returning NULL or an empty matrix depending on context for SO(0).")
      return(matrix(0,0,0)) # SO(0) is technically the group with one element, an empty map.
  }

  # Further filter for correct dimensions
  valid_Rs <- Filter(function(R) all(dim(R) == c(k_dim, k_dim)), valid_Rs)
  num_matrices <- length(valid_Rs)

  if (num_matrices == 0) {
    warning(sprintf("No valid %d x %d matrices in R_list. Returning identity.", k_dim, k_dim))
    return(diag(k_dim))
  }

  # Initialize R_mean_current
  if (!is.null(initial_mean) && is.matrix(initial_mean) && all(dim(initial_mean) == c(k_dim, k_dim))) {
    R_mean_current <- initial_mean
  } else {
    R_mean_current <- valid_Rs[[1]]
  }
  # Ensure initial R_mean_current is a valid rotation or close to it for stability
  if (project_to_SOk) {
      svd_init <- svd(R_mean_current)
      R_mean_current <- svd_init$u %*% t(svd_init$v)
      if (det(R_mean_current) < 0) { # Ensure it's SO(k) not just O(k)
          V_prime <- svd_init$v
          V_prime[,k_dim] <- -V_prime[,k_dim]
          R_mean_current <- svd_init$u %*% t(V_prime)
      }
  }

  for (iter in 1:max_iter) {
    tangent_vectors_sum <- matrix(0, nrow = k_dim, ncol = k_dim)
    num_valid_tangents <- 0
    logm_failures <- 0

    for (i in 1:num_matrices) {
      R_i <- valid_Rs[[i]]
      arg_logm <- crossprod(R_mean_current, R_i) # t(R_mean_current) %*% R_i
      tangent_i <- NULL
      if (k_dim == 2 || k_dim == 3) {
        # Use closed-form logm for SO(2)/SO(3)
        try({
          tangent_i <- .so_logm_closed_form(arg_logm)
        }, silent = TRUE)
      }
      if (is.null(tangent_i)) {
        # Fallback to expm::logm
        tangent_i <- tryCatch(expm::logm(arg_logm, method="Higham08.b"), error = function(e) NULL)
      }
      if (!is.null(tangent_i) && !all(tangent_i == 0)) {
        tangent_vectors_sum <- tangent_vectors_sum + tangent_i
        num_valid_tangents <- num_valid_tangents + 1
      } else if (all(arg_logm == diag(k_dim))) {
        num_valid_tangents <- num_valid_tangents + 1
      } else {
        logm_failures <- logm_failures + 1
      }
    }
    if (logm_failures > 0) {
      warning(sprintf("frechet_mean_so_k: %d logm failures in iteration %d (skipped in mean).", logm_failures, iter))
    }
    # If all logm operations failed or resulted in zero (e.g. all R_i are R_mean_current)
    if (num_valid_tangents == 0) {
        if (all(tangent_vectors_sum == 0)) break # Converged if sum is zero
        warning("No valid tangent vectors could be computed in an iteration of frechet_mean_so_k. Returning current mean.")
        break 
    }
    mean_tangent <- tangent_vectors_sum / num_valid_tangents
    # Update R_mean: R_mean_new = R_mean_current %*% expm(mean_tangent)
    R_mean_new <- R_mean_current %*% expm::expm(mean_tangent)
    if (project_to_SOk) {
      # Normalize R_mean_new to be strictly in SO(k) using SVD
      svd_R_mean_new <- svd(R_mean_new)
      R_mean_new_proj <- svd_R_mean_new$u %*% t(svd_R_mean_new$v)
      if (det(R_mean_new_proj) < 0) { # Ensure it's SO(k) not just O(k)
          V_prime <- svd_R_mean_new$v
          V_prime[,k_dim] <- -V_prime[,k_dim] # Flip last column of V
          R_mean_new_proj <- svd_R_mean_new$u %*% t(V_prime)
      }
      R_mean_new <- R_mean_new_proj
    }
    # Check for convergence: norm of the mean_tangent vector
    if (norm(mean_tangent, type = "F") < tol) {
      R_mean_current <- R_mean_new
      break
    }
    R_mean_current <- R_mean_new
    if (iter == max_iter) {
      warning(sprintf("Fréchet mean on SO(%d) did not converge after %d iterations.", k_dim, max_iter))
    }
  }
  return(R_mean_current)
}

#' Compute Bures-Wasserstein Barycenter of SPD Matrices
#'
#' Calculates the Bures-Wasserstein (BW) barycenter of a list of symmetric
#' positive-definite (SPD) matrices. The BW barycenter is the geodesic mean
#' under the BW metric. This function implements an iterative algorithm.
#'
#' @param S_list A list of SPD matrices (p x p).
#' @param weights Optional. A numeric vector of non-negative weights for each matrix
#'   in `S_list`. If NULL (default), uniform weights (1/N) are used. Must sum to 1
#'   if provided, or will be normalized.
#' @param initial_mean Optional. A p x p SPD matrix to use as the initial estimate
#'   for the barycenter. If NULL, the (weighted) arithmetic mean of `S_list`
#'   is used after regularization.
#' @param max_iter Integer, maximum number of iterations for the fixed-point algorithm.
#'   Default: 50.
#' @param tol Numeric, tolerance for convergence. The algorithm stops when the
#'   Frobenius norm of the difference between successive estimates of the barycenter
#'   is below this tolerance. Default: 1e-7.
#' @param regularize_epsilon Numeric, small positive value for regularizing input
#'   matrices and intermediate results to ensure positive definiteness. Default: 1e-6.
#' @param verbose Logical, if TRUE, prints iteration information. Default: FALSE.
#' @param damping Numeric, damping factor for the update step. Default 0.5.
#'
#' @return A p x p SPD matrix representing the Bures-Wasserstein barycenter.
#'   Returns NULL if computation fails or inputs are invalid.
#' @export
#' @examples
#' # S1 <- matrix(c(2,1,1,2), 2,2)
#' # S2 <- matrix(c(3,0,0,3), 2,2)
#' # S_list_bw <- list(S1, S2)
#' # bw_mean <- bures_wasserstein_barycenter(S_list_bw, verbose = TRUE)
#' # print(bw_mean)
#' 
#' # Weighted example
#' # S3 <- matrix(c(1.5,0.5,0.5,1.5), 2,2)
#' # bw_mean_weighted <- bures_wasserstein_barycenter(list(S1,S2,S3), weights=c(0.5,0.25,0.25))
#' # print(bw_mean_weighted)
bures_wasserstein_barycenter <- function(S_list,
                                         weights = NULL,
                                         initial_mean = NULL,
                                         max_iter = 50,
                                         tol = 1e-7,
                                         regularize_epsilon = 1e-6,
                                         verbose = FALSE,
                                         damping = 0.5) {

  # --- Input Validation and Initialization ---
  if (!is.list(S_list) || length(S_list) == 0) {
    stop("S_list must be a non-empty list of matrices.")
  }
  if (!all(sapply(S_list, function(m) is.matrix(m) && isSymmetric.matrix(m, tol = 1e-9)))) {
    stop("All elements in S_list must be symmetric matrices.")
  }
  p <- nrow(S_list[[1]])
  if (p == 0) return(matrix(0,0,0))
  if (!all(sapply(S_list, function(m) nrow(m) == p && ncol(m) == p))) {
    stop("All matrices in S_list must have the same dimensions (p x p).")
  }

  N <- length(S_list)

  # Validate and normalize weights
  if (is.null(weights)) {
    weights <- rep(1/N, N)
  } else {
    if (!is.numeric(weights) || length(weights) != N || any(weights < 0)) {
      stop("Weights must be a numeric vector of non-negative values, same length as S_list.")
    }
    sum_w <- sum(weights)
    if (abs(sum_w - 1.0) > 1e-6 && sum_w > 1e-9) { # Normalize if not sum to 1 (and sum is not zero)
      if (verbose) message("Normalizing weights to sum to 1.")
      weights <- weights / sum_w
    } else if (sum_w < 1e-9) {
      stop("Sum of weights is too close to zero.")
    }
  }

  # Regularize all input matrices
  S_list_reg <- lapply(S_list, function(S) .regularize_spd(S, regularize_epsilon))

  # Initialize barycenter M_current
  M_current <- initial_mean
  if (is.null(M_current)) {
    # Weighted arithmetic mean as initial guess
    M_sum <- matrix(0, p, p)
    for (i in 1:N) {
      M_sum <- M_sum + weights[i] * S_list_reg[[i]]
    }
    M_current <- M_sum # No division by N as weights sum to 1
  } else {
    if (!is.matrix(M_current) || !isSymmetric.matrix(M_current, tol = 1e-9) || 
        nrow(M_current) != p || ncol(M_current) != p) {
      stop("Provided initial_mean is not a valid p x p symmetric matrix.")
    }
  }
  M_current <- .regularize_spd(M_current, regularize_epsilon)

  # --- Iterative Algorithm (Fixed-Point Iteration) ---
  # Based on Bures-Wasserstein barycenter fixed point equation, e.g.,
  # M_new = (sum_i w_i * (M_current^(1/2) S_i M_current^(1/2))^(1/2) ) M_current^(-1/2) ... and then symmetrize + square
  # Or simpler (from Bhatia, Positive Definite Matrices, Chapter X, or Moakher 2005):
  # M_k+1 = M_k^(1/2) ( sum w_i (M_k^(-1/2) S_i M_k^(-1/2))^(1/2) )^2 M_k^(1/2)
  # This can be unstable. A common iterative scheme is:
  # T_k = sum_i w_i * (M_k^(1/2) S_i M_k^(1/2))^(1/2)
  # M_k+1 = M_k^(-1/2) T_k^2 M_k^(1/2)

  if (verbose) message_stage(sprintf("Starting BW Barycenter iteration (max_iter=%d, tol=%.2e)...", max_iter, tol), interactive_only = TRUE)

  for (iter in 1:max_iter) {
    M_old <- M_current
    M_current_sqrt <- matrix_sqrt_spd(M_current, regularize_epsilon)
    M_current_inv_sqrt <- tryCatch(solve(M_current_sqrt), error = function(e) {
        warning("solve(M_current_sqrt) failed, trying pseudo-inverse. BW barycenter may be less stable.")
        if (!requireNamespace("MASS", quietly = TRUE)) stop("MASS package needed for ginv fallback.")
        MASS::ginv(M_current_sqrt)
    })

    if (any(is.na(M_current_inv_sqrt))) {
        warning("M_current_inv_sqrt contains NAs. Stopping BW iteration.")
        return(M_old) # Or NULL
    }

    T_k_sum <- matrix(0, p, p)
    for (i in 1:N) {
      transformed_S_i <- M_current_inv_sqrt %*% S_list_reg[[i]] %*% M_current_inv_sqrt
      transformed_S_i_reg <- .regularize_spd(transformed_S_i, regularize_epsilon) 
      sqrt_transformed_S_i <- matrix_sqrt_spd(transformed_S_i_reg, regularize_epsilon)
      T_k_sum <- T_k_sum + weights[i] * sqrt_transformed_S_i
    }
    
    # Symmetrize T_k_sum before squaring
    T_k_sym <- (T_k_sum + t(T_k_sum)) / 2
    # Update M_current: M_k+1 = M_k^(1/2) T_k_sym^2 M_k^(1/2)
    M_new <- M_current_sqrt %*% (T_k_sym %*% T_k_sym) %*% M_current_sqrt
    M_new <- (M_new + t(M_new)) / 2 # Ensure symmetry
    M_new <- .regularize_spd(M_new, regularize_epsilon) # Ensure SPD
    # Damping update
    M_current <- (1 - damping) * M_old + damping * M_new
    M_current <- (M_current + t(M_current)) / 2
    M_current <- .regularize_spd(M_current, regularize_epsilon)

    # Check for convergence
    diff_norm <- norm(M_current - M_old, type = "F")
    if (verbose) {
      message(sprintf("  BW Iter %d: Change in M = %.3e", iter, diff_norm))
    }
    if (diff_norm < tol) {
      if (verbose) message_stage("BW Barycenter converged.", interactive_only = TRUE)
      break
    }
    if (iter == max_iter && verbose) {
      message_stage("BW Barycenter reached max iterations without specified convergence.", interactive_only = TRUE)
    }
  }

  return(M_current)
}
</file>

<file path="R/riemannian_methods_hatsa.R">
# S3 Methods for Riemannian Geometry computations on HATSA objects

#' Compute Pairwise Riemannian Distances Between Subject SPD Representations
#'
#' Calculates a matrix of Riemannian distances (Log-Euclidean metric)
#' between subjects, based on specified SPD matrix representations derived from
#' the HATSA object (e.g., covariance of aligned coefficients or various FC matrices).
#'
#' @param object A `hatsa_projector` or `task_hatsa_projector` object.
#' @param type Character string indicating the type of SPD representation to use.
#'   Commonly `"cov_coeffs"` (default) for the `k x k` covariance of aligned
#'   spectral coefficients. Other types (e.g., `"fc_conn"`, `"fc_task"`) would
#'   rely on `get_spd_representations` (from RGEOM-003) to provide the SPD matrices.
#' @param subject_data_list Optional. A list of subject time-series matrices
#'   (T_i x V_p). May be needed by `get_spd_representations` for certain `type` values.
#' @param spd_regularize_epsilon Epsilon for regularizing SPD matrices.
#' @param k_conn_params Parameters for connectivity calculation if type involves FC,
#'   passed to `get_spd_representations`.
#' @param ... Additional arguments passed to `riemannian_distance_spd` or
#'   to `get_spd_representations` if used.
#' @return A symmetric `N_subjects x N_subjects` matrix of pairwise Riemannian distances.
#'   Diagonal will be zero. `NA` if a distance cannot be computed.
#' @export
riemannian_distance_matrix_spd <- function(object, ...) {
  UseMethod("riemannian_distance_matrix_spd")
}

#' @rdname riemannian_distance_matrix_spd
#' @export
riemannian_distance_matrix_spd.hatsa_projector <- function(object,
                                             type = "cov_coeffs",
                                             subject_data_list = NULL,
                                             spd_regularize_epsilon = 1e-6,
                                             k_conn_params = NULL,
                                             ...) {
  # N_subjects derived from spd_matrices_list length or object parameters
  
  spd_matrices_list_all_subjects <- get_spd_representations(object,
                                                            type = type,
                                                            subject_idx = NULL, # Get for all subjects
                                                            regularize_epsilon = spd_regularize_epsilon,
                                                            subject_data_list_for_fc = subject_data_list,
                                                            k_conn_params_for_fc = k_conn_params,
                                                            ...)
  
  valid_indices_in_list <- which(!sapply(spd_matrices_list_all_subjects, is.null))
  
  # Use names from the list returned by get_spd_representations if available
  # These names should correspond to original subject indices as characters
  all_subject_names_in_input_list <- names(spd_matrices_list_all_subjects)
  
  # Determine the total number of subjects based on the input object
  N_total_subjects_in_object <- object$parameters$N_subjects
  # Generate character names for all subjects in the object for the final matrix
  all_original_subj_indices_char <- as.character(1:N_total_subjects_in_object)

  if (length(valid_indices_in_list) < length(spd_matrices_list_all_subjects)) {
    num_failed = length(spd_matrices_list_all_subjects) - length(valid_indices_in_list)
    # Ensure spd_matrices_list_all_subjects is not NULL or empty before trying to get its length
    total_subjects_in_list = if(!is.null(spd_matrices_list_all_subjects)) length(spd_matrices_list_all_subjects) else 0
    
    warning_msg <- sprintf("Could not obtain valid SPD matrices for %d out of %d subject(s) processed by get_spd_representations. Distances involving them will be NA.", 
                           num_failed, total_subjects_in_list)
    if (total_subjects_in_list != N_total_subjects_in_object) {
        warning_msg <- paste(warning_msg, sprintf("Note: get_spd_representations processed %d subjects, but object has %d total subjects.", total_subjects_in_list, N_total_subjects_in_object))
    }
    warning(warning_msg)
  }
  
  # Initialize the final distance matrix with NAs, sized for all subjects in the object
  final_dist_mat <- matrix(NA_real_, nrow = N_total_subjects_in_object, ncol = N_total_subjects_in_object)
  rownames(final_dist_mat) <- colnames(final_dist_mat) <- all_original_subj_indices_char
  diag(final_dist_mat) <- 0.0

  # If no valid matrices or only one, distance matrix computation for pairs is not possible
  if (length(valid_indices_in_list) < 2) {
    warning("Need at least 2 subjects with valid SPD matrices to compute a pairwise distance matrix. Returning matrix with NAs (and 0s on diagonal).")
    return(final_dist_mat) # Returns the initialized NA matrix with 0s on diagonal
  }

  spd_matrices_valid_subset <- spd_matrices_list_all_subjects[valid_indices_in_list]
  num_valid_subjects <- length(spd_matrices_valid_subset)
  valid_subj_original_indices_char <- names(spd_matrices_valid_subset) 

  # Create a temporary distance matrix for the valid subset
  dist_mat_subset <- matrix(NA_real_, nrow = num_valid_subjects, ncol = num_valid_subjects)
  rownames(dist_mat_subset) <- colnames(dist_mat_subset) <- valid_subj_original_indices_char
  diag(dist_mat_subset) <- 0.0
  
  for (i in 1:(num_valid_subjects - 1)) {
    idx1_name <- valid_subj_original_indices_char[i]
    S1 <- spd_matrices_valid_subset[[idx1_name]]
    
    for (j in (i + 1):num_valid_subjects) {
      idx2_name <- valid_subj_original_indices_char[j]
      S2 <- spd_matrices_valid_subset[[idx2_name]]
      
      current_dist <- NA_real_ # Default to NA
      if(!is.null(S1) && !is.null(S2)){ # Ensure matrices themselves are not null (double check after list filter)
         current_dist <- tryCatch(
            riemannian_distance_spd(S1, S2, regularize_epsilon = spd_regularize_epsilon, ...),
            error = function(e) {
              warning(sprintf("Riemannian distance computation failed between subject %s and %s: %s", idx1_name, idx2_name, e$message))
              NA_real_
            }
          )
      } else {
           warning(sprintf("Skipping distance between subject %s and %s due to one or both SPD matrices being NULL.", idx1_name, idx2_name))
      }
      dist_mat_subset[idx1_name, idx2_name] <- current_dist
      dist_mat_subset[idx2_name, idx1_name] <- current_dist # Symmetrize
    }
  }
  
  # Fill in the computed distances for valid subjects into the full-sized matrix
  # This uses character indexing which should align names correctly.
  final_dist_mat[valid_subj_original_indices_char, valid_subj_original_indices_char] <- dist_mat_subset
  
  return(final_dist_mat)
}


#' Compute Riemannian Dispersion of SPD Matrices on the Manifold
#'
#' Calculates the dispersion of a set of SPD matrices around their Fréchet mean
#' using the Riemannian (Log-Euclidean by default) distance.
#'
#' @param object A `hatsa_projector` or `task_hatsa_projector` object.
#' @param type Character string, either `"cov_coeffs"` (default) or others like
#'   `"fc_conn"`, `"fc_task"` (relying on RGEOM-003 `get_spd_representations`).
#'   Specifies which matrices to compute dispersion for.
#' @param subject_data_list Optional, may be needed by `get_spd_representations`.
#' @param use_geometric_median Logical, if TRUE, attempts to use AIRM Fréchet mean 
#'   (via `shapes::mediancov` if available, or iterative Log-Euclidean if not). 
#'   If FALSE (default), uses iterative Log-Euclidean Fréchet mean.
#' @param k_conn_params Parameters for connectivity if type involves FC.
#' @param spd_regularize_epsilon Epsilon for regularizing SPD matrices.
#' @param verbose Logical, controls verbose output during calculations.
#' @param ... Additional arguments passed to `frechet_mean_spd`, 
#'   `riemannian_distance_spd` or `get_spd_representations`.
#' @return A list containing: `mean_spd_matrix`, `distances_to_mean` (named vector), 
#'   `num_valid_subjects`, `original_num_subjects`, `mean_dispersion`, `median_dispersion`.
#' @export
#' @importFrom stats median
riemannian_dispersion_spd <- function(object, ...) {
  UseMethod("riemannian_dispersion_spd")
}

#' @rdname riemannian_dispersion_spd
#' @export
riemannian_dispersion_spd.hatsa_projector <- function(object,
                                          type = "cov_coeffs",
                                          subject_data_list = NULL,
                                          use_geometric_median = FALSE,
                                          k_conn_params = NULL,
                                          spd_regularize_epsilon = 1e-6,
                                          verbose = FALSE,
                                          ...) {
  
  spd_matrices_list_all_subjects <- get_spd_representations(object,
                                                            type = type,
                                                            subject_idx = NULL, 
                                                            regularize_epsilon = spd_regularize_epsilon,
                                                            subject_data_list_for_fc = subject_data_list,
                                                            k_conn_params_for_fc = k_conn_params,
                                                            ...) 
  
  valid_spd_matrices_with_names <- Filter(function(m) !is.null(m) && is.matrix(m) && !all(is.na(m)) && all(dim(m) > 0), spd_matrices_list_all_subjects)
  
  num_valid_subjects <- length(valid_spd_matrices_with_names)
  original_num_subjects <- object$parameters$N_subjects

  empty_return <- list(mean_spd_matrix = NULL, 
                       distances_to_mean = setNames(numeric(0), character(0)),
                       num_valid_subjects = num_valid_subjects, # Could be 0
                       original_num_subjects = original_num_subjects,
                       mean_dispersion = NA_real_, 
                       median_dispersion = NA_real_)

  if (num_valid_subjects == 0) {
    if(verbose) message_stage("No valid SPD matrices found to compute dispersion.", interactive_only=TRUE)
    # Update num_valid_subjects in the return list for consistency
    empty_return$num_valid_subjects <- 0
    return(empty_return)
  }
  
  if (num_valid_subjects == 1) {
      single_matrix <- valid_spd_matrices_with_names[[1]]
      single_matrix_name <- names(valid_spd_matrices_with_names)[1]
      if (is.null(single_matrix_name) || single_matrix_name == "") single_matrix_name <- "1" 

      warning_msg <- if (original_num_subjects > 1) {
          "Only one valid SPD matrix found out of multiple subjects. Dispersion is 0 for this single matrix."
      } else {
          "Only one subject provided. Dispersion is 0."
      }
      if(verbose) message_stage(warning_msg, interactive_only=TRUE)
      
       return(list(mean_spd_matrix = single_matrix, 
                   distances_to_mean = setNames(0, single_matrix_name),
                   num_valid_subjects = 1,
                   original_num_subjects = original_num_subjects,
                   mean_dispersion = 0, 
                   median_dispersion = 0))
  }
  
  mean_matrix_metric <- "logeuclidean" # Default
  if (use_geometric_median) {
      if (requireNamespace("shapes", quietly = TRUE)) {
          mean_matrix_metric <- "airm"
          if (verbose && interactive()) message_stage("Attempting AIRM Fréchet mean (via shapes::mediancov).", interactive_only=TRUE)
      } else {
          if (verbose && interactive()) message_stage("`shapes` package not available for AIRM Fréchet mean. Using iterative Log-Euclidean mean.", interactive_only=TRUE)
      }
  } else {
      if (verbose && interactive()) message_stage("Using iterative Log-Euclidean Fréchet mean.", interactive_only=TRUE)
  }

  current_mean_matrix <- frechet_mean_spd(S_list = unname(valid_spd_matrices_with_names), 
                                          metric = mean_matrix_metric, 
                                          regularize_epsilon = spd_regularize_epsilon,
                                          verbose = verbose, 
                                          ...) 
  
  if (is.null(current_mean_matrix)) {
      if(verbose) message_stage("Failed to compute mean SPD matrix. Cannot calculate dispersion.", interactive_only=TRUE)
      # Update distances_to_mean to be correctly named NA vector
      empty_return$distances_to_mean <- setNames(rep(NA_real_, num_valid_subjects), names(valid_spd_matrices_with_names))
      return(empty_return)
  }

  # Determine distance metric for individual distances to mean. Default to LogEuclidean.
  # This might differ from mean_matrix_metric if shapes was used for AIRM mean.
  distance_metric_to_mean <- "logeuclidean" 
  
  distances_to_mean_vals <- sapply(valid_spd_matrices_with_names, function(S_i) {
    dist_val <- NA_real_
    if (!is.null(S_i) && !is.null(current_mean_matrix)){
         dist_val <- tryCatch({
            if (distance_metric_to_mean == "airm") {
                airm_distance(S_i, current_mean_matrix, regularize_epsilon = spd_regularize_epsilon, ...)
            } else { # Default to LogEuclidean
                riemannian_distance_spd(S_i, current_mean_matrix, regularize_epsilon = spd_regularize_epsilon, ...)
            }
         }, error = function(e) {
            warning(sprintf("Distance calculation failed for subject '%s' to mean: %s", 
                            # Attempt to get name of S_i if list was named, else use placeholder
                            names(S_i) %||% "unknown", # Using a hypothetical %||% for brevity
                            e$message)); 
            NA_real_
        })
    }
    dist_val
  })
  # sapply should preserve names from valid_spd_matrices_with_names
  
  return(list(
    mean_spd_matrix = current_mean_matrix,
    distances_to_mean = distances_to_mean_vals, 
    num_valid_subjects = num_valid_subjects,
    original_num_subjects = original_num_subjects,
    mean_dispersion = mean(distances_to_mean_vals, na.rm = TRUE),
    median_dispersion = stats::median(distances_to_mean_vals, na.rm = TRUE)
  ))
}

# Placeholder for task_hatsa_projector methods if they are to be different
# If they are identical, we can rely on class inheritance or explicitly define them
# to call the hatsa_projector version.

# For now, assume task_hatsa_projector might have its own nuances or might just call the parent.
# Example:
# riemannian_distance_matrix_spd.task_hatsa_projector <- function(object, ...) {
#   # Could add task-specific logic or simply call the parent method:
#   # NextMethod() 
#   # OR
#   # riemannian_distance_matrix_spd.hatsa_projector(object, ...)
#   # For now, let users define it if specific behavior is needed, or rely on NextMethod if appropriate.
#   message("Using default riemannian_distance_matrix_spd for task_hatsa_projector. Define a specific method if needed.")
#   NextMethod()
# }

# riemannian_dispersion_spd.task_hatsa_projector <- function(object, ...) {
#   message("Using default riemannian_dispersion_spd for task_hatsa_projector. Define a specific method if needed.")
#   NextMethod()
# } 

#' Project Subject SPD Representations to a Common Tangent Space
#'
#' Computes the Fréchet mean of specified SPD representations for a group of
#' subjects and then projects each subject's SPD matrix to the tangent space
#' anchored at this mean. This allows for applying standard Euclidean
#' multivariate analyses to these Riemannian data.
#'
#' @param object A `hatsa_projector` or `task_hatsa_projector` object.
#' @param type Character string indicating the type of SPD representation to use
#'   (e.g., `"cov_coeffs"`). Passed to `get_spd_representations`.
#' @param tangent_metric Character string, `"logeuclidean"` (default) or `"airm"`,
#'   specifying which metric's log map to use for projection. This also influences
#'   the metric for Fréchet mean computation if not overridden by `mean_options`.
#' @param subject_data_list Optional. Passed to `get_spd_representations`.
#' @param k_conn_params Optional. Passed to `get_spd_representations`.
#' @param spd_regularize_epsilon Epsilon for regularizing SPD matrices.
#' @param mean_options A list of options for `frechet_mean_spd` (e.g., `max_iter`, `tol`).
#'   The `metric` argument within this list will override `tangent_metric` for the mean
#'   computation step if provided.
#' @param verbose Logical, controls verbosity.
#' @param ... Additional arguments passed to `get_spd_representations` or `frechet_mean_spd`.
#' @return A list containing:
#'   - `tangent_vectors`: A list of symmetric matrices (tangent vectors), one per valid subject.
#'                      Names correspond to subject original indices.
#'   - `mean_spd_matrix`: The Fréchet mean SPD matrix used as the tangent point.
#'   - `metric_used`: The metric used for the log map (`"logeuclidean"` or `"airm"`).
#'   - `num_valid_subjects`: Number of subjects for whom tangent vectors were computed.
#'   - `original_num_subjects`: Total number of subjects in the input object.
#' @export
get_tangent_space_coords <- function(object, ...) {
  UseMethod("get_tangent_space_coords")
}

#' @rdname get_tangent_space_coords
#' @export
get_tangent_space_coords.hatsa_projector <- function(object,
                                                     type = "cov_coeffs",
                                                     tangent_metric = c("logeuclidean", "airm"),
                                                     subject_data_list = NULL,
                                                     k_conn_params = NULL,
                                                     spd_regularize_epsilon = 1e-6,
                                                     mean_options = list(),
                                                     verbose = FALSE,
                                                     ...) {
  tangent_metric <- match.arg(tangent_metric)

  # 1. Get SPD matrices for all subjects
  spd_matrices_list <- get_spd_representations(object,
                                               type = type,
                                               subject_idx = NULL, # All subjects
                                               regularize_epsilon = spd_regularize_epsilon,
                                               subject_data_list_for_fc = subject_data_list,
                                               k_conn_params_for_fc = k_conn_params,
                                               ...)
  
  valid_spd_matrices <- Filter(function(m) !is.null(m) && is.matrix(m) && !all(is.na(m)) && all(dim(m) > 0), spd_matrices_list)
  
  num_valid_subjects <- length(valid_spd_matrices)
  original_num_subjects <- object$parameters$N_subjects

  if (num_valid_subjects < 1) {
    if(verbose && interactive()) message_stage("No valid SPD matrices to compute Fréchet mean or tangent vectors.", interactive_only=TRUE)
    return(list(tangent_vectors = list(), mean_spd_matrix = NULL, metric_used = tangent_metric, 
                num_valid_subjects = 0, original_num_subjects = original_num_subjects))
  }

  # 2. Compute Fréchet mean
  frechet_mean_metric <- if (!is.null(mean_options$metric)) mean_options$metric else tangent_metric

  frechet_args <- list(
    S_list = unname(valid_spd_matrices),
    metric = frechet_mean_metric,
    regularize_epsilon = spd_regularize_epsilon,
    verbose = verbose
  )
  # Merge with user-provided mean_options, giving precedence to mean_options
  # for common parameters like max_iter, tol, etc. and also pass further ... args from main call
  custom_frechet_params <- utils::modifyList(mean_options, list(...))
  # Remove params already explicitly set or those not for frechet_mean_spd
  custom_frechet_params$S_list <- NULL 
  custom_frechet_params$metric <- NULL
  custom_frechet_params$regularize_epsilon <- NULL
  custom_frechet_params$verbose <- NULL
  
  final_frechet_args <- utils::modifyList(frechet_args, custom_frechet_params)
  
  mean_spd <- do.call(frechet_mean_spd, final_frechet_args)

  if (is.null(mean_spd)) {
    if(verbose && interactive()) message_stage("Failed to compute Fréchet mean. Cannot compute tangent space coordinates.", interactive_only=TRUE)
    return(list(tangent_vectors = list(), mean_spd_matrix = NULL, metric_used = tangent_metric, 
                num_valid_subjects = num_valid_subjects, original_num_subjects = original_num_subjects))
  }

  # 3. Map each SPD matrix to the tangent space at the mean
  logmap_function <- if (tangent_metric == "logeuclidean") {
    logmap_spd_logeuclidean
  } else { # airm
    logmap_spd_airm
  }

  tangent_vectors_list <- vector("list", num_valid_subjects)
  names(tangent_vectors_list) <- names(valid_spd_matrices) 

  for (i in seq_along(valid_spd_matrices)) {
    subj_name_for_warning <- names(valid_spd_matrices)[i]
    if(is.null(subj_name_for_warning) || subj_name_for_warning == "") subj_name_for_warning <- as.character(i)

    S_i <- valid_spd_matrices[[i]]
    tv_i <- tryCatch(
      logmap_function(mean_spd, S_i, regularize_epsilon = spd_regularize_epsilon),
      error = function(e) {
        warning(sprintf("Logmap failed for subject %s: %s", subj_name_for_warning, e$message))
        NULL
      }
    )
    # Use the original name from valid_spd_matrices for assignment
    tangent_vectors_list[[names(valid_spd_matrices)[i]]] <- tv_i 
  }
  
  successful_tangent_vectors <- Filter(Negate(is.null), tangent_vectors_list)
  num_successful_projections <- length(successful_tangent_vectors)

  if (num_successful_projections < num_valid_subjects && verbose && interactive()){
      message_stage(sprintf("Successfully projected %d out of %d valid SPD matrices to tangent space.", 
                          num_successful_projections, num_valid_subjects), interactive_only=TRUE)
  }

  return(list(
    tangent_vectors = successful_tangent_vectors,
    mean_spd_matrix = mean_spd,
    metric_used = tangent_metric,
    num_valid_subjects = num_successful_projections, 
    original_num_subjects = original_num_subjects
  ))
}

# Potential helper for vectorizing symmetric matrices (conceptual)
# .vectorize_symmetric_matrix <- function(symm_matrix, method = "upper_tri_diag") {
#   if (!isSymmetric.matrix(symm_matrix)) stop("Matrix must be symmetric.")
#   if (method == "upper_tri_diag") {
#     return(symm_matrix[upper.tri(symm_matrix, diag = TRUE)])
#   }
#   stop("Unsupported vectorization method.")
# }
</file>

<file path="R/spd_representations.R">
# Helper function to extract a single subject's aligned sketch
# It should handle cases where U_aligned_list is present or fallback to object$s
#' @importFrom Matrix isSymmetric
NULL

.get_subject_aligned_sketch <- function(object, subject_idx) {
  if (!inherits(object, "hatsa_projector")) {
    stop(".get_subject_aligned_sketch expects a hatsa_projector object")
  }
  if (subject_idx < 1 || subject_idx > object$parameters$N_subjects) {
    stop(sprintf("Invalid subject_idx %d for N_subjects = %d", subject_idx, object$parameters$N_subjects))
  }

  # Prefer U_aligned_list if available and populated for this subject
  if (!is.null(object$U_aligned_list) && length(object$U_aligned_list) >= subject_idx) {
    sketch <- object$U_aligned_list[[subject_idx]]
    if (!is.null(sketch)) {
      if (is.matrix(sketch) && ncol(sketch) == object$parameters$k && nrow(sketch) == object$parameters$V_p) {
        return(sketch)
      } else {
        warning(sprintf("U_aligned_list[[%d]] is not a valid matrix. Falling back to object$s.", subject_idx))
      }
    }
  }
  
  # Fallback to object$s and block_indices
  if (!is.null(object$s) && !is.null(object$block_indices) && 
      length(object$block_indices) >= subject_idx && 
      !is.null(object$block_indices[[subject_idx]])) {
    
    rows_subj_i <- object$block_indices[[subject_idx]]
    if (length(rows_subj_i) == 0 && object$parameters$V_p > 0) { # Subject might have had no data
        warning(sprintf("Subject %d has no rows in object$s (block_indices are empty). Returning NULL sketch.", subject_idx))
        return(NULL)
    } else if (length(rows_subj_i) == 0 && object$parameters$V_p == 0) {
        return(matrix(NA_real_, nrow=0, ncol=object$parameters$k)) # Consistent 0-row matrix if V_p = 0
    }
    
    if (max(rows_subj_i) > nrow(object$s)) {
        warning(sprintf("Subject %d block_indices exceed nrow(object$s). Returning NULL sketch.", subject_idx))
        return(NULL)
    }

    U_aligned_i_from_s <- object$s[rows_subj_i, , drop = FALSE]
    
    if (nrow(U_aligned_i_from_s) == object$parameters$V_p && ncol(U_aligned_i_from_s) == object$parameters$k) {
      if (any(is.na(U_aligned_i_from_s))) {
        warning(sprintf("Sketch for subject %d from object$s contains NAs.", subject_idx))
      }
      return(U_aligned_i_from_s)
    } else {
      warning(sprintf("Sketch for subject %d from object$s has unexpected dimensions: %d x %d (expected %d x %d). Returning NULL.", 
                      subject_idx, nrow(U_aligned_i_from_s), ncol(U_aligned_i_from_s), 
                      object$parameters$V_p, object$parameters$k))
      return(NULL)
    }
  } else {
    warning(sprintf("Could not extract sketch for subject %d using U_aligned_list or object$s.", subject_idx))
    return(NULL)
  }
}

#' Compute Covariance of Aligned Spectral Coefficients
#'
#' For a given subject's aligned spectral sketch `U_aligned_i` (V_p x k),
#' this function computes the `k x k` covariance matrix of these coefficients
#' across the V_p parcels.
#'
#' @param U_aligned_subject A numeric matrix (V_p x k) of aligned spectral coefficients.
#' @return A `k x k` covariance matrix. Returns a matrix of NAs if V_p <= 1 or k = 0, or if input is NULL.
#' @importFrom stats cov
#' @keywords internal
.compute_cov_spectral_coeffs <- function(U_aligned_subject) {
  if (is.null(U_aligned_subject)) {
    return(NULL) 
  }
  if (!is.matrix(U_aligned_subject) || !is.numeric(U_aligned_subject)) {
    stop("U_aligned_subject must be a numeric matrix.")
  }
  V_p <- nrow(U_aligned_subject)
  k <- ncol(U_aligned_subject)

  if (k == 0) {
    return(matrix(NA_real_, nrow = 0, ncol = 0))
  }
  if (V_p <= 1) {
    return(matrix(NA_real_, nrow = k, ncol = k))
  }
  return(stats::cov(U_aligned_subject))
}


#' Get SPD Matrix Representations from a HATSA Projector Object
#'
#' Extracts or computes various types of Symmetric Positive-Definite (SPD) matrix
#' representations for subjects from a `hatsa_projector` object.
#'
#' @param object A `hatsa_projector` or `task_hatsa_projector` object.
#' @param type Character string indicating the type of SPD representation to use.
#'   Currently supported for `hatsa_projector`:
#'   `"cov_coeffs"` (covariance of aligned spectral coefficients).
#'   Other types like `"fc_conn"` might be added or supported by specific methods.
#' @param subject_idx Optional integer or vector of integers. If provided, returns
#'   SPD matrices only for these subjects. If NULL (default), for all subjects.
#' @param regularize_epsilon Small positive value for SPD regularization. Default from RGEOM-001.
#' @param subject_data_list_for_fc Optional list of subject time-series matrices
#'   (T_i x V_p), needed for `type = "fc_conn"`.
#' @param k_conn_params_for_fc List of parameters for `compute_subject_connectivity_graph_sparse`,
#'   e.g., `list(k_conn_pos = 10, k_conn_neg = 10, zscore_type = "abs", ...)`.
#'   Needed for `type = "fc_conn"`.
#' @param ... Additional arguments, potentially passed to specific computation functions.
#' @return A list of SPD matrices. Each element corresponds to a subject. Returns NULL
#'   for a subject if its SPD matrix cannot be computed.
#' @export
get_spd_representations <- function(object, ...) {
  UseMethod("get_spd_representations")
}

#' @rdname get_spd_representations
#' @export
get_spd_representations.hatsa_projector <- function(object,
                                                  type = c("cov_coeffs", "fc_conn"),
                                                  subject_idx = NULL,
                                                  regularize_epsilon = 1e-6, 
                                                  subject_data_list_for_fc = NULL,
                                                  k_conn_params_for_fc = list(),
                                                  ...) {
  type <- match.arg(type)
  N_subjects <- object$parameters$N_subjects
  
  if (is.null(subject_idx)) {
    subject_indices_to_process <- 1:N_subjects
  } else {
    if (!is.numeric(subject_idx) || !all(subject_idx >= 1 & subject_idx <= N_subjects)) {
      stop(sprintf("Invalid subject_idx. Must be NULL or integers between 1 and %d.", N_subjects))
    }
    subject_indices_to_process <- as.integer(unique(subject_idx))
  }
  
  if (length(subject_indices_to_process) == 0 && N_subjects > 0) {
    return(list())
  } else if (N_subjects == 0) {
    return(list())
  }

  all_spd_matrices <- vector("list", length(subject_indices_to_process))
  names(all_spd_matrices) <- as.character(subject_indices_to_process) 

  for (i in seq_along(subject_indices_to_process)) {
    current_subj_orig_idx <- subject_indices_to_process[i]
    spd_matrix_i <- NULL 

    if (type == "cov_coeffs") {
      U_aligned_i <- .get_subject_aligned_sketch(object, current_subj_orig_idx)
      if (!is.null(U_aligned_i)) {
        cov_coeffs_i <- .compute_cov_spectral_coeffs(U_aligned_i)
        if (!is.null(cov_coeffs_i) && is.matrix(cov_coeffs_i) && all(dim(cov_coeffs_i) > 0) && !all(is.na(cov_coeffs_i))) {
            spd_matrix_i <- .regularize_spd(cov_coeffs_i, regularize_epsilon)
        } else {
            warning(sprintf("Covariance of coefficients for subject %d resulted in NULL, NA, or zero-dim matrix.", current_subj_orig_idx))
        }
      } else {
        warning(sprintf("Could not get aligned sketch for subject %d for cov_coeffs.", current_subj_orig_idx))
      }
    } else if (type == "fc_conn") {
      if (is.null(subject_data_list_for_fc)) {
        stop("`subject_data_list_for_fc` is required for type = 'fc_conn'.")
      }
      if (length(subject_data_list_for_fc) < current_subj_orig_idx || is.null(subject_data_list_for_fc[[current_subj_orig_idx]])) {
        warning(sprintf("No data in `subject_data_list_for_fc` for subject index %d.", current_subj_orig_idx))
        all_spd_matrices[[i]] <- NULL
        next
      }
      subj_ts_data <- subject_data_list_for_fc[[current_subj_orig_idx]]
      
      conn_args <- list(
        X_subject = subj_ts_data,
        k_conn_pos = ifelse(is.null(k_conn_params_for_fc$k_conn_pos), 10, k_conn_params_for_fc$k_conn_pos),
        k_conn_neg = ifelse(is.null(k_conn_params_for_fc$k_conn_neg), 10, k_conn_params_for_fc$k_conn_neg),
        zscore_type = ifelse(is.null(k_conn_params_for_fc$zscore_type), "abs", k_conn_params_for_fc$zscore_type),
        corr_method = ifelse(is.null(k_conn_params_for_fc$corr_method), "pearson", k_conn_params_for_fc$corr_method),
        cache_dir = NULL, 
        verbose = FALSE   
      )
      extra_conn_args <- k_conn_params_for_fc[!names(k_conn_params_for_fc) %in% names(conn_args)]
      final_conn_args <- c(conn_args, extra_conn_args)
      
      W_conn_i_list <- tryCatch(
        do.call(hatsa::compute_subject_connectivity_graph_sparse, final_conn_args),
        error = function(e) {
          warning(sprintf("Error computing W_conn for subject %d: %s", current_subj_orig_idx, e$message))
          NULL
        }
      )
      
      if (!is.null(W_conn_i_list) && !is.null(W_conn_i_list$W_sparse)) {
        W_conn_i <- W_conn_i_list$W_sparse
        if (!Matrix::isSymmetric(W_conn_i)) {
            warning(sprintf("W_conn_i for subject %d was not symmetric. Symmetrizing: (W+t(W))/2.", current_subj_orig_idx))
            W_conn_i <- (W_conn_i + t(W_conn_i)) / 2
        }
        if (inherits(W_conn_i, "sparseMatrix")) {
            W_conn_i_dense <- as.matrix(W_conn_i)
             spd_matrix_i <- .regularize_spd(W_conn_i_dense, regularize_epsilon)
        } else {
             spd_matrix_i <- .regularize_spd(W_conn_i, regularize_epsilon)
        }
      } else {
        warning(sprintf("W_conn_i computation failed or returned NULL for subject %d.", current_subj_orig_idx))
      }
    } else {
      warning(sprintf("Type '%s' not yet fully implemented for get_spd_representations.hatsa_projector.", type))
    }
    all_spd_matrices[[i]] <- spd_matrix_i
  }
  
  if (is.numeric(subject_idx) && length(subject_idx) == 1 && length(all_spd_matrices) == 1) {
      return(all_spd_matrices[[1]])
  }
  return(all_spd_matrices)
}

#' @rdname get_spd_representations
#' @export
get_spd_representations.task_hatsa_projector <- function(object,
                                                       type = c("cov_coeffs", "fc_conn", "fc_task", "fc_hybrid"),
                                                       subject_idx = NULL,
                                                       regularize_epsilon = 1e-6, 
                                                       subject_data_list_for_fc = NULL,
                                                       k_conn_params_for_fc = list(),
                                                       lambda_blend_for_hybrid = NULL, 
                                                       ...) {
  type <- match.arg(type)
  N_subjects <- object$parameters$N_subjects

  if (is.null(subject_idx)) {
    subject_indices_to_process <- 1:N_subjects
  } else {
    if (!is.numeric(subject_idx) || !all(subject_idx >= 1 & subject_idx <= N_subjects)) {
      stop(sprintf("Invalid subject_idx. Must be NULL or integers between 1 and %d.", N_subjects))
    }
    subject_indices_to_process <- as.integer(unique(subject_idx))
  }

  if (length(subject_indices_to_process) == 0 && N_subjects > 0) return(list())
  if (N_subjects == 0) return(list())

  all_spd_matrices <- vector("list", length(subject_indices_to_process))
  names(all_spd_matrices) <- as.character(subject_indices_to_process)

  if (type %in% c("cov_coeffs", "fc_conn")) {
    spd_matrices_from_parent <- get_spd_representations.hatsa_projector(
        object = object, 
        type = type, 
        subject_idx = subject_indices_to_process, 
        regularize_epsilon = regularize_epsilon,
        subject_data_list_for_fc = subject_data_list_for_fc,
        k_conn_params_for_fc = k_conn_params_for_fc,
        ...
    )
    if (is.numeric(subject_idx) && length(subject_idx) == 1 && !is.list(spd_matrices_from_parent)){
        all_spd_matrices[[1]] <- spd_matrices_from_parent
    } else {
        all_spd_matrices <- spd_matrices_from_parent 
    }
    
  } else if (type %in% c("fc_task", "fc_hybrid")) {
    for (i in seq_along(subject_indices_to_process)) {
      current_subj_orig_idx <- subject_indices_to_process[i]
      spd_matrix_i <- NULL
      W_matrix <- NULL

      if (type == "fc_task") {
        if (!is.null(object$W_task_list) && length(object$W_task_list) >= current_subj_orig_idx) {
          W_matrix <- object$W_task_list[[current_subj_orig_idx]]
          if(is.null(W_matrix)) warning(sprintf("Stored W_task_list for subject %d is NULL.", current_subj_orig_idx))
        } else {
          warning(sprintf("W_task_list not available or not populated for subject %d.", current_subj_orig_idx))
        }
      } else if (type == "fc_hybrid") {
        if (!is.null(object$W_hybrid_list) && length(object$W_hybrid_list) >= current_subj_orig_idx) {
          W_matrix <- object$W_hybrid_list[[current_subj_orig_idx]]
           if(is.null(W_matrix)) warning(sprintf("Stored W_hybrid_list for subject %d is NULL.", current_subj_orig_idx))
        } else {
          warning(sprintf("W_hybrid_list not available or not populated for subject %d.", current_subj_orig_idx))
        }
      }

      if (!is.null(W_matrix)) {
        if (!Matrix::isSymmetric(W_matrix)) {
            warning(sprintf("%s for subject %d was not symmetric. Symmetrizing: (W+t(W))/2.", type, current_subj_orig_idx))
            W_matrix <- (W_matrix + t(W_matrix)) / 2
        }
        current_W_dense <- if (inherits(W_matrix, "sparseMatrix")) as.matrix(W_matrix) else W_matrix
        if (is.matrix(current_W_dense) && all(dim(current_W_dense) > 0) && !all(is.na(current_W_dense))){
            spd_matrix_i <- .regularize_spd(current_W_dense, regularize_epsilon)
        } else {
            warning(sprintf("%s for subject %d resulted in NULL, NA, or zero-dim matrix after densification.", type, current_subj_orig_idx))
        }
      } else {
        warning(sprintf("Could not retrieve or compute %s matrix for subject %d.", type, current_subj_orig_idx))
      }
      all_spd_matrices[[as.character(current_subj_orig_idx)]] <- spd_matrix_i 
    }
  } else {
    stop(sprintf("Unsupported type '%s' in get_spd_representations.task_hatsa_projector.", type))
  }
  
  if (is.numeric(subject_idx) && length(subject_idx) == 1 && length(all_spd_matrices) == 1) {
      return(all_spd_matrices[[1]])
  }
  return(all_spd_matrices)
}
</file>

<file path="R/spectral_graph_construction.R">
#' Compute subject-specific sparse connectivity graph `W_conn_i`
#'
#' Calculates the sparse connectivity graph for a single subject.
#' Steps:
#' 1. Compute Pearson correlation matrix from `X_subject` (with guard for large V_p).
#' 2. Identify and mask zero-variance parcels.
#' 3. Sparsify: For each parcel, identify indices of `k_conn_pos` strongest
#'    positive and `k_conn_neg` strongest negative correlations, using partial sort.
#'    Exclude zero-variance parcels from selection candidates.
#'    Construct a directed sparse graph `W_dir` from these.
#' 4. Symmetrize `W_dir` using `W_sym_raw = (W_dir + t(W_dir)) / 2`, then `drop0`.
#' 5. Ensure strict symmetry for z-scoring: `W_symmetric = Matrix::forceSymmetric(W_sym_raw, uplo="U")`.
#' 6. Z-score non-zero edge weights in `W_symmetric` (assumes `zscore_nonzero_sparse` is stable).
#'
#' @param X_subject A numeric matrix of time-series data for one subject
#'   (`T_i` time points x `V_p` parcels).
#' @param parcel_names A character vector of parcel names.
#' @param k_conn_pos An integer, number of positive connections to retain per node.
#' @param k_conn_neg An integer, number of negative connections to retain per node.
#' @param use_dtw Logical, defaults to `FALSE`. (Placeholder).
#' @return A sparse symmetric `Matrix::dgCMatrix` of size `V_p x V_p`
#'   representing the z-scored connectivity graph `W_conn_i`.
#' @importFrom Matrix Matrix sparseMatrix drop0 t forceSymmetric
#' @importFrom stats cor sd
#' @keywords internal
compute_subject_connectivity_graph_sparse <- function(X_subject, parcel_names,
                                                      k_conn_pos, k_conn_neg,
                                                      use_dtw = FALSE) {
  V_p <- ncol(X_subject)
  if (V_p == 0) return(Matrix::Matrix(0, 0, 0, sparse = TRUE, dimnames = list(character(0), character(0))))

  if (use_dtw && interactive()) {
    message("Note: DTW is not yet implemented. Proceeding with standard correlations.")
  }
  
  if (V_p^2 > 1e8) { # Audit suggestion: Guard for large V_p dense correlation
      stop(sprintf("V_p (%d) is too large (%d x %d) for dense correlation matrix. Consider alternative methods.", V_p, V_p, V_p))
  }

  col_sds <- apply(X_subject, 2, stats::sd, na.rm = TRUE)
  zero_var_indices <- which(col_sds == 0)
  if (length(zero_var_indices) > 0 && interactive()) {
    message(sprintf("Found %d parcel(s) with zero variance. These will be excluded from k-NN selection and their correlations are 0.", length(zero_var_indices)))
  }
  
  # Use suppressWarnings to handle "the standard deviation is zero" warnings from cor()
  corr_matrix_dense <- suppressWarnings(stats::cor(X_subject, use = "pairwise.complete.obs"))
  corr_matrix_dense[is.na(corr_matrix_dense)] <- 0 
  diag(corr_matrix_dense) <- 0 
  
  # Mask correlations involving zero-variance parcels directly in the dense matrix before selection
  if (length(zero_var_indices) > 0) {
      corr_matrix_dense[zero_var_indices, ] <- 0
      corr_matrix_dense[, zero_var_indices] <- 0
  }
  # Additional safeguard for any other NaNs/Infs that might have arisen from cor()
  corr_matrix_dense[!is.finite(corr_matrix_dense)] <- 0

  row_indices_list <- vector("list", V_p)
  col_indices_list <- vector("list", V_p)
  values_list <- vector("list", V_p)
  
  # Create a full index of parcels to potentially select from, excluding zero-variance ones for candidates
  selectable_parcel_indices <- setdiff(1:V_p, zero_var_indices)

  if ((k_conn_pos > 0 || k_conn_neg > 0) && length(selectable_parcel_indices) > 0) {
    for (i in 1:V_p) {
      if (i %in% zero_var_indices) { # Skip k-NN selection for zero-variance parcels themselves
          row_indices_list[[i]] <- integer(0) # Ensure it's not NULL for unlist logic
          col_indices_list[[i]] <- integer(0)
          values_list[[i]] <- numeric(0)
          next
      }
      
      node_corrs_full_row <- corr_matrix_dense[i, ]
      # Consider only selectable parcels for finding top-k connections
      node_corrs_candidates <- node_corrs_full_row[selectable_parcel_indices]
      
      current_selected_indices_in_selectable <- integer(0)
      current_selected_values <- numeric(0)
      
      if (k_conn_pos > 0) {
        pos_candidates_idx_in_subset <- which(node_corrs_candidates > 0)
        if (length(pos_candidates_idx_in_subset) > 0) {
          pos_subset_vals <- node_corrs_candidates[pos_candidates_idx_in_subset]
          num_to_keep_pos <- min(k_conn_pos, length(pos_subset_vals))
          # Use head(order(...)) for partial sort efficiency
          top_pos_in_subset_ordered_idx <- head(order(pos_subset_vals, decreasing = TRUE), num_to_keep_pos)
          
          current_selected_indices_in_selectable <- c(current_selected_indices_in_selectable, pos_candidates_idx_in_subset[top_pos_in_subset_ordered_idx])
          current_selected_values <- c(current_selected_values, pos_subset_vals[top_pos_in_subset_ordered_idx])
        }
      }
      
      if (k_conn_neg > 0) {
        neg_candidates_idx_in_subset <- which(node_corrs_candidates < 0)
        if (length(neg_candidates_idx_in_subset) > 0) {
          neg_subset_vals <- node_corrs_candidates[neg_candidates_idx_in_subset]
          num_to_keep_neg <- min(k_conn_neg, length(neg_subset_vals))
          top_neg_in_subset_ordered_idx <- head(order(neg_subset_vals, decreasing = FALSE), num_to_keep_neg)

          current_selected_indices_in_selectable <- c(current_selected_indices_in_selectable, neg_candidates_idx_in_subset[top_neg_in_subset_ordered_idx])
          current_selected_values <- c(current_selected_values, neg_subset_vals[top_neg_in_subset_ordered_idx])
        }
      }
      
      if(length(current_selected_indices_in_selectable) > 0) {
          # Map indices from subset back to original V_p indices
          final_selected_original_indices <- selectable_parcel_indices[current_selected_indices_in_selectable]
          row_indices_list[[i]] <- rep(i, length(final_selected_original_indices))
          col_indices_list[[i]] <- final_selected_original_indices
          values_list[[i]] <- current_selected_values
      } else {
          row_indices_list[[i]] <- integer(0) # Ensure it's not NULL
          col_indices_list[[i]] <- integer(0)
          values_list[[i]] <- numeric(0)
      }
    }
  }
  
  final_row_indices <- unlist(row_indices_list)
  final_col_indices <- unlist(col_indices_list)
  final_values <- unlist(values_list)

  if (length(final_row_indices) > 0) {
      W_dir <- Matrix::sparseMatrix(
        i = final_row_indices, j = final_col_indices, x = final_values,
        dims = c(V_p, V_p), dimnames = list(parcel_names, parcel_names)
      )
      W_dir <- Matrix::drop0(W_dir)
  } else { 
      W_dir <- Matrix::Matrix(0, nrow=V_p, ncol=V_p, sparse=TRUE, dimnames = list(parcel_names, parcel_names))
  }

  # Symmetrize W_dir 
  W_dir_t <- Matrix::t(W_dir)
  W_sum <- W_dir + W_dir_t # Sum of weights where edges exist from either direction

  # Create an empty Matrix of appropriate size
  W_symmetric <- Matrix::Matrix(0, nrow=V_p, ncol=V_p, sparse=TRUE, dimnames = list(parcel_names, parcel_names))
  
  # Only process non-zero entries if they exist
  nnz_W_sum <- tryCatch(Matrix::nnzero(W_sum), error = function(e) 0)
  if (!is.na(nnz_W_sum) && nnz_W_sum > 0) {
      # Create a denominator matrix W_den based on the non-zero pattern of W_sum
      idx <- Matrix::which(W_sum != 0, arr.ind = TRUE)
      den_values_at_idx <- pmax(1, as.numeric(W_dir[idx] != 0) + as.numeric(W_dir_t[idx] != 0))
      
      W_den_sparse <- Matrix::sparseMatrix(i = idx[,1], j = idx[,2], x = den_values_at_idx, 
                                         dims = dim(W_sum), dimnames = dimnames(W_sum))
      
      # Perform element-wise division for non-zero elements of W_sum
      W_symmetric_raw <- W_sum / W_den_sparse
      W_symmetric_raw[!is.finite(W_symmetric_raw)] <- 0
      
      # Ensure diagonal is zero
      diag(W_symmetric_raw) <- 0 
      W_symmetric_raw <- Matrix::drop0(W_symmetric_raw)
      
      # Make symmetric
      W_symmetric <- Matrix::forceSymmetric(W_symmetric_raw, uplo = "U")
      if (!is(W_symmetric, "sparseMatrix")) {
        W_symmetric <- Matrix::Matrix(W_symmetric, sparse = TRUE)
      }
  }
  
  # Ensure zero-variance parcels have zero connections
  if (length(zero_var_indices) > 0) {
      for (zero_idx in zero_var_indices) {
        # Create zero slices since we know these should be all zeros
        zero_row <- Matrix::Matrix(0, nrow=1, ncol=V_p, sparse=TRUE)
        zero_col <- Matrix::Matrix(0, nrow=V_p, ncol=1, sparse=TRUE)
        
        # Replace entire rows/columns for zero-variance indices
        W_symmetric[zero_idx, ] <- zero_row
        W_symmetric[, zero_idx] <- zero_col
      }
      W_symmetric <- Matrix::drop0(W_symmetric)
  }

  # Z-score non-zero elements
  W_conn_i <- zscore_nonzero_sparse(W_symmetric)
  
  return(as(W_conn_i, "dgCMatrix"))
}

#' Compute sparse α-lazy random-walk normalized graph Laplacian `L = I - α D⁻¹ W`
#'
#' @param W_sparse A sparse, symmetric adjacency matrix (`Matrix::dgCMatrix`, `V_p x V_p`).
#' @param alpha Numeric, the laziness parameter. Default is 0.93.
#'   Will be clamped to `[epsilon, 1]` range if outside `(0,1]`.
#' @param degree_type Character string, how to calculate node degrees if `W_sparse` has negative values.
#'   One of `"abs"` (default, sum of absolute weights), `"positive"` (sum of positive weights only),
#'   or `"signed"` (sum of raw weights). Documented for clarity.
#' @return A sparse, symmetric graph Laplacian matrix (`Matrix::dgCMatrix`, `V_p x V_p`).
#' @importFrom Matrix Diagonal rowSums t forceSymmetric
#' @keywords internal
compute_graph_laplacian_sparse <- function(W_sparse, alpha = 0.93, degree_type = "abs") {
  stopifnot(inherits(W_sparse, "Matrix"))
  V_p <- nrow(W_sparse)
  if (V_p == 0) return(Matrix::Matrix(0,0,0,sparse=TRUE))
  
  # Audit suggestion: Clamp alpha
  if (alpha <= 0 || alpha > 1) {
      original_alpha <- alpha
      alpha <- min(max(alpha, .Machine$double.eps), 1)
      warning(sprintf("alpha value %.3f is outside typical (0,1] range. Clamped to %.3f.", original_alpha, alpha))
  }
  
  degree_type <- match.arg(degree_type, c("abs", "positive", "signed"))

  degree_vec <- switch(degree_type,
    abs = Matrix::rowSums(abs(W_sparse)),
    positive = Matrix::rowSums(W_sparse * (W_sparse > 0)), # or pmax(0, W_sparse)
    signed = Matrix::rowSums(W_sparse)
  )
  
  if (degree_type == "abs" && any(W_sparse@x < 0, na.rm=TRUE)) {
      # message("Note: Using 'abs' degree type with a graph containing negative weights. Interpret D^-1 W with care.")
  } else if (degree_type == "signed" && any(degree_vec <= 0, na.rm=TRUE)) {
      # message("Note: Using 'signed' degree type, and some node degrees are <=0. D^-1 will be 0 or undefined for these.")
  }
  
  zero_degree_indices <- which(degree_vec == 0)
  if (length(zero_degree_indices) > 0 && interactive()) {
      # message(sprintf("Found %d nodes with zero degree (type: %s). D^-1 will be zero for these.", length(zero_degree_indices), degree_type))
  }
  
  inv_degree_vec <- ifelse(degree_vec == 0, 0, 1 / degree_vec)
  D_inv_sparse <- Matrix::Diagonal(n = V_p, x = inv_degree_vec)
  
  L_rw_lazy <- Matrix::Diagonal(n=V_p) - alpha * (D_inv_sparse %*% W_sparse)
  
  # Instead of using forceSymmetric, use explicit symmetrization to match the test
  L_rw_lazy_sym <- (L_rw_lazy + Matrix::t(L_rw_lazy)) / 2
  L_rw_lazy_sym <- Matrix::drop0(L_rw_lazy_sym)
  
  if (!inherits(L_rw_lazy_sym, "sparseMatrix")) {
      L_rw_lazy_sym <- Matrix::Matrix(L_rw_lazy_sym, sparse = TRUE)
  }

  return(as(Matrix::drop0(L_rw_lazy_sym), "dgCMatrix"))
}

#' Compute spectral sketch `U_orig_i` using `RSpectra` or `base::eigen`
#'
#' Computes the `k` eigenvectors of the sparse graph Laplacian `L_conn_i_sparse`
#' corresponding to the smallest, non-trivial eigenvalues.
#' Eigenvectors for eigenvalues numerically close to zero are discarded based on a dynamic tolerance.
#'
#' @param L_conn_i_sparse A sparse graph Laplacian matrix (`Matrix::dgCMatrix`, `V_p x V_p`).
#'   Must be symmetric.
#' @param k An integer, the desired spectral rank. Must be `k >= 0`.
#' @return A list containing two elements:
#'   - `vectors`: A dense matrix `U_orig_i` (`V_p x k_actual`) of eigenvectors.
#'     `k_actual` may be less than `k` if not enough informative eigenvectors are found.
#'   - `values`: A vector of eigenvalues corresponding to the eigenvectors.
#' @importFrom RSpectra eigs_sym
#' @keywords internal
compute_spectral_sketch_sparse <- function(L_conn_i_sparse, k) {
  V_p <- nrow(L_conn_i_sparse)
  # Default eigenvalue_tol, will be updated dynamically later if possible
  eigenvalue_tol_floor <- 1e-8 

  if (k < 0) stop("`spectral_rank_k` (k) must be non-negative.")
  if (V_p == 0) return(list(vectors = matrix(0, 0, k), values = numeric(0)))
  if (k == 0) return(list(vectors = matrix(0, V_p, 0), values = numeric(0)))

  num_eigs_to_request <- min(V_p -1, k + 10) 
  if (V_p <= 1) num_eigs_to_request = 0

  use_dense_eigen <- FALSE
  if (V_p <= 5 || k >= V_p -1 ) { 
      use_dense_eigen <- TRUE
      if (interactive() && V_p > 5) message(sprintf("Note: k (%d) is high relative to V_p (%d) or V_p is small. Using base::eigen.", k, V_p))
  }
  if (V_p > 1 && num_eigs_to_request >= V_p) { 
      num_eigs_to_request = V_p -1 
  }
  if (num_eigs_to_request <=0 && V_p > 0) { 
      use_dense_eigen <- TRUE 
  }

  if (use_dense_eigen) {
    if (interactive() && V_p > 100) { 
        message(paste("Warning: Converting potentially large sparse Laplacian (", V_p, "x", V_p,
                      ") to dense for eigendecomposition. This may be memory intensive.", sep=""))
    }
    L_dense <- as.matrix(L_conn_i_sparse)
    if(any(!is.finite(L_dense))) {
        stop("Non-finite values in dense Laplacian before eigen decomposition.")
    }
    eigen_decomp <- eigen(L_dense, symmetric = TRUE)
    eigen_vals_raw <- eigen_decomp$values
    eigen_vecs_raw <- eigen_decomp$vectors
  } else {
    eigs_result <- RSpectra::eigs_sym(L_conn_i_sparse, 
                                      k = num_eigs_to_request, 
                                      which = "SM", 
                                      opts = list(retvec = TRUE, tol = 1e-9))
    eigen_vals_raw <- eigs_result$values
    eigen_vecs_raw <- eigs_result$vectors
  }
  
  sorted_order <- order(eigen_vals_raw)
  eigen_vals_sorted <- eigen_vals_raw[sorted_order]
  eigen_vecs_sorted <- eigen_vecs_raw[, sorted_order, drop = FALSE]
  
  # Audit suggestion: Dynamic eigenvalue tolerance
  if (length(eigen_vals_sorted) > 0) {
      median_abs_lambda <- stats::median(abs(eigen_vals_sorted), na.rm = TRUE)
      dynamic_tol_component <- 1e-4 * median_abs_lambda
      eigenvalue_tol <- max(eigenvalue_tol_floor, dynamic_tol_component, na.rm = TRUE) # Ensure NA doesn't break max if median_abs_lambda is NA
      if (is.na(eigenvalue_tol) || !is.finite(eigenvalue_tol)) eigenvalue_tol <- eigenvalue_tol_floor # Fallback
  } else {
      eigenvalue_tol <- eigenvalue_tol_floor
  }
  
  non_trivial_indices <- which(eigen_vals_sorted > eigenvalue_tol)
  
  U_informative <- eigen_vecs_sorted[, non_trivial_indices, drop = FALSE]
  Lambda_informative <- eigen_vals_sorted[non_trivial_indices]
  
  num_found_informative <- ncol(U_informative)
  
  if (num_found_informative == 0 && k > 0) {
      stop(sprintf("No informative eigenvectors found (all eigenvalues <= %.2e). Requested k=%d. Check graph structure or eigenvalue tolerance (current: %.2e).", eigenvalue_tol, k, eigenvalue_tol))
  }
  
  if (num_found_informative < k) {
    stop(sprintf("Found only %d informative eigenvectors (eigenvalues > %.2e), but k=%d was requested. Try reducing k or inspecting graph connectivity/Laplacian spectrum.", 
                    num_found_informative, eigenvalue_tol, k))
  } else {
    k_to_select <- k
  }
  
  U_orig_i <- U_informative[, 1:k_to_select, drop = FALSE]
  Lambda_orig_i <- Lambda_informative[1:k_to_select]
  
  return(list(vectors = U_orig_i, values = Lambda_orig_i))
}
</file>

<file path="R/task_graph_construction.R">
#' Compute Task-Based Parcel Similarity Graph (W_task) from Activations
#'
#' Calculates a sparse, z-scored similarity graph between parcels based on their
#' activation profiles across different conditions or task features.
#'
#' @param activation_matrix A numeric matrix (`C x V_p`) where `C` is the number
#'   of conditions/features and `V_p` is the number of parcels. Each column
#'   represents the activation profile for a parcel.
#' @param parcel_names A character vector of length `V_p` specifying parcel names.
#' @param k_conn_task_pos Integer, number of strongest positive connections to
#'   retain per parcel during sparsification.
#' @param k_conn_task_neg Integer, number of strongest negative connections to
#'   retain per parcel during sparsification.
#' @param similarity_method Character string or function. Specifies the method to
#'   compute the initial `V_p x V_p` similarity matrix from `activation_matrix`.
#'   If "pearson" (default) or "spearman", `stats::cor` is used.
#'   If a function, it must take `activation_matrix` as input and return a
#'   `V_p x V_p` numeric matrix.
#' @param use_dtw Logical, defaults to `FALSE`. (Placeholder, currently unused but
#'   kept for potential future compatibility or signature consistency).
#'
#' @return A sparse, symmetric `Matrix::dgCMatrix` of size `V_p x V_p`
#'   representing the z-scored task-based similarity graph `W_task_i`.
#'
#' @export
#' @importFrom Matrix Matrix sparseMatrix drop0 t forceSymmetric
#' @importFrom stats cor sd
compute_W_task_from_activations <- function(activation_matrix,
                                            parcel_names,
                                            k_conn_task_pos,
                                            k_conn_task_neg,
                                            similarity_method = "pearson",
                                            use_dtw = FALSE) {

  V_p <- ncol(activation_matrix)
  C_n <- if (is.matrix(activation_matrix)) nrow(activation_matrix) else 0 # Number of conditions/rows

  if (V_p == 0) {
    # Ensure consistent return type (dgCMatrix) for empty graph
    empty_mat <- Matrix::Matrix(0, 0, 0, sparse = TRUE, dimnames = list(character(0), character(0)))
    return(as(empty_mat, "dgCMatrix"))
  }
  if (length(parcel_names) != V_p) {
    stop("Length of 'parcel_names' must match the number of columns (parcels) in 'activation_matrix'.")
  }
  
  # Handle C_n (number of conditions/rows) < 2, as cor() behaves unexpectedly or errors.
  if (C_n < 2) {
    warning(sprintf("compute_W_task_from_activations: activation_matrix has %d row(s) (conditions/features). Need at least 2 for meaningful correlation. Resulting graph will likely be empty or based on zero similarities.", C_n))
    # If C_n is 0 or 1, similarity matrix will be all 0s after NA replacement, leading to empty graph for kNN > 0.
    # So, can proceed, but the graph will be empty. Or return empty graph directly.
    # For safety and explicitness if C_n = 0 (which causes cor error):
    if (C_n == 0) {
        empty_mat_vp <- Matrix::Matrix(0, nrow=V_p, ncol=V_p, sparse=TRUE, dimnames = list(parcel_names, parcel_names))
        return(as(empty_mat_vp, "dgCMatrix"))
    }
    # If C_n = 1, cor returns NAs, which are then set to 0. Resulting graph is empty.
    # This path will be handled by existing logic turning NAs to 0.
  }

  # 1. Compute V_p x V_p dense similarity matrix
  sim_matrix_dense <- NULL
  if (is.character(similarity_method) && similarity_method %in% c("pearson", "spearman")) {
    if (nrow(activation_matrix) > 1) {
        col_sds <- apply(activation_matrix, 2, stats::sd, na.rm = TRUE)
        if (any(col_sds == 0, na.rm = TRUE)) {
            message(sprintf("Found %d parcel(s) with zero variance in activation profiles. Similarities involving these will be affected (likely 0 or NA).", sum(col_sds==0, na.rm=TRUE)))
        }
        # Check for all-constant columns to avoid the base R warning
        if (all(col_sds == 0, na.rm = TRUE)) {
            # All columns have zero variance, set correlation matrix to zeros
            sim_matrix_dense <- matrix(0, nrow = V_p, ncol = V_p)
        } else {
            sim_matrix_dense <- stats::cor(activation_matrix, method = similarity_method, use = "pairwise.complete.obs")
        }
    } else {
        sim_matrix_dense <- stats::cor(activation_matrix, method = similarity_method, use = "pairwise.complete.obs")
    }
  } else if (is.function(similarity_method)) {
    sim_matrix_dense <- tryCatch({
      similarity_method(activation_matrix)
    }, error = function(e) {
      stop(paste("The provided similarity_method function failed:", e$message))
    })
    if (!is.matrix(sim_matrix_dense) || nrow(sim_matrix_dense) != V_p || ncol(sim_matrix_dense) != V_p) {
      stop("The custom similarity_method function must return a V_p x V_p matrix.")
    }
  } else {
    stop("'similarity_method' must be 'pearson', 'spearman', or a function.")
  }

  sim_matrix_dense[is.na(sim_matrix_dense)] <- 0
  diag(sim_matrix_dense) <- 0

  # 2. Sparsify
  row_indices_list <- vector("list", V_p)
  col_indices_list <- vector("list", V_p)
  values_list <- vector("list", V_p)

  if (k_conn_task_pos > 0 || k_conn_task_neg > 0) {
    for (i in 1:V_p) {
      node_sims <- sim_matrix_dense[i, ]
      current_selected_indices <- integer(0)
      current_selected_values <- numeric(0)

      if (k_conn_task_pos > 0) {
        pos_candidates_idx <- which(node_sims > 1e-9) 
        if (length(pos_candidates_idx) > 0) {
          pos_candidates_vals <- node_sims[pos_candidates_idx]
          num_to_keep_pos <- min(k_conn_task_pos, length(pos_candidates_vals))
          ordered_in_pos_group <- order(pos_candidates_vals, decreasing = TRUE)
          top_pos_in_group_idx <- head(ordered_in_pos_group, num_to_keep_pos)

          current_selected_indices <- c(current_selected_indices, pos_candidates_idx[top_pos_in_group_idx])
          current_selected_values <- c(current_selected_values, pos_candidates_vals[top_pos_in_group_idx])
        }
      }

      if (k_conn_task_neg > 0) {
        neg_candidates_idx <- which(node_sims < -1e-9) 
        if (length(neg_candidates_idx) > 0) {
          neg_candidates_vals <- node_sims[neg_candidates_idx]
          num_to_keep_neg <- min(k_conn_task_neg, length(neg_candidates_vals))
          ordered_in_neg_group <- order(neg_candidates_vals, decreasing = FALSE)
          top_neg_in_group_idx <- head(ordered_in_neg_group, num_to_keep_neg)

          current_selected_indices <- c(current_selected_indices, neg_candidates_idx[top_neg_in_group_idx])
          current_selected_values <- c(current_selected_values, neg_candidates_vals[top_neg_in_group_idx])
        }
      }

      if(length(current_selected_indices) > 0) {
          row_indices_list[[i]] <- rep(i, length(current_selected_indices))
          col_indices_list[[i]] <- current_selected_indices
          values_list[[i]] <- current_selected_values
      }
    }
  }

  final_row_indices <- unlist(row_indices_list[!sapply(row_indices_list, is.null)])
  final_col_indices <- unlist(col_indices_list[!sapply(col_indices_list, is.null)])
  final_values <- unlist(values_list[!sapply(values_list, is.null)])

  W_dir_task <- if (length(final_row_indices) > 0) {
    Matrix::sparseMatrix(
      i = final_row_indices, j = final_col_indices, x = final_values,
      dims = c(V_p, V_p), dimnames = list(parcel_names, parcel_names)
    )
  } else {
    Matrix::Matrix(0, nrow=V_p, ncol=V_p, sparse=TRUE, dimnames = list(parcel_names, parcel_names))
  }
  W_dir_task <- Matrix::drop0(W_dir_task)

  # 3. Symmetrize
  W_dir_task_t <- Matrix::t(W_dir_task)
  W_sum_task <- W_dir_task + W_dir_task_t
  
  W_den_task_val <- as.numeric((W_dir_task != 0) + (W_dir_task_t != 0))
  W_den_task <- Matrix::Matrix(pmax(1, W_den_task_val), nrow=V_p, ncol=V_p, sparse=TRUE)

  W_symmetric_raw_task <- W_sum_task / W_den_task
  if (inherits(W_symmetric_raw_task, "sparseMatrix")) {
    if (any(is.nan(W_symmetric_raw_task@x))) W_symmetric_raw_task@x[is.nan(W_symmetric_raw_task@x)] <- 0
    if (any(is.infinite(W_symmetric_raw_task@x))) W_symmetric_raw_task@x[is.infinite(W_symmetric_raw_task@x)] <- 0
  } else {
    W_symmetric_raw_task[is.nan(W_symmetric_raw_task)] <- 0
    W_symmetric_raw_task[is.infinite(W_symmetric_raw_task)] <- 0
  }
  W_symmetric_raw_task <- Matrix::drop0(W_symmetric_raw_task)
  W_symmetric_task <- Matrix::forceSymmetric(W_symmetric_raw_task, uplo = "U")

  # 4. Z-score non-zero edge weights
  if (length(W_symmetric_task@x) > 0) { 
    non_zero_vals <- W_symmetric_task@x
    mean_val <- mean(non_zero_vals)
    sd_val <- stats::sd(non_zero_vals)
    if (is.na(sd_val) || sd_val < 1e-10) { 
      W_symmetric_task@x <- rep(0, length(non_zero_vals))
    } else {
      # Apply z-scoring
      z_scored_vals <- (non_zero_vals - mean_val) / sd_val
      # Verify the standard deviation is exactly 1.0 (for test precision)
      actual_sd <- stats::sd(z_scored_vals)
      if (!is.na(actual_sd) && actual_sd > 0) {
        # Fine-tune to ensure sd is exactly 1.0
        z_scored_vals <- z_scored_vals / actual_sd
      }
      W_symmetric_task@x <- z_scored_vals
    }
    W_task_i <- Matrix::drop0(W_symmetric_task)
  } else {
    W_task_i <- W_symmetric_task 
  }
  
  return(as(W_task_i, "dgCMatrix"))
}

#' Compute Task-Based Parcel Similarity Graph (W_task) from Encoding Weights
#'
#' Calculates a sparse, z-scored similarity graph between parcels based on their
#' encoding weight profiles for a set of features.
#'
#' @param encoding_weights_matrix A numeric matrix (`V_p x N_features`) where `V_p`
#'   is the number of parcels and `N_features` is the number of encoding features.
#'   Each row represents the encoding weight profile for a parcel.
#' @param parcel_names A character vector of length `V_p` specifying parcel names.
#' @param k_conn_task_pos Integer, number of strongest positive connections to
#'   retain per parcel during sparsification.
#' @param k_conn_task_neg Integer, number of strongest negative connections to
#'   retain per parcel during sparsification.
#' @param similarity_method Character string or function. Specifies the method to
#'   compute the initial `V_p x V_p` similarity matrix.
#'   If "pearson" (default) or "spearman", `stats::cor` is used on the
#'   transposed input (to compare rows/parcels).
#'   If a function, it must take `encoding_weights_matrix` (V_p x N_features)
#'   as input and return a `V_p x V_p` numeric matrix.
#'
#' @return A sparse, symmetric `Matrix::dgCMatrix` of size `V_p x V_p`
#'   representing the z-scored task-based similarity graph `W_task_i`.
#'
#' @export
#' @importFrom Matrix Matrix sparseMatrix drop0 t forceSymmetric
#' @importFrom stats cor sd
compute_W_task_from_encoding <- function(encoding_weights_matrix,
                                         parcel_names,
                                         k_conn_task_pos,
                                         k_conn_task_neg,
                                         similarity_method = "pearson") {

  V_p <- nrow(encoding_weights_matrix) # Parcels are rows
  N_features <- ncol(encoding_weights_matrix)

  if (V_p == 0) {
    # Ensure consistent return type (dgCMatrix) for empty graph
    empty_mat <- Matrix::Matrix(0, 0, 0, sparse = TRUE, dimnames = list(character(0), character(0)))
    return(as(empty_mat, "dgCMatrix"))
  }
  if (length(parcel_names) != V_p) {
    stop("Length of 'parcel_names' must match the number of rows (parcels) in 'encoding_weights_matrix'.")
  }
  if (N_features == 0) {
     warning("compute_W_task_from_encoding: encoding_weights_matrix has zero features. Resulting graph will be empty.")
     # Ensure consistent return type (dgCMatrix) for empty graph
     empty_mat <- Matrix::Matrix(0, nrow=V_p, ncol=V_p, sparse=TRUE, dimnames = list(parcel_names, parcel_names))
     return(as(empty_mat, "dgCMatrix"))
  }

  # 1. Compute V_p x V_p dense similarity matrix
  sim_matrix_dense <- NULL
  if (is.character(similarity_method) && similarity_method %in% c("pearson", "spearman")) {
    if (N_features > 1) { 
        row_sds <- apply(encoding_weights_matrix, 1, stats::sd, na.rm = TRUE)
        if (any(row_sds == 0, na.rm = TRUE)) {
            message(sprintf("Found %d parcel(s) with zero variance in encoding weights. Similarities involving these will be affected.", sum(row_sds==0, na.rm=TRUE)))
        }
    }
    sim_matrix_dense <- stats::cor(t(encoding_weights_matrix), method = similarity_method, use = "pairwise.complete.obs")
  } else if (is.function(similarity_method)) {
    sim_matrix_dense <- tryCatch({
      similarity_method(encoding_weights_matrix)
    }, error = function(e) {
      stop(paste("The provided similarity_method function failed:", e$message))
    })
    if (!is.matrix(sim_matrix_dense) || nrow(sim_matrix_dense) != V_p || ncol(sim_matrix_dense) != V_p) {
      stop("The custom similarity_method function must return a V_p x V_p matrix.")
    }
  } else {
    stop("'similarity_method' must be 'pearson', 'spearman', or a function.")
  }

  sim_matrix_dense[is.na(sim_matrix_dense)] <- 0
  diag(sim_matrix_dense) <- 0

  # 2. Sparsify
  row_indices_list <- vector("list", V_p)
  col_indices_list <- vector("list", V_p)
  values_list <- vector("list", V_p)

  if (k_conn_task_pos > 0 || k_conn_task_neg > 0) {
    for (i in 1:V_p) {
      node_sims <- sim_matrix_dense[i, ]
      current_selected_indices <- integer(0)
      current_selected_values <- numeric(0)

      if (k_conn_task_pos > 0) {
        pos_candidates_idx <- which(node_sims > 1e-9)
        if (length(pos_candidates_idx) > 0) {
          pos_candidates_vals <- node_sims[pos_candidates_idx]
          num_to_keep_pos <- min(k_conn_task_pos, length(pos_candidates_vals))
          ordered_in_pos_group <- order(pos_candidates_vals, decreasing = TRUE)
          top_pos_in_group_idx <- head(ordered_in_pos_group, num_to_keep_pos)
          current_selected_indices <- c(current_selected_indices, pos_candidates_idx[top_pos_in_group_idx])
          current_selected_values <- c(current_selected_values, pos_candidates_vals[top_pos_in_group_idx])
        }
      }

      if (k_conn_task_neg > 0) {
        neg_candidates_idx <- which(node_sims < -1e-9)
        if (length(neg_candidates_idx) > 0) {
          neg_candidates_vals <- node_sims[neg_candidates_idx]
          num_to_keep_neg <- min(k_conn_task_neg, length(neg_candidates_vals))
          ordered_in_neg_group <- order(neg_candidates_vals, decreasing = FALSE)
          top_neg_in_group_idx <- head(ordered_in_neg_group, num_to_keep_neg)
          current_selected_indices <- c(current_selected_indices, neg_candidates_idx[top_neg_in_group_idx])
          current_selected_values <- c(current_selected_values, neg_candidates_vals[top_neg_in_group_idx])
        }
      }

      if(length(current_selected_indices) > 0) {
          row_indices_list[[i]] <- rep(i, length(current_selected_indices))
          col_indices_list[[i]] <- current_selected_indices
          values_list[[i]] <- current_selected_values
      }
    }
  }

  final_row_indices <- unlist(row_indices_list[!sapply(row_indices_list, is.null)])
  final_col_indices <- unlist(col_indices_list[!sapply(col_indices_list, is.null)])
  final_values <- unlist(values_list[!sapply(values_list, is.null)])

  W_dir_task <- if (length(final_row_indices) > 0) {
    Matrix::sparseMatrix(
      i = final_row_indices, j = final_col_indices, x = final_values,
      dims = c(V_p, V_p), dimnames = list(parcel_names, parcel_names)
    )
  } else {
    Matrix::Matrix(0, nrow=V_p, ncol=V_p, sparse=TRUE, dimnames = list(parcel_names, parcel_names))
  }
  W_dir_task <- Matrix::drop0(W_dir_task)

  # 3. Symmetrize
  W_dir_task_t <- Matrix::t(W_dir_task)
  W_sum_task <- W_dir_task + W_dir_task_t
  W_den_task_val <- as.numeric((W_dir_task != 0) + (W_dir_task_t != 0))
  W_den_task <- Matrix::Matrix(pmax(1, W_den_task_val), nrow=V_p, ncol=V_p, sparse=TRUE)
  W_symmetric_raw_task <- W_sum_task / W_den_task
  if (inherits(W_symmetric_raw_task, "sparseMatrix")) {
    if (any(is.nan(W_symmetric_raw_task@x))) W_symmetric_raw_task@x[is.nan(W_symmetric_raw_task@x)] <- 0
    if (any(is.infinite(W_symmetric_raw_task@x))) W_symmetric_raw_task@x[is.infinite(W_symmetric_raw_task@x)] <- 0
  } else {
    W_symmetric_raw_task[is.nan(W_symmetric_raw_task)] <- 0
    W_symmetric_raw_task[is.infinite(W_symmetric_raw_task)] <- 0
  }
  W_symmetric_raw_task <- Matrix::drop0(W_symmetric_raw_task)
  W_symmetric_task <- Matrix::forceSymmetric(W_symmetric_raw_task, uplo = "U")

  # 4. Z-score non-zero edge weights
  if (length(W_symmetric_task@x) > 0) {
    non_zero_vals <- W_symmetric_task@x
    mean_val <- mean(non_zero_vals)
    sd_val <- stats::sd(non_zero_vals)
    if (is.na(sd_val) || sd_val < 1e-10) {
      W_symmetric_task@x <- rep(0, length(non_zero_vals))
    } else {
      # Apply z-scoring
      z_scored_vals <- (non_zero_vals - mean_val) / sd_val
      # Verify the standard deviation is exactly 1.0 (for test precision)
      actual_sd <- stats::sd(z_scored_vals)
      if (!is.na(actual_sd) && actual_sd > 0) {
        # Fine-tune to ensure sd is exactly 1.0
        z_scored_vals <- z_scored_vals / actual_sd
      }
      W_symmetric_task@x <- z_scored_vals
    }
    W_task_i <- Matrix::drop0(W_symmetric_task)
  } else {
    W_task_i <- W_symmetric_task
  }
  
  return(as(W_task_i, "dgCMatrix"))
}

#' Compute Correlation Between Two Sparse Graphs
#'
#' Calculates the Spearman correlation between the edge weights of two sparse graphs,
#' considering the union of their non-zero edges and filling missing edges with 0.
#' Only the upper triangle of the matrices is considered.
#'
#' @param W_graph1 A sparse, symmetric matrix (`Matrix::dgCMatrix`, `V_p x V_p`).
#'   Assumed to have z-scored non-zero entries.
#' @param W_graph2 A sparse, symmetric matrix (`Matrix::dgCMatrix`, `V_p x V_p`).
#'   Assumed to have z-scored non-zero entries.
#' @param max_edges An integer or `Inf`. If the number of unique edges in the
#'   union of the upper triangles exceeds `max_edges`, a random sample of
#'   `max_edges` edges will be used for the correlation calculation. Defaults to 2,000,000.
#'
#' @return The Spearman correlation coefficient (`rho`) between the edge weights
#'   of the two graphs based on the union of their upper triangle edges. Returns `NA`
#'   if correlation cannot be computed (e.g., too few edges, zero variance).
#'
#' @export
#' @importFrom Matrix summary
#' @importFrom methods is
#' @importFrom stats cor sd
compute_graph_correlation <- function(W_graph1, W_graph2, max_edges = 2000000) {

  if (!is(W_graph1, "sparseMatrix") || !is(W_graph2, "sparseMatrix")) {
    stop("Inputs must be sparse matrices (e.g., dgCMatrix).")
  }
  if (nrow(W_graph1) != ncol(W_graph1) || nrow(W_graph2) != ncol(W_graph2) || nrow(W_graph1) != nrow(W_graph2)) {
    stop("Input graphs must be square matrices of the same dimension.")
  }

  # Extract triplets (i, j, x) from summaries
  summary1 <- Matrix::summary(W_graph1)
  summary2 <- Matrix::summary(W_graph2)

  # Filter for upper triangle (i < j)
  # Using base subset for potential slight speed advantage on large summaries
  edges1 <- subset(summary1, summary1$i < summary1$j, select = c("i", "j", "x"))
  edges2 <- subset(summary2, summary2$i < summary2$j, select = c("i", "j", "x"))

  # Rename columns for merging
  names(edges1) <- c("row", "col", "w1")
  names(edges2) <- c("row", "col", "w2")

  # Merge based on row and col to get the union of edges (full outer join)
  # Handle empty edge cases
  if (nrow(edges1) == 0 && nrow(edges2) == 0) {
      return(NA_real_) # No edges in either graph
  } else if (nrow(edges1) == 0) {
      merged_edges <- data.frame(row = edges2$row, col = edges2$col, w1 = 0, w2 = edges2$w2)
  } else if (nrow(edges2) == 0) {
      merged_edges <- data.frame(row = edges1$row, col = edges1$col, w1 = edges1$w1, w2 = 0)
  } else {
       merged_edges <- merge(edges1, edges2, by = c("row", "col"), all = TRUE)
  }


  # Sample if needed
  num_edges <- nrow(merged_edges)
  if (num_edges > max_edges && is.finite(max_edges) && max_edges > 0) {
    if (interactive()) {
        message(sprintf("Sampling %d edges from union of %d for correlation calculation.", max_edges, num_edges))
    }
    sample_indices <- sample.int(num_edges, size = max_edges, replace = FALSE)
    merged_edges <- merged_edges[sample_indices, ]
  }

  # Fill missing values (NAs introduced by the full outer join) with 0
  merged_edges$w1[is.na(merged_edges$w1)] <- 0
  merged_edges$w2[is.na(merged_edges$w2)] <- 0

  # Compute Spearman correlation
  if (nrow(merged_edges) < 2) {
    warning("Too few edges (< 2) in the sampled union to compute correlation. Returning NA.")
    return(NA_real_)
  }

  # Check for zero variance before attempting correlation
  sd1 <- stats::sd(merged_edges$w1)
  sd2 <- stats::sd(merged_edges$w2)

  # Handle cases where correlation is undefined
  # stats::cor returns NA in these cases already, but adding a warning is helpful.
  if (is.na(sd1) || sd1 == 0 || is.na(sd2) || sd2 == 0) {
    warning("Zero variance in edge weights for at least one graph in the sampled union set. Correlation is undefined (NA).")
    # Let stats::cor return NA
  }

  rho <- stats::cor(merged_edges$w1, merged_edges$w2, method = "spearman", use = "complete.obs") # use="complete.obs" shouldn't be needed after NA fill

  return(rho)
}

# Internal helper to sparsify and z-score a symmetric matrix
# Takes a symmetric matrix (potentially dense) and applies k-NN
# sparsification using the same k for positive and negative edges.
# Then z-scores the non-zero elements.
.sparsify_symmetric_matrix <- function(input_matrix, k_nn, parcel_names) {
  V_p <- nrow(input_matrix)
  if (V_p == 0) {
    return(Matrix::Matrix(0, 0, 0, sparse = TRUE, dimnames = list(character(0), character(0))))
  }

  # Ensure input is a matrix (might be sparse or dense)
  input_matrix_dense <- as.matrix(input_matrix)
  diag(input_matrix_dense) <- 0 # Ensure diagonal is zero

  row_indices_list <- vector("list", V_p)
  col_indices_list <- vector("list", V_p)
  values_list <- vector("list", V_p)

  if (k_nn > 0) {
    for (i in 1:V_p) {
      node_vals <- input_matrix_dense[i, ]
      selected_indices <- integer(0)
      selected_values <- numeric(0)

      # Keep k largest positive values
      pos_idx <- which(node_vals > 1e-9)
      if (length(pos_idx) > 0) {
        pos_vals <- node_vals[pos_idx]
        num_keep <- min(k_nn, length(pos_vals))
        ordered_idx <- order(pos_vals, decreasing = TRUE)
        top_idx_local <- head(ordered_idx, num_keep)
        selected_indices <- c(selected_indices, pos_idx[top_idx_local])
        selected_values <- c(selected_values, pos_vals[top_idx_local])
      }

      # Keep k most negative values (smallest values)
      neg_idx <- which(node_vals < -1e-9)
      if (length(neg_idx) > 0) {
        neg_vals <- node_vals[neg_idx]
        num_keep <- min(k_nn, length(neg_vals))
        ordered_idx <- order(neg_vals, decreasing = FALSE)
        top_idx_local <- head(ordered_idx, num_keep)
        selected_indices <- c(selected_indices, neg_idx[top_idx_local])
        selected_values <- c(selected_values, neg_vals[top_idx_local])
      }

      if(length(selected_indices) > 0) {
        row_indices_list[[i]] <- rep(i, length(selected_indices))
        col_indices_list[[i]] <- selected_indices
        values_list[[i]] <- selected_values
      }
    }
  }

  final_row_indices <- unlist(row_indices_list[!sapply(row_indices_list, is.null)])
  final_col_indices <- unlist(col_indices_list[!sapply(col_indices_list, is.null)])
  final_values <- unlist(values_list[!sapply(values_list, is.null)])

  W_dir <- if (length(final_row_indices) > 0) {
    Matrix::sparseMatrix(
      i = final_row_indices, j = final_col_indices, x = final_values,
      dims = c(V_p, V_p), dimnames = list(parcel_names, parcel_names)
    )
  } else {
    Matrix::Matrix(0, nrow=V_p, ncol=V_p, sparse=TRUE, dimnames = list(parcel_names, parcel_names))
  }
  W_dir <- Matrix::drop0(W_dir)

  # Symmetrize
  W_dir_t <- Matrix::t(W_dir)
  W_sum <- W_dir + W_dir_t
  W_den_val <- as.numeric((W_dir != 0) + (W_dir_t != 0))
  W_den <- Matrix::Matrix(pmax(1, W_den_val), nrow=V_p, ncol=V_p, sparse=TRUE)
  W_sym_raw <- W_sum / W_den
  if (inherits(W_sym_raw, "sparseMatrix")) {
    if (any(is.nan(W_sym_raw@x))) W_sym_raw@x[is.nan(W_sym_raw@x)] <- 0
    if (any(is.infinite(W_sym_raw@x))) W_sym_raw@x[is.infinite(W_sym_raw@x)] <- 0
  } else {
    W_sym_raw[is.nan(W_sym_raw)] <- 0
    W_sym_raw[is.infinite(W_sym_raw)] <- 0
  }
  W_sym_raw <- Matrix::drop0(W_sym_raw)
  W_sym <- Matrix::forceSymmetric(W_sym_raw, uplo = "U")

  # Z-score
  if (length(W_sym@x) > 0) {
    non_zero_vals <- W_sym@x
    mean_val <- mean(non_zero_vals)
    sd_val <- stats::sd(non_zero_vals)
    if (is.na(sd_val) || sd_val < 1e-10) {
      W_sym@x <- rep(0, length(non_zero_vals))
    } else {
      # Apply z-scoring
      z_scored_vals <- (non_zero_vals - mean_val) / sd_val
      # Verify the standard deviation is exactly 1.0 (for test precision)
      actual_sd <- stats::sd(z_scored_vals)
      if (!is.na(actual_sd) && actual_sd > 0) {
        # Fine-tune to ensure sd is exactly 1.0
        z_scored_vals <- z_scored_vals / actual_sd
      }
      W_sym@x <- z_scored_vals
    }
    W_final <- Matrix::drop0(W_sym)
  } else {
    W_final <- W_sym
  }

  return(as(W_final, "dgCMatrix"))
}

#' Residualize Graph B based on Subspace from Graph A's Laplacian
#'
#' Projects `W_graph_to_residualize` onto the subspace spanned by the first
#' `k_eigenvectors_to_remove` smallest eigenvectors of
#' `L_graph_for_projection` and subtracts this projection.
#' The residual graph is then re-sparsified and re-z-scored.
#'
#' @param W_graph_to_residualize The sparse graph matrix (`dgCMatrix`) to be
#'   residualized (e.g., `W_task`).
#' @param L_graph_for_projection The sparse Laplacian matrix (`dgCMatrix`) from
#'   which the projection subspace is derived (e.g., `L_conn`). Must be symmetric.
#' @param k_eigenvectors_to_remove Integer, the number of smallest (by magnitude)
#'   eigenvectors of `L_graph_for_projection` to define the subspace for projection.
#'   Defaults to 64.
#' @param k_nn_resparsify Integer, the k value for k-NN sparsification applied
#'   to the residual graph (same k used for positive and negative edges).
#' @param eigenvalue_tol Numeric, tolerance for eigenvalue decomposition convergence
#'   and for identifying near-zero eigenvalues if needed (though projection uses the space).
#'   Default 1e-9.
#'
#' @return A sparse, symmetric, z-scored `dgCMatrix` representing the residualized graph.
#'
#' @importFrom Matrix t crossprod tcrossprod forceSymmetric
#' @importFrom RSpectra eigs_sym
#' @importFrom methods is
#' @keywords internal
residualize_graph_on_subspace <- function(W_graph_to_residualize,
                                          L_graph_for_projection,
                                          k_eigenvectors_to_remove = 64,
                                          k_nn_resparsify,
                                          eigenvalue_tol = 1e-9) {

  if (!is(W_graph_to_residualize, "sparseMatrix")) stop("W_graph_to_residualize must be a sparse Matrix.")
  if (!is(L_graph_for_projection, "sparseMatrix")) stop("L_graph_for_projection must be a sparse Matrix.")
  V_p <- nrow(W_graph_to_residualize)
  if (V_p == 0) return(W_graph_to_residualize) # Return empty graph if input is empty
  if (nrow(L_graph_for_projection) != V_p || ncol(L_graph_for_projection) != V_p || ncol(W_graph_to_residualize) != V_p) {
    stop("Input matrices must be square and of the same dimension.")
  }
  if (missing(k_nn_resparsify) || !is.numeric(k_nn_resparsify) || k_nn_resparsify <= 0) {
     stop("`k_nn_resparsify` must be a positive integer.")
  }

  k_proj <- min(k_eigenvectors_to_remove, V_p - 1) # Cannot request more than V_p-1
  if (k_proj <= 0) {
    warning("k_eigenvectors_to_remove is too small or V_p <= 1. Returning original graph.")
    return(W_graph_to_residualize)
  }

  # 1. Get eigenvectors U of L_graph_for_projection (L_A)
  message_stage(sprintf("Computing %d eigenvectors of projection Laplacian...", k_proj), interactive_only = TRUE)
  eigs_result <- tryCatch({
    RSpectra::eigs_sym(L_graph_for_projection,
                       k = k_proj,
                       which = "SM", # Smallest Magnitude
                       opts = list(retvec = TRUE, tol = eigenvalue_tol))
  }, error = function(e) {
    warning(paste("Eigen decomposition failed:", e$message, ". Returning original graph."))
    return(NULL)
  })

  if (is.null(eigs_result) || is.null(eigs_result$vectors) || ncol(eigs_result$vectors) < k_proj) {
      warning(sprintf("Eigen decomposition did not return the requested %d vectors. Returning original graph.", k_proj))
      return(W_graph_to_residualize)
  }
  U <- eigs_result$vectors # V_p x k_proj dense matrix
  
  # Orthonormalize U (important for projection formula)
  # Using SVD: U_ortho = svd(U)$u is robust
  # Note: svd(U)$u will have min(V_p, k_proj) columns. Should be k_proj if k_proj < V_p.
  U_ortho <- svd(U, nu = k_proj, nv = 0)$u
  if (ncol(U_ortho) != k_proj) {
      warning("Orthonormalization of eigenvectors failed to produce correct dimensions. Returning original graph.")
      return(W_graph_to_residualize)
  }

  # 2. Compute residual using low-rank formula
  # W_res = W_B - P W_B - W_B P + P W_B P, where P = U U^T
  # W_res = W_B - U(U^T W_B) - (W_B U)U^T + U(U^T W_B U)U^T
  # Let W_B be the graph to residualize
  message_stage("Computing residual graph projection...", interactive_only = TRUE)
  W_B <- W_graph_to_residualize
  U <- U_ortho # Use orthonormalized version
  
  # Calculate intermediate terms (sparse-dense and dense-dense products)
  # UT_WB = t(U) %*% W_B  (k_proj x V_p, result is dense)
  UT_WB <- Matrix::crossprod(U, W_B)
  # WB_U = W_B %*% U      (V_p x k_proj, result is dense)
  WB_U <- W_B %*% U
  # UT_WB_U = t(U) %*% W_B %*% U = UT_WB %*% U (k_proj x k_proj, dense)
  UT_WB_U <- UT_WB %*% U

  # Calculate terms of the residual formula
  # Term1 = U %*% UT_WB   (V_p x V_p, dense)
  Term1 <- U %*% UT_WB
  # Term2 = WB_U %*% t(U) (V_p x V_p, dense)
  Term2 <- Matrix::tcrossprod(WB_U, U)
  # Term3 = U %*% UT_WB_U %*% t(U) (V_p x V_p, dense)
  Term3 <- U %*% UT_WB_U %*% Matrix::t(U)
  
  # Compute residual W_res (potentially dense)
  # Ensure W_B is treated as dense for subtraction if needed
  if (!is(W_B,"matrix")) W_B_dense <- as.matrix(W_B)
  else W_B_dense <- W_B
  
  W_res_dense <- W_B_dense - Term1 - Term2 + Term3
  message_stage("Symmetrizing residual graph...", interactive_only = TRUE)

  # 3. Symmetrize W_res
  W_res_sym <- (W_res_dense + t(W_res_dense)) / 2
  
  # Get parcel names for the helper function
  pnames <- rownames(W_graph_to_residualize) # Assume dimnames exist and match
  if (is.null(pnames)) pnames <- paste0("P", 1:V_p)

  # 4. Re-sparsify and Re-z-score using the helper
  message_stage(sprintf("Re-sparsifying residual graph (k=%d)...", k_nn_resparsify), interactive_only = TRUE)
  W_res_final <- .sparsify_symmetric_matrix(W_res_sym, k_nn = k_nn_resparsify, parcel_names = pnames)

  message_stage("Residualization complete.", interactive_only = TRUE)
  return(W_res_final)
}

#' Blend Connectivity and Task Laplacians
#'
#' Combines a connectivity-based Laplacian and a task-based Laplacian using
#' a specified blending method. Currently, only linear blending is supported.
#'
#' @param L_conn A sparse Laplacian matrix derived from connectivity (`dgCMatrix`, `V_p x V_p`).
#' @param L_task A sparse Laplacian matrix derived from task activations/encodings
#'   (`dgCMatrix`, `V_p x V_p`). Must have the same dimensions as `L_conn`.
#' @param lambda_blend_value Numeric, the blending factor (`λ`). Must be between 0 and 1.
#'   `L_hybrid = (1 - λ) * L_conn + λ * L_task`.
#' @param method Character string, the blending method. Currently only "linear" is supported.
#'
#' @return The blended sparse Laplacian matrix `L_hybrid` (`dgCMatrix`, `V_p x V_p`).
#'
#' @importFrom methods is
#' @keywords internal
blend_laplacians <- function(L_conn, L_task, lambda_blend_value, method = "linear") {

  if (!is(L_conn, "sparseMatrix") || !is(L_task, "sparseMatrix")) {
    stop("Inputs L_conn and L_task must be sparse matrices.")
  }
  if (nrow(L_conn) != ncol(L_conn) || nrow(L_task) != ncol(L_task) || nrow(L_conn) != nrow(L_task)) {
    stop("Input Laplacians must be square matrices of the same dimension.")
  }
  if (!is.numeric(lambda_blend_value) || length(lambda_blend_value) != 1 || lambda_blend_value < 0 || lambda_blend_value > 1) {
    stop("lambda_blend_value must be a single numeric value between 0 and 1.")
  }

  if (tolower(method) == "linear") {
    if (lambda_blend_value == 0) return(L_conn)
    if (lambda_blend_value == 1) return(L_task)

    # Sparse matrix arithmetic handles the weighted sum efficiently
    L_hybrid <- (1 - lambda_blend_value) * L_conn + lambda_blend_value * L_task

    # Ensure output is dgCMatrix
    return(as(L_hybrid, "dgCMatrix"))

  } else {
    # Placeholder for future methods like "geo"
    stop(sprintf("Blending method '%s' is not currently supported. Only 'linear' is implemented.", method))
  }
}
</file>

<file path="R/task_hatsa_helpers.R">
# R/task_hatsa_helpers.R

#' Validate arguments and initialize variables for task_hatsa
#'
#' @param subject_data_list List of subject data matrices
#' @param anchor_indices Indices of anchor parcels
#' @param spectral_rank_k Spectral rank k
#' @param task_data_list List of task data
#' @param task_method Method for task incorporation
#' @param lambda_blend_value Blend value for lambda_blend task method
#' @param k_gev_dims Number of dimensions for gev_patch method
#' @param row_augmentation Whether to augment with task rows
#' @param residualize_condition_anchors Whether to residualize condition anchors
#' @param omega_weights Fixed weights for omega_mode
#' @param omega_mode Mode for omega calculation
#' @param reliability_scores_list List of reliability scores for adaptive weighting
#' @param scale_omega_trace Whether to scale omega trace
#' @param alpha_laplacian Alpha parameter for graph Laplacian
#' @param degree_type_laplacian Degree type for graph Laplacian
#' @param k_conn_pos Number of positive connections for connectivity graph
#' @param k_conn_neg Number of negative connections for connectivity graph
#' @param k_conn_task_pos Number of positive connections for task graph
#' @param k_conn_task_neg Number of negative connections for task graph
#' @param similarity_method_task Similarity method for task graph
#' @param W_task_helper_func Function to compute task graph from task data
#' @param n_refine Number of GPA refinement iterations
#' @param check_redundancy Whether to check for redundancy
#' @param redundancy_threshold Threshold for redundancy check
#' @param residualize_k_conn_proj Number of eigenvectors to remove for residualization
#' @param residualize_k_conn_labels Number of nearest neighbors to resparsify for residualization
#' @param gev_lambda_max Maximum eigenvalue for gev_patch method
#' @param gev_epsilon_reg Regularization parameter for gev_patch method
#' @param parcel_names Names of parcels
#' @param verbose Whether to print messages
#'
#' @return List of validated arguments and initialized variables
#' @noRd
validate_and_initialize_args <- function(
    subject_data_list,
    anchor_indices,
    spectral_rank_k,
    task_data_list,
    task_method,
    lambda_blend_value,
    k_gev_dims,
    row_augmentation,
    residualize_condition_anchors,
    omega_weights,
    omega_mode,
    reliability_scores_list,
    scale_omega_trace,
    alpha_laplacian,
    degree_type_laplacian,
    k_conn_pos,
    k_conn_neg,
    k_conn_task_pos,
    k_conn_task_neg,
    similarity_method_task,
    W_task_helper_func,
    n_refine,
    check_redundancy,
    redundancy_threshold,
    residualize_k_conn_proj,
    residualize_k_conn_labels,
    gev_lambda_max,
    gev_epsilon_reg,
    parcel_names,
    verbose
) {
    if (verbose) message_stage(sprintf("Starting task_hatsa run (method: %s)...", task_method), interactive_only = TRUE)

    if (!is.list(subject_data_list) || length(subject_data_list) == 0) {
        stop("subject_data_list must be a non-empty list.")
    }
    N_subjects <- length(subject_data_list)

    first_valid_subj_idx <- which(sapply(subject_data_list, function(x) !is.null(x) && is.matrix(x) && ncol(x) > 0))[1]
    if (is.na(first_valid_subj_idx)) stop("No valid subject data found in subject_data_list.")
    V_p <- ncol(subject_data_list[[first_valid_subj_idx]])
    if (V_p == 0) stop("Subject data has 0 columns (V_p = 0).")

    if (is.null(parcel_names)) {
        parcel_names <- paste0("P", 1:V_p)
    } else if (length(parcel_names) != V_p) {
        stop(sprintf("parcel_names length (%d) does not match inferred V_p (%d).", length(parcel_names), V_p))
    }

    if (!is.numeric(anchor_indices) || any(anchor_indices < 1) || any(anchor_indices > V_p)) {
        stop("anchor_indices must be numeric indices between 1 and V_p.")
    }
    m_parcel_rows <- length(anchor_indices)
    if (m_parcel_rows == 0) warning("No parcel anchor indices provided.")
    if (spectral_rank_k <= 0) stop("spectral_rank_k must be a positive integer.")
    if (spectral_rank_k >= V_p) warning("spectral_rank_k should generally be less than V_p.")

    # First check if this is core_hatsa and handle it separately (allow NULL task_data_list)
    if (task_method == "core_hatsa") {
        # For core_hatsa, we allow NULL task_data_list but disable row_augmentation if it's NULL
        if (row_augmentation && is.null(task_data_list)) {
            if (verbose) message("row_augmentation is TRUE but task_data_list is NULL. Disabling row augmentation for core_hatsa.")
            row_augmentation <- FALSE
        }
    } else {
        # For non-core_hatsa methods, task_data_list is required
        needs_task_data <- task_method != "core_hatsa" || row_augmentation
        if (needs_task_data && (is.null(task_data_list) || length(task_data_list) != N_subjects)) {
            stop("task_data_list is required and must have the same length as subject_data_list when task_method is not 'core_hatsa' or row_augmentation is TRUE.")
        }
    }

    # Handle W_task_helper_func
    current_W_task_helper_func <- W_task_helper_func
    if (task_method != "core_hatsa") {
        if (is.null(current_W_task_helper_func)) {
            warning("W_task_helper_func is NULL. Assuming task_data_list contains activation matrices (C x Vp) for compute_W_task_from_activations.")
            current_W_task_helper_func <- hatsa::compute_W_task_from_activations
        } else if (!is.function(current_W_task_helper_func)) {
            stop("W_task_helper_func must be a function.")
        }
    }

    # Construct the output list with all necessary parameters explicitly included
    out_list <- list(
        N_subjects = N_subjects,
        V_p = V_p,
        m_parcel_rows = m_parcel_rows,
        parcel_names = parcel_names,
        task_method = task_method,
        row_augmentation = row_augmentation,
        lambda_blend_value = lambda_blend_value,
        k_gev_dims = k_gev_dims,
        residualize_condition_anchors = residualize_condition_anchors,
        omega_weights = omega_weights,
        omega_mode = omega_mode,
        reliability_scores_list = reliability_scores_list,
        scale_omega_trace = scale_omega_trace,
        alpha_laplacian = alpha_laplacian,
        degree_type_laplacian = degree_type_laplacian,
        k_conn_pos = k_conn_pos,
        k_conn_neg = k_conn_neg,
        k_conn_task_pos = k_conn_task_pos,
        k_conn_task_neg = k_conn_task_neg,
        similarity_method_task = similarity_method_task,
        W_task_helper_func = current_W_task_helper_func,
        n_refine = n_refine,
        check_redundancy = check_redundancy,
        redundancy_threshold = redundancy_threshold,
        residualize_k_conn_proj = residualize_k_conn_proj,
        residualize_k_conn_labels = residualize_k_conn_labels,
        gev_lambda_max = gev_lambda_max,
        gev_epsilon_reg = gev_epsilon_reg,
        verbose = verbose,
        subject_data_list = subject_data_list,
        anchor_indices = anchor_indices,
        spectral_rank_k = spectral_rank_k,
        task_data_list = task_data_list
    )

    return(out_list)
}

#' Process each subject for basis shaping
#'
#' @param args Validated arguments from validate_and_initialize_args
#' @param subject_data_list List of subject data matrices
#' @param task_data_list List of task data
#' @param anchor_indices Indices of anchor parcels
#'
#' @return List of processing results for each subject
#' @importFrom future.apply future_lapply
#' @noRd
process_subjects <- function(
    args,
    subject_data_list,
    task_data_list,
    anchor_indices
) {
    N_subjects <- args$N_subjects
    verbose <- args$verbose
    
    if (verbose) message_stage(sprintf("Processing %d subjects (Graph construction, Basis shaping) using future_lapply...", N_subjects), interactive_only = TRUE)

    subject_indices <- 1:N_subjects

    # The core logic for a single subject, to be applied in parallel.
    # This function will be serialized and sent to workers if future.globals = FALSE is not specific enough,
    # or if it captures a large environment. Here, it's defined locally so its environment is small.
    # However, process_single_subject and its callees are in the package namespace / helper file scope.
    single_subject_processor_for_future <- function(idx, s_data_list, t_data_list, common_args) {
        # Note: `verbose` from `process_subjects` scope won't be available here if future.globals=FALSE
        # unless explicitly passed or part of common_args and common_args$verbose is used.
        # process_single_subject uses common_args$verbose, which is fine.

        current_subj_data <- s_data_list[[idx]]
        current_task_data <- if (!is.null(t_data_list) && length(t_data_list) >= idx) t_data_list[[idx]] else NULL

        tryCatch({
            result_val <- process_single_subject(
                subject_idx = idx, 
                subj_data_i = current_subj_data, 
                task_data_i = current_task_data, 
                args = common_args
            )
            list(ok = TRUE, subject_idx = idx, result = result_val)
        }, error = function(e) {
            # Optionally log the full error: common_args$verbose might not be reliable here depending on future setup.
            # For now, just capture message.
            warning(sprintf("Error in worker for subject %d: %s", idx, e$message)) # Warning will appear on worker console
            list(ok = FALSE, subject_idx = idx, error_message = e$message, error_object = e)
        })
    }

    all_subject_results_structured <- future.apply::future_lapply(
        X = subject_indices,
        FUN = single_subject_processor_for_future,
        # Pass data and args explicitly to FUN for each iteration
        s_data_list = subject_data_list,
        t_data_list = task_data_list,
        common_args = args, # args now contains all validated & necessary parameters
        future.seed = TRUE,
        future.globals = FALSE # Prevent shipping unnecessary parts of process_subjects environment
                               # Requires FUN to get all its needs from its arguments.
    )

    if (verbose) message_stage(sprintf("Finished parallel processing for %d subjects. Restructuring results...", N_subjects), interactive_only = TRUE)

    # Initialize lists to store results and collect errors
    W_conn_list          <- vector("list", N_subjects)
    L_conn_list          <- vector("list", N_subjects)
    W_task_list          <- vector("list", N_subjects)
    L_task_list          <- vector("list", N_subjects)
    U_original_list      <- vector("list", N_subjects)
    Lambda_original_list <- vector("list", N_subjects)
    U_patch_list         <- vector("list", N_subjects)
    Lambda_patch_list    <- vector("list", N_subjects)
    gev_diagnostics_list <- vector("list", N_subjects)
    qc_metrics_list      <- vector("list", N_subjects)
    
    failed_subjects_info <- list()

    for (i in seq_along(all_subject_results_structured)) {
        res_wrapper <- all_subject_results_structured[[i]]
        # If the result itself is an error (e.g., from future framework), treat as failure
        if (inherits(res_wrapper, "error")) {
             # This case might happen if the error is from the future framework itself, not caught by our tryCatch
            subj_idx <- subject_indices[i] # Best guess for subject index
            err_msg <- conditionMessage(res_wrapper)
            warning(sprintf("Future framework error for subject index %d (approx): %s", subj_idx, err_msg))
            failed_subjects_info[[length(failed_subjects_info) + 1]] <- list(subject_idx = subj_idx, error_message = err_msg)
            # Fill with NULLs for this subject
            W_conn_list[[subj_idx]] <- NULL
            L_conn_list[[subj_idx]] <- NULL
            W_task_list[[subj_idx]] <- NULL
            L_task_list[[subj_idx]] <- NULL
            U_original_list[[subj_idx]] <- NULL
            Lambda_original_list[[subj_idx]] <- NULL
            U_patch_list[[subj_idx]] <- NULL
            Lambda_patch_list[[subj_idx]] <- NULL
            gev_diagnostics_list[[subj_idx]] <- NULL
            qc_metrics_list[[subj_idx]] <- list(rho_redundancy = NA, was_residualized = FALSE) # Default QC
            next
        }


        subj_idx <- res_wrapper$subject_idx # Guarantees correct subject index

        if (res_wrapper$ok) {
            res <- res_wrapper$result
            W_conn_list[[subj_idx]]          <- if (!is.null(res)) res$W_conn else NULL
            L_conn_list[[subj_idx]]          <- if (!is.null(res)) res$L_conn else NULL
            W_task_list[[subj_idx]]          <- if (!is.null(res)) res$W_task else NULL
            L_task_list[[subj_idx]]          <- if (!is.null(res)) res$L_task else NULL
            U_original_list[[subj_idx]]      <- if (!is.null(res)) res$U_original else NULL
            Lambda_original_list[[subj_idx]] <- if (!is.null(res)) res$Lambda_original else NULL
            U_patch_list[[subj_idx]]         <- if (!is.null(res)) res$U_patch else NULL
            Lambda_patch_list[[subj_idx]]    <- if (!is.null(res)) res$Lambda_patch else NULL
            gev_diagnostics_list[[subj_idx]] <- if (!is.null(res)) res$gev_diagnostics else NULL
            qc_metrics_list[[subj_idx]]      <- if (!is.null(res) && !is.null(res$qc_metrics)) {
                                                  res$qc_metrics
                                              } else {
                                                  list(rho_redundancy = NA, was_residualized = FALSE)
                                              }
        } else {
            failed_subjects_info[[length(failed_subjects_info) + 1]] <- list(subject_idx = subj_idx, error_message = res_wrapper$error_message)
            # Fill with NULLs for this subject, qc_metrics with defaults
            W_conn_list[[subj_idx]] <- NULL
            L_conn_list[[subj_idx]] <- NULL
            W_task_list[[subj_idx]] <- NULL
            L_task_list[[subj_idx]] <- NULL
            U_original_list[[subj_idx]] <- NULL
            Lambda_original_list[[subj_idx]] <- NULL
            U_patch_list[[subj_idx]] <- NULL
            Lambda_patch_list[[subj_idx]] <- NULL
            gev_diagnostics_list[[subj_idx]] <- NULL
            qc_metrics_list[[subj_idx]] <- list(rho_redundancy = NA, was_residualized = FALSE)
        }
    }
    
    if (length(failed_subjects_info) > 0) {
        warning_message <- sprintf("Processing failed for %d subject(s):
", length(failed_subjects_info))
        for (fail_info in failed_subjects_info) {
            warning_message <- paste0(warning_message, sprintf("  - Subject %d: %s
", fail_info$subject_idx, fail_info$error_message))
        }
        warning(warning_message, call. = FALSE)
        # Potentially add failed_subjects_info to the returned list or a QC object if more detail is needed upstream.
    }
    
    # The previous loop for restructuring is now replaced by the lapply calls above.
    # The old loop for handling NULL result_i has been incorporated into the lapply logic.

    return(list(
        W_conn_list = W_conn_list,
        L_conn_list = L_conn_list,
        W_task_list = W_task_list,
        L_task_list = L_task_list,
        U_original_list = U_original_list,
        Lambda_original_list = Lambda_original_list,
        U_patch_list = U_patch_list,
        Lambda_patch_list = Lambda_patch_list,
        gev_diagnostics_list = gev_diagnostics_list,
        qc_metrics_list = qc_metrics_list
    ))
}

#' Process a single subject
#'
#' @param subject_idx Subject index.
#' @param subj_data_i Subject data matrix
#' @param task_data_i Task data for the subject
#' @param args Validated arguments
#'
#' @return List of processing results for this subject
#' @noRd
process_single_subject <- function(subject_idx, subj_data_i, task_data_i, args) {
    # Initialize results
    result <- list(
        W_conn = NULL,
        L_conn = NULL,
        W_task = NULL,
        L_task = NULL,
        U_original = NULL, 
        Lambda_original = NULL,
        U_patch = NULL,
        Lambda_patch = NULL,
        gev_diagnostics = NULL,
        qc_metrics = list(rho_redundancy = NA, was_residualized = FALSE)
    )
    
    # Extract arguments from args list for local use if needed, or pass args directly
    V_p <- args$V_p
    verbose <- args$verbose
    parcel_names <- args$parcel_names
    k_conn_pos <- args$k_conn_pos
    k_conn_neg <- args$k_conn_neg
    spectral_rank_k <- args$spectral_rank_k # Used in shape_basis
    task_method <- args$task_method # Used in shape_basis and here
    
    # Basic validation of subject data
    if (is.null(subj_data_i) || !is.matrix(subj_data_i) || ncol(subj_data_i) != V_p) {
        warning(sprintf("Skipping subject %d due to invalid data (expected %d columns).", subject_idx, V_p))
        return(result)
    }
    
    # --- Compute W_conn and L_conn ---
    W_conn_i <- tryCatch({
        compute_subject_connectivity_graph_sparse(subj_data_i, parcel_names, k_conn_pos, k_conn_neg)
    }, error = function(e) {
        warning(sprintf("Error computing W_conn for subject %d: %s. Skipping subject.", subject_idx, e$message)); NULL
    })
    if (is.null(W_conn_i)) return(result) # Return initialized result list (all NULLs essentially)
    result$W_conn <- W_conn_i
    
    L_conn_i <- tryCatch({
        compute_graph_laplacian_sparse(W_conn_i, alpha = args$alpha_laplacian, degree_type = args$degree_type_laplacian)
    }, error = function(e) {
        warning(sprintf("Error computing L_conn for subject %d: %s. Skipping subject.", subject_idx, e$message)); NULL
    })
    if (is.null(L_conn_i)) return(result)
    result$L_conn <- L_conn_i
    
    # --- Compute W_task and L_task if needed ---
    W_task_i_for_basis <- NULL # Initialize to NULL, will be set if task_method requires it
    L_task_i_for_basis <- NULL # Initialize to NULL

    if (task_method != "core_hatsa") {
        task_result_data <- compute_task_matrices(subject_idx, task_data_i, args, W_conn_i, L_conn_i)
        result$W_task <- task_result_data$W_task_i # Store final W_task (raw or residualized)
        result$L_task <- task_result_data$L_task_i # Store final L_task
        result$qc_metrics <- task_result_data$qc_metrics # Update QC from task processing

        # These are used by shape_basis
        W_task_i_for_basis <- result$W_task 
        L_task_i_for_basis <- result$L_task
    }
    
    # --- Perform basis shaping based on task_method ---
    # Pass W_conn_i and potentially W_task_i_for_basis to shape_basis for lambda_blend
    basis_result <- shape_basis(subject_idx, L_conn_i, L_task_i_for_basis, args, W_conn_i, W_task_i_for_basis)
    result$U_original <- basis_result$U_original
    result$Lambda_original <- basis_result$Lambda_original
    result$U_patch <- basis_result$U_patch
    result$Lambda_patch <- basis_result$Lambda_patch
    result$gev_diagnostics <- basis_result$gev_diagnostics
    
    return(result)
}

#' Compute task matrices (W_task and L_task)
#'
#' @param i Subject index. Renamed to subject_idx for clarity.
#' @param task_data_i Task data for subject i
#' @param args Validated arguments
#' @param W_conn_i Connectivity matrix for subject i
#' @param L_conn_i Laplacian of W_conn_i, needed for residualization projection
#'
#' @return List with W_task_i, L_task_i, and qc_metrics
#' @noRd
compute_task_matrices <- function(subject_idx, task_data_i, args, W_conn_i, L_conn_i) {
    result <- list(
        W_task_i = NULL,
        L_task_i = NULL,
        qc_metrics = list(rho_redundancy = NA, was_residualized = FALSE)
    )
    
    # Extract arguments
    verbose <- args$verbose
    parcel_names <- args$parcel_names
    k_conn_task_pos <- args$k_conn_task_pos
    k_conn_task_neg <- args$k_conn_task_neg
    similarity_method_task <- args$similarity_method_task
    W_task_helper_func <- args$W_task_helper_func
    check_redundancy <- args$check_redundancy
    redundancy_threshold <- args$redundancy_threshold
    alpha_laplacian <- args$alpha_laplacian
    degree_type_laplacian <- args$degree_type_laplacian
    
    # Basic validation for task data
    if (is.null(task_data_i) || !is.matrix(task_data_i)) {
        warning(sprintf("Task data for subject %d is missing or not a matrix. Cannot compute W_task.", subject_idx))
        return(result)
    }
    
    # Compute W_task_raw
    W_task_i_raw <- tryCatch({
        W_task_helper_func(task_data_i, parcel_names = parcel_names,
                          k_conn_task_pos = k_conn_task_pos, k_conn_task_neg = k_conn_task_neg,
                          similarity_method = similarity_method_task)
    }, error = function(e) {
        warning(sprintf("Error computing W_task_raw for subject %d: %s.", subject_idx, e$message)); NULL
    })
    
    if (is.null(W_task_i_raw)) return(result)
    
    W_task_i <- W_task_i_raw # Start with raw
    
    # Redundancy Check
    if (check_redundancy) {
        rho_i <- tryCatch({
            compute_graph_correlation(W_conn_i, W_task_i_raw)
        }, error = function(e) {
            warning(sprintf("Error computing graph correlation for subject %d: %s", subject_idx, e$message)); NA
        })
        
        result$qc_metrics$rho_redundancy <- rho_i
        
        if (!is.na(rho_i) && rho_i >= redundancy_threshold) {
            if (verbose) message(sprintf("  - Subject %d: W_task/W_conn redundancy rho=%.3f >= %.3f. Residualizing W_task.", subject_idx, rho_i, redundancy_threshold))
            W_task_i <- tryCatch({
                residualize_graph_on_subspace(
                    W_graph_to_residualize = W_task_i_raw,
                    L_graph_for_projection = L_conn_i,
                    k_eigenvectors_to_remove = args$residualize_k_conn_proj,
                    k_nn_resparsify = args$residualize_k_conn_labels
                )
            }, error = function(e) {
                warning(sprintf("Error residualizing W_task for subject %d: %s. Using W_task_raw.", subject_idx, e$message)); W_task_i_raw
            })
            result$qc_metrics$was_residualized <- TRUE
        }
    }
    
    # Compute L_task from the final W_task_i (raw or residualized)
    L_task_i <- tryCatch({
        compute_graph_laplacian_sparse(W_task_i, alpha = alpha_laplacian, degree_type = degree_type_laplacian)
    }, error = function(e) {
        warning(sprintf("Error computing L_task for subject %d: %s.", subject_idx, e$message)); NULL
    })
    
    result$W_task_i <- W_task_i
    result$L_task_i <- L_task_i
    
    return(result)
}

#' Shape basis based on task method
#'
#' @param subject_idx Subject index.
#' @param L_conn_i Laplacian matrix for connectivity
#' @param L_task_i Laplacian matrix for task
#' @param args Validated arguments
#' @param W_conn_i Connectivity matrix for subject i
#' @param W_task_i Task matrix for subject i
#'
#' @return List with basis matrices and eigenvalues
#' @noRd
shape_basis <- function(subject_idx, L_conn_i, L_task_i, args, W_conn_i, W_task_i) {
    result <- list(
        U_original = NULL,
        Lambda_original = NULL,
        U_patch = NULL,
        Lambda_patch = NULL,
        gev_diagnostics = NULL
    )
    
    task_method <- args$task_method
    spectral_rank_k <- args$spectral_rank_k
    
    if (task_method == "core_hatsa") {
        sketch <- tryCatch({
            compute_spectral_sketch_sparse(L_conn_i, spectral_rank_k)
        }, error = function(e) {
            warning(sprintf("Error computing spectral sketch (core) for subject %d: %s.", subject_idx, e$message)); NULL
        })
        if (is.null(sketch)) return(result)
        result$U_original <- sketch$vectors
        result$Lambda_original <- sketch$values
        
    } else if (task_method == "lambda_blend") {
        if (is.null(W_conn_i)) { # W_conn_i is essential
            warning(sprintf("W_conn_i is NULL for subject %d (task_method=%s). Cannot perform blend. Skipping basis for this subject.", subject_idx, task_method))
            return(result) # Return empty result
        }
        if (is.null(W_task_i) && args$lambda_blend_value > 0) { # W_task_i is essential if lambda > 0
            warning(sprintf("W_task_i is NULL for subject %d (task_method=%s) but lambda_blend_value > 0. Cannot perform blend. Using core W_conn for basis.", subject_idx, task_method))
            # Fallback to core L_conn logic if W_task is missing but was expected
            if (is.null(L_conn_i)) { # Should not happen if W_conn_i was present
                 warning(sprintf("L_conn_i also NULL for subject %d. Skipping basis.", subject_idx))
                 return(result)
            }
            sketch <- tryCatch({ 
                compute_spectral_sketch_sparse(L_conn_i, spectral_rank_k) 
            }, error = function(e) {
                warning(sprintf("Error computing spectral sketch (lambda_blend fallback to core L_conn) for subject %d: %s.", subject_idx, e$message)); NULL
            })
        } else {
            # Blend W matrices first
            if (args$lambda_blend_value == 0) { # Effectively core_hatsa
                W_hybrid_i <- W_conn_i
            } else if (args$lambda_blend_value == 1 && !is.null(W_task_i)) { # Effectively task-only graph
                W_hybrid_i <- W_task_i
            } else if (is.null(W_task_i)) { # lambda > 0 but W_task is null, already warned, use W_conn
                 W_hybrid_i <- W_conn_i
            } else { # Actual blend
                W_hybrid_i <- (1 - args$lambda_blend_value) * W_conn_i + args$lambda_blend_value * W_task_i
            }
            
            # Compute Laplacian from the blended W_hybrid_i
            L_hybrid_i <- tryCatch({
                compute_graph_laplacian_sparse(W_hybrid_i, alpha = args$alpha_laplacian, degree_type = args$degree_type_laplacian)
            }, error = function(e) {
                warning(sprintf("Error computing L_hybrid_i from blended W for subject %d: %s. Skipping basis.", subject_idx, e$message)); NULL
            })

            if(is.null(L_hybrid_i)) return(result) # Stop if Laplacian computation failed

            sketch <- tryCatch({
                compute_spectral_sketch_sparse(L_hybrid_i, spectral_rank_k)
            }, error = function(e) {
                warning(sprintf("Error computing spectral sketch (lambda_blend) for subject %d: %s.", subject_idx, e$message)); NULL
            })
        }
        if (is.null(sketch)) return(result) # U_original and Lambda_original will be NULL
        result$U_original <- sketch$vectors
        result$Lambda_original <- sketch$values
        
    } else if (task_method == "gev_patch") {
        # Core sketch
        core_sketch <- tryCatch({
            compute_spectral_sketch_sparse(L_conn_i, spectral_rank_k)
        }, error = function(e) {
            warning(sprintf("Error computing spectral sketch (core for GEV) for subject %d: %s.", subject_idx, e$message)); NULL
        })
        if (is.null(core_sketch)) return(result)
        result$U_original <- core_sketch$vectors
        result$Lambda_original <- core_sketch$values
        
        # GEV Patch
        if (is.null(L_task_i)) {
            warning(sprintf("L_task is NULL for subject %d (task_method=%s). Cannot compute GEV patch.", subject_idx, task_method))
        } else {
            gev_results <- tryCatch({
                solve_gev_laplacian_primme(
                    L_task_i, L_conn_i,
                    k_request = args$k_gev_dims * 2, # Request more, filter later
                    lambda_max_thresh = args$gev_lambda_max,
                    epsilon_reg_B = args$gev_epsilon_reg
                )
            }, error = function(e) {
                warning(sprintf("Error solving GEV for subject %d: %s.", subject_idx, e$message)); NULL
            })
            
            if (!is.null(gev_results)) {
                result$U_patch <- gev_results$vectors
                result$Lambda_patch <- gev_results$values
                result$gev_diagnostics <- compute_gev_spectrum_diagnostics(gev_results$values, args$gev_lambda_max)
            }
        }
    }
    
    return(result)
}

#' Perform anchor augmentation
#'
#' @param args Validated arguments
#' @param processing_results Results from process_subjects
#' @param anchor_indices Indices of anchor parcels
#'
#' @return List with augmentation results
#' @noRd
perform_anchor_augmentation <- function(args, processing_results, anchor_indices) {
    verbose <- args$verbose
    N_subjects <- args$N_subjects
    row_augmentation <- args$row_augmentation
    task_data_list <- args$task_data_list
    m_parcel_rows <- args$m_parcel_rows
    spectral_rank_k <- args$spectral_rank_k
    
    U_original_list <- processing_results$U_original_list
    
    if (verbose) message_stage("Performing Anchor Augmentation (if enabled)...", interactive_only = TRUE)
    
    valid_subject_indices_for_gpa <- which(!sapply(U_original_list, is.null))
    if (length(valid_subject_indices_for_gpa) < 2) {
        stop(sprintf("Need at least 2 subjects with valid computed bases for GPA, found only %d.", length(valid_subject_indices_for_gpa)))
    }
    
    # Initialize storage
    A_originals_list_for_gpa <- vector("list", N_subjects)
    m_task_rows_effective <- 0
    condition_labels_for_anchors <- NULL
    condition_labels_set <- FALSE
    
    for (i in valid_subject_indices_for_gpa) {
        augmentation_result <- augment_anchors_for_subject(
            i, 
            U_original_list[[i]], 
            row_augmentation,
            if (!is.null(task_data_list) && length(task_data_list) >= i) task_data_list[[i]] else NULL,
            anchor_indices,
            args
        )
        
        A_originals_list_for_gpa[[i]] <- augmentation_result$A_augmented
        
        if (m_task_rows_effective == 0 && augmentation_result$C_subj > 0) {
            m_task_rows_effective <- augmentation_result$C_subj
        }
        
        if (!condition_labels_set && !is.null(augmentation_result$condition_labels)) {
            condition_labels_for_anchors <- augmentation_result$condition_labels
            condition_labels_set <- TRUE
        }
    }
    
    # Check consistency of augmented matrices before GPA
    final_m_total_rows <- m_parcel_rows + m_task_rows_effective
    valid_A_indices_for_gpa_final <- sapply(A_originals_list_for_gpa, function(A) {
        !is.null(A) && is.matrix(A) && nrow(A) == final_m_total_rows && ncol(A) == spectral_rank_k
    })
    num_valid_for_gpa <- sum(valid_A_indices_for_gpa_final)
    
    if (num_valid_for_gpa < 2) {
        stop(sprintf("Need at least 2 subjects with valid anchor matrices for GPA (found %d with %d rows). Check basis computation and anchor augmentation steps.", num_valid_for_gpa, final_m_total_rows))
    }
    
    if (verbose) message(sprintf("Proceeding to GPA with %d subjects having valid anchor matrices (%d parcel + %d task rows).", num_valid_for_gpa, m_parcel_rows, m_task_rows_effective))
    
    return(list(
        A_originals_list_for_gpa = A_originals_list_for_gpa,
        valid_A_indices_for_gpa_final = valid_A_indices_for_gpa_final,
        m_task_rows_effective = m_task_rows_effective,
        condition_labels_for_anchors = condition_labels_for_anchors
    ))
}

#' Augment anchors for a single subject
#'
#' @param i Subject index
#' @param U_basis_i Basis matrix for subject i
#' @param row_augmentation Whether to augment with task rows
#' @param task_data_i Task data for subject i
#' @param anchor_indices Indices of anchor parcels
#' @param args Validated arguments
#'
#' @return List with augmented anchor matrix and related information
#' @noRd
augment_anchors_for_subject <- function(i, U_basis_i, row_augmentation, task_data_i, anchor_indices, args) {
    result <- list(
        A_augmented = NULL,
        C_subj = 0,
        condition_labels = NULL
    )
    
    verbose <- args$verbose
    V_p <- args$V_p
    
    # Extract parcel anchors
    if (any(anchor_indices > nrow(U_basis_i))) {
        stop(sprintf("Subject %d: anchor_indices out of bounds for basis matrix.", i))
    }
    
    A_parc_i <- U_basis_i[anchor_indices, , drop = FALSE]
    result$A_augmented <- A_parc_i # Default if no augmentation
    
    if (row_augmentation && !is.null(task_data_i)) {
        # Check if task_data_i is suitable (e.g., matrix C x Vp)
        is_suitable_task_data <- is.matrix(task_data_i) && nrow(task_data_i) > 0 && ncol(task_data_i) == V_p
        
        if (is_suitable_task_data) {
            result$C_subj <- nrow(task_data_i)
            
            # Store condition labels if available
            if (!is.null(rownames(task_data_i))) {
                result$condition_labels <- rownames(task_data_i)
            }
            
            # Project features
            Act_i <- t(task_data_i) # Transpose for projection
            Z_i_projected <- tryCatch({
                project_features_to_spectral_space(feature_matrix = Act_i, U_basis = U_basis_i)
            }, error = function(e) {
                warning(sprintf("Error projecting task features for subject %d: %s.", i, e$message)); NULL
            })
            
            if (!is.null(Z_i_projected)) {
                # Output is k x C, need C x k for rbind
                Z_i <- t(Z_i_projected)
                
                # Optional Residualization
                if (args$residualize_condition_anchors) {
                    if (verbose) message(sprintf("  - Subject %d: Residualizing condition anchors.", i))
                    Z_i <- tryCatch({
                        residualize_matrix_on_subspace(matrix_to_residualize = Z_i, subspace_basis_matrix = A_parc_i)
                    }, error = function(e) {
                        warning(sprintf("Error residualizing task anchors for subject %d: %s.", i, e$message)); Z_i
                    })
                }
                
                # Build augmented matrix
                result$A_augmented <- build_augmented_anchor_matrix(A_parc_i, Z_i)
            } else {
                if (verbose) message(sprintf("  - Subject %d: Task feature projection failed. Using parcel anchors only.", i))
            }
        } else {
            if (verbose) message(sprintf("  - Subject %d: Task data not suitable for row augmentation (expected C x Vp matrix). Using parcel anchors only.", i))
        }
    }
    
    return(result)
}

#' Prepare and run GPA refinement
#'
#' @param args Validated arguments
#' @param augmentation_results Results from perform_anchor_augmentation
#' @param reliability_scores_list List of reliability scores for adaptive weighting
#'
#' @return Results from GPA refinement
#' @noRd
prepare_and_run_gpa <- function(args, augmentation_results, reliability_scores_list) {
    verbose <- args$verbose
    N_subjects <- args$N_subjects
    original_omega_mode <- args$omega_mode # Store the original mode
    n_refine <- args$n_refine
    m_parcel_rows <- args$m_parcel_rows
    spectral_rank_k <- args$spectral_rank_k
    scale_omega_trace <- args$scale_omega_trace
    
    m_task_rows_effective <- augmentation_results$m_task_rows_effective
    A_originals_list_for_gpa <- augmentation_results$A_originals_list_for_gpa
    valid_A_indices_for_gpa_final <- augmentation_results$valid_A_indices_for_gpa_final
    
    if (verbose) message_stage(sprintf("Performing GPA (%d iterations)...", n_refine), interactive_only = TRUE)
    
    # Prepare reliability scores list for GPA
    reliability_scores_for_gpa <- prepare_reliability_scores(
        N_subjects, 
        m_task_rows_effective, 
        reliability_scores_list, 
        valid_A_indices_for_gpa_final, 
        original_omega_mode, # Pass original mode here
        verbose
    )
    
    # Determine effective omega_mode for the solver call
    effective_omega_mode <- original_omega_mode
    if (original_omega_mode == "adaptive" && is.null(reliability_scores_for_gpa)) {
        if (verbose && m_task_rows_effective > 0) { # Only message if adaptive was relevant
            message("  - GPA: omega_mode was 'adaptive' but no valid reliability scores were available. Defaulting to 'fixed' omega_mode for GPA solver.")
        }
        effective_omega_mode <- "fixed"
    }

    # Run GPA refinement
    gpa_results <- tryCatch({
        perform_gpa_refinement(
            A_originals_list = A_originals_list_for_gpa,
            n_refine = n_refine,
            k = spectral_rank_k,
            m_parcel_rows = m_parcel_rows,
            m_task_rows = m_task_rows_effective,
            omega_mode = effective_omega_mode, # Use the potentially adjusted mode
            fixed_omega_weights = args$omega_weights,
            reliability_scores_list = reliability_scores_for_gpa, # This might be NULL
            scale_omega_trace = scale_omega_trace
        )
    }, error = function(e) {
        stop(sprintf("Error during GPA refinement: %s", e$message)); NULL
    })
    
    if (is.null(gpa_results)) stop("GPA refinement failed.")
    
    return(gpa_results)
}

#' Prepare reliability scores for GPA
#'
#' @param N_subjects Number of subjects
#' @param m_task_rows_effective Effective number of task rows
#' @param reliability_scores_list List of reliability scores
#' @param valid_A_indices Valid indices for GPA
#' @param omega_mode Mode for omega calculation
#' @param verbose Whether to print messages
#'
#' @return Prepared reliability scores list
#' @noRd
prepare_reliability_scores <- function(N_subjects, m_task_rows_effective, reliability_scores_list, valid_A_indices, omega_mode, verbose) {
    if (omega_mode == "adaptive" && m_task_rows_effective > 0 && !is.null(reliability_scores_list)) {
        reliability_scores_for_gpa <- vector("list", N_subjects)
        valid_reliability_provided <- FALSE
        
        for (i in 1:N_subjects) {
            if (valid_A_indices[i] && length(reliability_scores_list) >= i && !is.null(reliability_scores_list[[i]])) {
                if (length(reliability_scores_list[[i]]) == m_task_rows_effective) {
                    reliability_scores_for_gpa[[i]] <- reliability_scores_list[[i]]
                    valid_reliability_provided <- TRUE
                } else {
                    warning(sprintf("Subject %d: reliability_scores length (%d) doesn't match effective task rows (%d). Ignoring for this subject.", 
                                   i, length(reliability_scores_list[[i]]), m_task_rows_effective))
                }
            }
        }
        
        if (!valid_reliability_provided) {
            # Warning is now more informational, as prepare_and_run_gpa will handle the mode switch.
            if (verbose) {
                message("  - GPA Setup: omega_mode='adaptive' but no valid reliability_scores_list provided or matched subjects. GPA will use fixed weights.")
            }
            reliability_scores_for_gpa <- NULL
        }
        
        return(reliability_scores_for_gpa)
    }
    
    return(NULL)
}

#' Perform patch alignment if GEV method is used
#'
#' @param args Validated arguments
#' @param processing_results Results from process_subjects
#' @param gpa_results Results from GPA
#'
#' @return Patch alignment results or NULL
#' @noRd
perform_patch_alignment <- function(args, processing_results, gpa_results) {
    task_method <- args$task_method
    verbose <- args$verbose
    N_subjects <- args$N_subjects
    m_parcel_rows <- args$m_parcel_rows
    n_refine <- args$n_refine
    anchor_indices <- args$anchor_indices
    
    U_patch_list <- processing_results$U_patch_list
    
    R_patch_list <- NULL
    
    if (task_method == "gev_patch") {
        if (verbose) message_stage("Performing GEV Patch Alignment...", interactive_only = TRUE)
        
        # Check if any patches were actually computed
        valid_patch_indices <- which(!sapply(U_patch_list, function(p) is.null(p) || ncol(p) == 0))
        
        if (length(valid_patch_indices) >= 2) {
            patch_results <- align_gev_patches(valid_patch_indices, U_patch_list, anchor_indices, m_parcel_rows, n_refine, verbose)
            R_patch_list <- patch_results$R_patch_list
        } else {
            if (verbose) message("Not enough valid GEV patches computed (need >= 2) for patch alignment.")
        }
    }
    
    return(list(R_patch_list = R_patch_list))
}

#' Align GEV patches
#'
#' @param valid_patch_indices Indices of valid patches
#' @param U_patch_list List of patch matrices
#' @param anchor_indices Indices of anchor parcels
#' @param m_parcel_rows Number of parcel rows
#' @param n_refine Number of GPA refinement iterations
#' @param verbose Whether to print messages
#'
#' @return List with patch rotation matrices
#' @noRd
align_gev_patches <- function(valid_patch_indices, U_patch_list, anchor_indices, m_parcel_rows, n_refine, verbose) {
    k_patch_dims <- ncol(U_patch_list[[valid_patch_indices[1]]])
    N_subjects <- length(U_patch_list)
    
    # Create anchor list for patches
    A_patch_originals_list <- vector("list", N_subjects)
    valid_indices_for_patch_gpa <- logical(N_subjects)
    
    for (i in valid_patch_indices) {
        U_patch_i <- U_patch_list[[i]]
        
        if (ncol(U_patch_i) == k_patch_dims) {
            if (any(anchor_indices > nrow(U_patch_i))) {
                warning(sprintf("Cannot extract anchors for GEV patch for subject %d (indices out of bounds). Skipping patch alignment for this subject.", i))
                next
            }
            
            A_patch_originals_list[[i]] <- U_patch_i[anchor_indices, , drop = FALSE]
            valid_indices_for_patch_gpa[i] <- TRUE
        } else {
            warning(sprintf("Subject %d GEV patch dimension (%d) differs from expected (%d). Skipping patch alignment for this subject.", 
                           i, ncol(U_patch_i), k_patch_dims))
        }
    }
    
    if (sum(valid_indices_for_patch_gpa) >= 2) {
        gpa_patch_results <- tryCatch({
            # Use unweighted GPA for patches by default (m_task_rows = 0)
            perform_gpa_refinement(
                A_originals_list = A_patch_originals_list,
                n_refine = n_refine,
                k = k_patch_dims,
                m_parcel_rows = m_parcel_rows,
                m_task_rows = 0 # No task augmentation for patches
            )
        }, error = function(e) {
            warning(sprintf("Error during GEV Patch GPA refinement: %s.", e$message)); NULL
        })
        
        if (!is.null(gpa_patch_results)) {
            return(list(R_patch_list = gpa_patch_results$R_final_list))
        } else {
            warning("GEV Patch GPA failed. Patch rotations not computed.")
            return(list(R_patch_list = NULL))
        }
    } else {
        warning("Fewer than 2 subjects have valid, consistent GEV patches for patch alignment.")
        return(list(R_patch_list = NULL))
    }
}

#' Construct aligned U matrices and final output
#'
#' @param args Validated arguments
#' @param processing_results Results from process_subjects
#' @param augmentation_results Results from perform_anchor_augmentation
#' @param gpa_results Results from GPA
#' @param patch_results Results from patch alignment
#'
#' @return Final task_hatsa output
#' @noRd
construct_output <- function(args, processing_results, augmentation_results, gpa_results, patch_results) {
    verbose <- args$verbose
    N_subjects <- args$N_subjects
    
    U_original_list <- processing_results$U_original_list
    Lambda_original_list <- processing_results$Lambda_original_list
    U_patch_list <- processing_results$U_patch_list
    Lambda_patch_list <- processing_results$Lambda_patch_list
    gev_diagnostics_list <- processing_results$gev_diagnostics_list
    qc_metrics_list <- processing_results$qc_metrics_list
    
    # Extract task-related outputs if available
    W_task_list <- processing_results$W_task_list
    L_task_list <- processing_results$L_task_list
    W_hybrid_list <- processing_results$W_hybrid_list  
    L_hybrid_list <- processing_results$L_hybrid_list
    U_task_list <- processing_results$U_task_list
    Lambda_task_list <- processing_results$Lambda_task_list
    
    R_final_list <- gpa_results$R_final_list
    T_anchor_final <- gpa_results$T_anchor_final
    R_patch_list <- patch_results$R_patch_list
    
    if (verbose) message_stage("Constructing output object...", interactive_only = TRUE)
    
    # Compute aligned U matrices (core basis)
    U_aligned_list <- compute_aligned_U_matrices(N_subjects, U_original_list, R_final_list)
    
    # Build the final result list
    result_list <- list(
        method = "task_hatsa", # This will be set/overridden by the constructor based on parameters
        parameters = create_parameters_list(args),
        # Core alignment results
        R_final_list = R_final_list,
        T_anchor_final = T_anchor_final,
        U_original_list = U_original_list,
        Lambda_original_list = Lambda_original_list,
        U_aligned_list = U_aligned_list,
        
        # Task-specific outputs
        W_task_list = W_task_list,
        L_task_list = L_task_list,
        W_hybrid_list = W_hybrid_list,
        L_hybrid_list = L_hybrid_list,
        U_task_list = U_task_list,
        Lambda_task_list = Lambda_task_list,
        
        # QC and augmentation info
        qc_metrics = qc_metrics_list,
        anchor_augmentation_info = list(
            m_parcel_rows = args$m_parcel_rows,
            m_task_rows_effective = augmentation_results$m_task_rows_effective,
            condition_labels = augmentation_results$condition_labels_for_anchors,
            was_residualized = args$residualize_condition_anchors,
            omega_mode_used = args$omega_mode,
            omega_weights_params = args$omega_weights,
            trace_scaled = args$scale_omega_trace
        ),
        gev_patch_data = if (args$task_method == "gev_patch") list(
            U_patch_list = U_patch_list,
            Lambda_patch_list = Lambda_patch_list,
            R_patch_list = R_patch_list,
            diagnostics = gev_diagnostics_list
        ) else NULL
    )
    
    if (verbose) message_stage("task_hatsa run completed.", interactive_only = TRUE)
    # Call the constructor to create the S3 object
    return(task_hatsa_projector(result_list))
}

#' Compute aligned U matrices
#'
#' @param N_subjects Number of subjects
#' @param U_original_list List of original U matrices
#' @param R_final_list List of rotation matrices
#'
#' @return List of aligned U matrices
#' @noRd
compute_aligned_U_matrices <- function(N_subjects, U_original_list, R_final_list) {
    U_aligned_list <- vector("list", N_subjects)
    
    for (i in 1:N_subjects) {
        if (!is.null(U_original_list[[i]]) && !is.null(R_final_list[[i]])) {
            U_aligned_list[[i]] <- tryCatch({
                U_original_list[[i]] %*% R_final_list[[i]]
            }, error = function(e) {
                warning(sprintf("Error aligning U for subject %d: %s", i, e$message)); NULL
            })
        }
    }
    
    return(U_aligned_list)
}

#' Create parameters list for output
#'
#' @param args Validated arguments
#'
#' @return List of parameters for output
#' @noRd
create_parameters_list <- function(args) {
    return(list(
        k = args$spectral_rank_k,
        spectral_rank_k = args$spectral_rank_k,
        task_method = args$task_method,
        lambda_blend_value = args$lambda_blend_value,
        k_gev_dims = args$k_gev_dims,
        row_augmentation = args$row_augmentation,
        residualize_condition_anchors = args$residualize_condition_anchors,
        omega_mode = args$omega_mode,
        fixed_omega_weights = args$omega_weights,
        scale_omega_trace = args$scale_omega_trace,
        alpha_laplacian = args$alpha_laplacian,
        degree_type_laplacian = args$degree_type_laplacian,
        k_conn_pos = args$k_conn_pos,
        k_conn_neg = args$k_conn_neg,
        k_conn_task_pos = args$k_conn_task_pos,
        k_conn_task_neg = args$k_conn_task_neg,
        similarity_method_task = args$similarity_method_task,
        n_refine = args$n_refine,
        check_redundancy = args$check_redundancy,
        redundancy_threshold = args$redundancy_threshold,
        residualize_k_conn_proj = args$residualize_k_conn_proj,
        residualize_k_conn_labels = args$residualize_k_conn_labels,
        gev_lambda_max = args$gev_lambda_max,
        gev_epsilon_reg = args$gev_epsilon_reg,
        V_p = args$V_p,
        N_subjects = args$N_subjects,
        anchor_indices = args$anchor_indices
    ))
}

#' Helper for status messages
#'
#' @param message_text Text message to display
#' @param verbose Whether to print the message
#' @param interactive_only Whether to print only in interactive mode
#'
#' @noRd
message_stage <- function(message_text, verbose = TRUE, interactive_only = FALSE) {
    if (verbose && (!interactive_only || interactive())) {
        message(rep("-", nchar(message_text)))
        message(message_text)
        message(rep("-", nchar(message_text)))
    }
}
</file>

<file path="R/task_hatsa_main.R">
# R/task_hatsa_main.R

#' Run Task-Informed HATSA (task_hatsa)
#'
#' Performs Hyperalignment via Task-Informed Shared Analysis (task_hatsa) on a list of subject data,
#' incorporating task information during basis shaping and/or alignment refinement.
#'
#' @param subject_data_list List where each element is a numeric matrix (`T_i x V_p`)
#'   of time-series data for one subject.
#' @param anchor_indices Integer vector, indices of the canonical anchor parcels (`1` to `V_p`).
#' @param spectral_rank_k Integer, the desired dimensionality of the primary spectral sketch.
#' @param task_data_list List (parallel to `subject_data_list`). Each element provides
#'   task-related data for a subject. Structure depends on `task_method` and subsequent
#'   processing (e.g., a `C x V_p` matrix of activations/betas for `compute_W_task_from_activations`
#'   or anchor augmentation). Required if `task_method != "core_hatsa"` or `row_augmentation=TRUE`.
#'   Assumed cross-validated if necessary before input.
#' @param task_method Character string: `"core_hatsa"`, `"lambda_blend"`, or `"gev_patch"`. Default: `"lambda_blend"`.
#' @param lambda_blend_value Numeric `lambda` in `[0,1]`. Weight for `L_task` in blend. Default 0.15.
#' @param k_gev_dims Integer, requested dimension for GEV patches. Default 10. Used if `task_method == "gev_patch"`.
#' @param row_augmentation Logical. If `TRUE`, add projected task features to anchor matrices
#'   for GPA refinement. Requires suitable `task_data_list`. Default `TRUE` if suitable data provided.
#' @param residualize_condition_anchors Logical. If `TRUE` and `row_augmentation` is `TRUE`,
#'   residualize projected task anchors against parcel anchors. Default `FALSE`.
#' @param omega_weights List specifying fixed weights for weighted Procrustes (e.g.,
#'   `list(parcel = 1.0, condition = 0.5)`). Used if `row_augmentation=TRUE` and `omega_mode == "fixed"`.
#'   Defaults handled by `solve_procrustes_rotation_weighted`.
#' @param omega_mode Character string: `"fixed"` or `"adaptive"`. Controls weighting in GPA. Default `"fixed"`.
#' @param reliability_scores_list List (parallel to `subject_data_list`), each element a numeric
#'   vector of reliability scores (e.g., R^2) for task data (length `C`). Used if `omega_mode == "adaptive"`.
#' @param scale_omega_trace Logical. Whether to rescale weights in weighted GPA so trace equals total anchors. Default `TRUE`.
#' @param alpha_laplacian Numeric, laziness parameter for graph Laplacians (`L = I - alpha D^{-1} W`). Default 0.93.
#' @param degree_type_laplacian Character string (`"abs"`, `"positive"`, `"signed"`). Type of degree calculation for Laplacian. Default `"abs"`.
#' @param k_conn_pos Integer >= 0. k-NN sparsification for positive edges in `W_conn`.
#' @param k_conn_neg Integer >= 0. k-NN sparsification for negative edges in `W_conn`.
#' @param k_conn_task_pos Integer >= 0. k-NN sparsification for positive edges in `W_task`.
#' @param k_conn_task_neg Integer >= 0. k-NN sparsification for negative edges in `W_task`.
#' @param similarity_method_task Character string or function. Method to compute similarity for `W_task`
#'   (e.g., "pearson", "spearman"). Default "pearson".
#' @param W_task_helper_func Function. The specific function to compute `W_task` (e.g.,
#'   `compute_W_task_from_activations`, `compute_W_task_from_encoding`). If `NULL`, attempts
#'   to infer based on `task_data_list` structure (currently assumes activations `C x Vp`). Default `NULL`.
#' @param n_refine Integer >= 0. Number of GPA refinement iterations.
#' @param check_redundancy Logical. If `TRUE`, check correlation between `W_conn` and `W_task`. Default `TRUE`.
#' @param redundancy_threshold Numeric. Spearman rho threshold for triggering `W_task` residualization. Default 0.45.
#' @param residualize_k_conn_proj Integer. Number of `L_conn` eigenvectors to project `W_task` out of. Default 64.
#' @param residualize_k_conn_labels Integer. k-NN value for re-sparsifying `W_task_res` after residualization. Default 10.
#' @param gev_lambda_max Numeric. Max GEV eigenvalue `lambda` to retain for patches. Default 0.8.
#' @param gev_epsilon_reg Numeric. Small regularization for `L_conn` in GEV. Default 1e-6.
#' @param parcel_names Optional character vector of parcel names. If `NULL`, names like "P1", "P2"... are generated.
#' @param verbose Logical. Print progress messages? Default `TRUE`.
#'
#' @return A list representing the `task_hatsa_projector` object (structure TBD, needs final class definition).
#'         Contains aligned bases, rotations, parameters, QC metrics, etc.
#'
#' @export
#' @importFrom Matrix Matrix Diagonal t crossprod sparseMatrix drop0 forceSymmetric is
#' @importFrom stats cor sd median runif
#' @importFrom utils head
#' @importFrom methods as
run_task_hatsa <- function(
    subject_data_list,
    anchor_indices,
    spectral_rank_k,
    task_data_list = NULL,
    task_method = c("lambda_blend", "gev_patch", "core_hatsa"),
    lambda_blend_value = 0.15,
    k_gev_dims = 10,
    row_augmentation = TRUE, # Default TRUE, but logic checks if task_data allows it
    residualize_condition_anchors = FALSE,
    omega_weights = NULL, # Defaults handled in weighted solver
    omega_mode = c("fixed", "adaptive"),
    reliability_scores_list = NULL,
    scale_omega_trace = TRUE,
    alpha_laplacian = 0.93,
    degree_type_laplacian = c("abs", "positive", "signed"),
    k_conn_pos = 10, # Default based on typical usage
    k_conn_neg = 10, # Default based on typical usage
    k_conn_task_pos = 10, # Default, align with k_conn_pos/neg
    k_conn_task_neg = 10, # Default, align with k_conn_pos/neg
    similarity_method_task = "pearson",
    W_task_helper_func = NULL, # Will default based on task_data_list structure
    n_refine = 5, # Default based on typical usage
    check_redundancy = TRUE,
    redundancy_threshold = 0.45,
    residualize_k_conn_proj = 64,
    residualize_k_conn_labels = 10,
    gev_lambda_max = 0.8,
    gev_epsilon_reg = 1e-6,
    parcel_names = NULL,
    verbose = TRUE
) {
    # Match arguments from vectors of choices
    task_method <- match.arg(task_method)
    omega_mode <- match.arg(omega_mode)
    degree_type_laplacian <- match.arg(degree_type_laplacian)
    
    # --- Step 1: Validate arguments and initialize variables ---
    args <- validate_and_initialize_args(
        subject_data_list = subject_data_list,
        anchor_indices = anchor_indices,
            spectral_rank_k = spectral_rank_k,
        task_data_list = task_data_list,
            task_method = task_method,
            lambda_blend_value = lambda_blend_value,
            k_gev_dims = k_gev_dims,
            row_augmentation = row_augmentation,
            residualize_condition_anchors = residualize_condition_anchors,
        omega_weights = omega_weights,
            omega_mode = omega_mode,
        reliability_scores_list = reliability_scores_list,
            scale_omega_trace = scale_omega_trace,
            alpha_laplacian = alpha_laplacian,
            degree_type_laplacian = degree_type_laplacian,
        k_conn_pos = k_conn_pos,
        k_conn_neg = k_conn_neg,
        k_conn_task_pos = k_conn_task_pos,
        k_conn_task_neg = k_conn_task_neg,
            similarity_method_task = similarity_method_task,
        W_task_helper_func = W_task_helper_func,
            n_refine = n_refine,
            check_redundancy = check_redundancy,
            redundancy_threshold = redundancy_threshold,
            residualize_k_conn_proj = residualize_k_conn_proj,
            residualize_k_conn_labels = residualize_k_conn_labels,
            gev_lambda_max = gev_lambda_max,
            gev_epsilon_reg = gev_epsilon_reg,
        parcel_names = parcel_names,
        verbose = verbose
    )

    # --- Step 2: Process each subject for graph construction and basis shaping ---
    processing_results <- process_subjects(
        args = args,
        subject_data_list = subject_data_list,
        task_data_list = task_data_list,
        anchor_indices = anchor_indices
    )

    # --- Step 3: Anchor Augmentation ---
    augmentation_results <- perform_anchor_augmentation(
        args = args,
        processing_results = processing_results, 
        anchor_indices = anchor_indices
    )

    # --- Step 4: Iterative Refinement (GPA) ---
    gpa_results <- prepare_and_run_gpa(
        args = args,
        augmentation_results = augmentation_results,
        reliability_scores_list = reliability_scores_list
    )

    # --- Step 5: Patch Alignment (If GEV) ---
    patch_results <- perform_patch_alignment(
        args = args,
        processing_results = processing_results,
        gpa_results = gpa_results
    )

    # --- Step 6: Construct Output ---
    result <- construct_output(
        args = args,
        processing_results = processing_results,
        augmentation_results = augmentation_results,
        gpa_results = gpa_results,
        patch_results = patch_results
    )

    return(result)
}

# # Helper for status messages (can be moved to utils)
# message_stage <- function(message_text, verbose = TRUE) {
#     if (verbose) {
#         message(rep("-", nchar(message_text)))
#         message(message_text)
#         message(rep("-", nchar(message_text)))
#     }
# }

#' Advanced Options for Task-Informed HATSA
#'
#' Creates a list of advanced parameters for the task_hatsa function.
#'
#' @param lambda_blend_value Numeric `lambda` in `[0,1]`. Weight for `L_task` in blend. Default 0.15.
#' @param k_gev_dims Integer, requested dimension for GEV patches. Default 10. Used if `task_method == "gev_patch"`.
#' @param row_augmentation Logical. If `TRUE`, add projected task features to anchor matrices
#'   for GPA refinement. Requires suitable `task_data_list`. Default `TRUE` if suitable data provided.
#' @param residualize_condition_anchors Logical. If `TRUE` and `row_augmentation` is `TRUE`,
#'   residualize projected task anchors against parcel anchors. Default `FALSE`.
#' @param omega_weights List specifying fixed weights for weighted Procrustes (e.g.,
#'   `list(parcel = 1.0, condition = 0.5)`). Used if `row_augmentation=TRUE` and `omega_mode == "fixed"`.
#'   Defaults handled by `solve_procrustes_rotation_weighted`.
#' @param omega_mode Character string: `"fixed"` or `"adaptive"`. Controls weighting in GPA. Default `"fixed"`.
#' @param reliability_scores_list List (parallel to `subject_data_list`), each element a numeric
#'   vector of reliability scores (e.g., R^2) for task data (length `C`). Used if `omega_mode == "adaptive"`.
#' @param scale_omega_trace Logical. Whether to rescale weights in weighted GPA so trace equals total anchors. Default `TRUE`.
#' @param alpha_laplacian Numeric, laziness parameter for graph Laplacians (`L = I - alpha D^{-1} W`). Default 0.93.
#' @param degree_type_laplacian Character string (`"abs"`, `"positive"`, `"signed"`). Type of degree calculation for Laplacian. Default `"abs"`.
#' @param k_conn_pos Integer >= 0. k-NN sparsification for positive edges in `W_conn`.
#' @param k_conn_neg Integer >= 0. k-NN sparsification for negative edges in `W_conn`.
#' @param k_conn_task_pos Integer >= 0. k-NN sparsification for positive edges in `W_task`.
#' @param k_conn_task_neg Integer >= 0. k-NN sparsification for negative edges in `W_task`.
#' @param similarity_method_task Character string or function. Method to compute similarity for `W_task`
#'   (e.g., "pearson", "spearman"). Default "pearson".
#' @param W_task_helper_func Function. The specific function to compute `W_task` (e.g.,
#'   `compute_W_task_from_activations`, `compute_W_task_from_encoding`). If `NULL`, attempts
#'   to infer based on `task_data_list` structure (currently assumes activations `C x Vp`). Default `NULL`.
#' @param n_refine Integer >= 0. Number of GPA refinement iterations.
#' @param check_redundancy Logical. If `TRUE`, check correlation between `W_conn` and `W_task`. Default `TRUE`.
#' @param redundancy_threshold Numeric. Spearman rho threshold for triggering `W_task` residualization. Default 0.45.
#' @param residualize_k_conn_proj Integer. Number of `L_conn` eigenvectors to project `W_task` out of. Default 64.
#' @param residualize_k_conn_labels Integer. k-NN value for re-sparsifying `W_task_res` after residualization. Default 10.
#' @param gev_lambda_max Numeric. Max GEV eigenvalue `lambda` to retain for patches. Default 0.8.
#' @param gev_epsilon_reg Numeric. Small regularization for `L_conn` in GEV. Default 1e-6.
#' @param parcel_names Optional character vector of parcel names. If `NULL`, names like "P1", "P2"... are generated.
#'
#' @return A list of options to pass to the task_hatsa function.
#'
#' @export
task_hatsa_opts <- function(
    lambda_blend_value = 0.15,
    k_gev_dims = 10,
    row_augmentation = TRUE,
    residualize_condition_anchors = FALSE,
    omega_weights = NULL,
    omega_mode = c("fixed", "adaptive"),
    reliability_scores_list = NULL,
    scale_omega_trace = TRUE,
    alpha_laplacian = 0.93,
    degree_type_laplacian = c("abs", "positive", "signed"),
    k_conn_pos = 10,
    k_conn_neg = 10,
    k_conn_task_pos = 10,
    k_conn_task_neg = 10,
    similarity_method_task = "pearson",
    W_task_helper_func = NULL,
    n_refine = 5,
    check_redundancy = TRUE,
    redundancy_threshold = 0.45,
    residualize_k_conn_proj = 64,
    residualize_k_conn_labels = 10,
    gev_lambda_max = 0.8,
    gev_epsilon_reg = 1e-6,
    parcel_names = NULL
) {
    omega_mode <- match.arg(omega_mode)
    degree_type_laplacian <- match.arg(degree_type_laplacian)
    
    list(
        lambda_blend_value = lambda_blend_value,
        k_gev_dims = k_gev_dims,
        row_augmentation = row_augmentation,
        residualize_condition_anchors = residualize_condition_anchors,
        omega_weights = omega_weights,
        omega_mode = omega_mode,
        reliability_scores_list = reliability_scores_list,
        scale_omega_trace = scale_omega_trace,
        alpha_laplacian = alpha_laplacian,
        degree_type_laplacian = degree_type_laplacian,
        k_conn_pos = k_conn_pos,
        k_conn_neg = k_conn_neg,
        k_conn_task_pos = k_conn_task_pos,
        k_conn_task_neg = k_conn_task_neg,
        similarity_method_task = similarity_method_task,
        W_task_helper_func = W_task_helper_func,
        n_refine = n_refine,
        check_redundancy = check_redundancy,
        redundancy_threshold = redundancy_threshold,
        residualize_k_conn_proj = residualize_k_conn_proj,
        residualize_k_conn_labels = residualize_k_conn_labels,
        gev_lambda_max = gev_lambda_max,
        gev_epsilon_reg = gev_epsilon_reg,
        parcel_names = parcel_names
    )
}

#' Hyperalignment via Task-Informed Shared Analysis
#' 
#' A user-friendly interface to run Task-Informed HATSA (task_hatsa), which incorporates 
#' task information during basis shaping and/or alignment refinement.
#'
#' @param subject_data_list List where each element is a numeric matrix (`T_i x V_p`)
#'   of time-series data for one subject.
#' @param anchor_indices Integer vector, indices of the canonical anchor parcels (`1` to `V_p`).
#' @param spectral_rank_k Integer, the desired dimensionality of the primary spectral sketch.
#'   Default is 40.
#' @param task_data_list List (parallel to `subject_data_list`). Each element provides
#'   task-related data for a subject. Structure depends on `task_method` and subsequent
#'   processing. Default is NULL.
#' @param task_method Character string: `"lambda_blend"`, `"gev_patch"`, or `"core_hatsa"`. 
#'   Default: `"lambda_blend"`.
#' @param graph_mode Character string: `"schur_complement"`, `"anchor_block"`, or `"full_graph"`.
#'   Determines which graph construction approach to use. Default: `"schur_complement"`.
#' @param opts List of advanced options created by `task_hatsa_opts()`.
#' @param verbose Logical. Print progress messages? Default `TRUE`.
#' @param ... Additional arguments passed to `task_hatsa_opts()` if not provided directly via `opts`.
#'
#' @return A list representing the `task_hatsa_projector` object containing aligned bases, 
#'         rotations, parameters, QC metrics, etc.
#'
#' @export
task_hatsa <- function(
    subject_data_list,
    anchor_indices,
    spectral_rank_k = 40,
    task_data_list = NULL,
    task_method = c("lambda_blend", "gev_patch", "core_hatsa"),
    graph_mode = c("schur_complement", "anchor_block", "full_graph"),
    opts = task_hatsa_opts(),
    verbose = TRUE,
    ...
) {
    task_method <- match.arg(task_method)
    graph_mode <- match.arg(graph_mode)
    
    # Handle additional options passed via ...
    if (...length() > 0) {
        additional_opts <- list(...)
        # Override existing opts with additional options
        for (opt_name in names(additional_opts)) {
            opts[[opt_name]] <- additional_opts[[opt_name]]
        }
    }
    
    # Call the existing implementation with all parameters
    run_task_hatsa(
        subject_data_list = subject_data_list,
        anchor_indices = anchor_indices,
        spectral_rank_k = spectral_rank_k,
        task_data_list = task_data_list,
        task_method = task_method,
        lambda_blend_value = opts$lambda_blend_value,
        k_gev_dims = opts$k_gev_dims,
        row_augmentation = opts$row_augmentation,
        residualize_condition_anchors = opts$residualize_condition_anchors,
        omega_weights = opts$omega_weights,
        omega_mode = opts$omega_mode,
        reliability_scores_list = opts$reliability_scores_list,
        scale_omega_trace = opts$scale_omega_trace,
        alpha_laplacian = opts$alpha_laplacian,
        degree_type_laplacian = opts$degree_type_laplacian,
        k_conn_pos = opts$k_conn_pos,
        k_conn_neg = opts$k_conn_neg,
        k_conn_task_pos = opts$k_conn_task_pos,
        k_conn_task_neg = opts$k_conn_task_neg,
        similarity_method_task = opts$similarity_method_task,
        W_task_helper_func = opts$W_task_helper_func,
        n_refine = opts$n_refine,
        check_redundancy = opts$check_redundancy,
        redundancy_threshold = opts$redundancy_threshold,
        residualize_k_conn_proj = opts$residualize_k_conn_proj,
        residualize_k_conn_labels = opts$residualize_k_conn_labels,
        gev_lambda_max = opts$gev_lambda_max,
        gev_epsilon_reg = opts$gev_epsilon_reg,
        parcel_names = opts$parcel_names,
        verbose = verbose
    )
}

# Now we need to save the original run_task_hatsa implementation
# and redefine it to call task_hatsa, maintaining backward compatibility

# First, rename the original function to .task_hatsa_engine 
.task_hatsa_engine <- run_task_hatsa

# Now redefine run_task_hatsa to call task_hatsa while preserving the original signature
#' @rdname task_hatsa
#' @export
run_task_hatsa <- function(
    subject_data_list,
    anchor_indices,
    spectral_rank_k,
    task_data_list = NULL,
    task_method = c("lambda_blend", "gev_patch", "core_hatsa"),
    lambda_blend_value = 0.15,
    k_gev_dims = 10,
    row_augmentation = TRUE,
    residualize_condition_anchors = FALSE,
    omega_weights = NULL,
    omega_mode = c("fixed", "adaptive"),
    reliability_scores_list = NULL,
    scale_omega_trace = TRUE,
    alpha_laplacian = 0.93,
    degree_type_laplacian = c("abs", "positive", "signed"),
    k_conn_pos = 10,
    k_conn_neg = 10,
    k_conn_task_pos = 10,
    k_conn_task_neg = 10,
    similarity_method_task = "pearson",
    W_task_helper_func = NULL,
    n_refine = 5,
    check_redundancy = TRUE,
    redundancy_threshold = 0.45,
    residualize_k_conn_proj = 64,
    residualize_k_conn_labels = 10,
    gev_lambda_max = 0.8,
    gev_epsilon_reg = 1e-6,
    parcel_names = NULL,
    verbose = TRUE
) {
    # Print a deprecation message but don't break anything
    .Deprecated("task_hatsa", msg = "run_task_hatsa() is kept for backward compatibility; new code should call task_hatsa()")
    
    # Call the original implementation
    .task_hatsa_engine(
        subject_data_list = subject_data_list,
        anchor_indices = anchor_indices,
        spectral_rank_k = spectral_rank_k,
        task_data_list = task_data_list,
        task_method = task_method,
        lambda_blend_value = lambda_blend_value,
        k_gev_dims = k_gev_dims,
        row_augmentation = row_augmentation,
        residualize_condition_anchors = residualize_condition_anchors,
        omega_weights = omega_weights,
        omega_mode = omega_mode,
        reliability_scores_list = reliability_scores_list,
        scale_omega_trace = scale_omega_trace,
        alpha_laplacian = alpha_laplacian,
        degree_type_laplacian = degree_type_laplacian,
        k_conn_pos = k_conn_pos,
        k_conn_neg = k_conn_neg,
        k_conn_task_pos = k_conn_task_pos,
        k_conn_task_neg = k_conn_task_neg,
        similarity_method_task = similarity_method_task,
        W_task_helper_func = W_task_helper_func,
        n_refine = n_refine,
        check_redundancy = check_redundancy,
        redundancy_threshold = redundancy_threshold,
        residualize_k_conn_proj = residualize_k_conn_proj,
        residualize_k_conn_labels = residualize_k_conn_labels,
        gev_lambda_max = gev_lambda_max,
        gev_epsilon_reg = gev_epsilon_reg,
        parcel_names = parcel_names,
        verbose = verbose
    )
}
</file>

<file path="R/task_hatsa_projector.R">
# R/task_hatsa_projector.R

#' Task-HATSA Projector Object
#'
#' An S3 object of class `task_hatsa_projector` that stores the results of a
#' Task-Informed Harmonized Tensors SVD Alignment (task_hatsa) analysis.
#' This object inherits from `hatsa_projector`.
#'
#' @inherit hatsa_projector
#' @field qc_metrics A list per subject containing QC metrics like `rho_redundancy`
#'   (correlation between W_conn and W_task_raw) and `was_residualized` (boolean flag).
#' @field anchor_augmentation_info A list containing information related to anchor augmentation,
#'   such as `m_parcel_rows`, `m_task_rows_effective`, `condition_labels`,
#'   `was_residualized` (for condition anchors), `omega_mode_used`, `omega_weights_params`,
#'   and `trace_scaled`.
#' @field gev_patch_data A list containing GEV-specific outputs if `task_method == "gev_patch"`.
#'   Includes `U_patch_list`, `Lambda_patch_list`, `R_patch_list`, and `diagnostics`.
#'   NULL otherwise.
#'
#' @return A `task_hatsa_projector` object.
#' @seealso `run_task_hatsa`, `hatsa_projector`
#' @name task_hatsa_projector
NULL

#' Constructor for task_hatsa_projector S3 class
#'
#' Creates a `task_hatsa_projector` object. It takes the full results list from
#' the main `task_hatsa` computation (as prepared by `construct_output` helper)
#' and structures it into an S3 object, inheriting from `hatsa_projector`.
#'
#' @param task_hatsa_results A list containing the outputs from the `task_hatsa` run.
#'   This list is expected to have specific named elements corresponding to various
#'   stages and parameters of the analysis (e.g., `parameters`, `R_final_list`,
#'   `U_original_list`, `Lambda_original_list`, `T_anchor_final`, `U_aligned_list`,
#'   `qc_metrics`, `anchor_augmentation_info`, `gev_patch_data`).
#'
#' @return An object of class `c("task_hatsa_projector", "hatsa_projector", "multiblock_biprojector", "projector", "list")`.
#' @export
task_hatsa_projector <- function(task_hatsa_results) {

  # Extract parameters needed for base hatsa_projector components
  params <- task_hatsa_results$parameters
  if (is.null(params)) stop("task_hatsa_results$parameters is missing.")

  k <- params$spectral_rank_k
  N_subjects <- params$N_subjects
  V_p <- params$V_p

  if (is.null(k) || is.null(N_subjects) || is.null(V_p)) {
    stop("Essential parameters (k, N_subjects, V_p) missing from task_hatsa_results$parameters.")
  }

  # ---- Construct components for the base hatsa_projector ----
  # v: Mean aligned sketch (V_p x k)
  if (!is.list(task_hatsa_results$U_aligned_list) || length(task_hatsa_results$U_aligned_list) == 0) {
    # Allow for cases where U_aligned_list might be NULL if N_subjects is too small for GPA, etc.
    # but if N_subjects > 0, it should ideally be a list (even of NULLs for failed subjects)
    if (N_subjects > 0 && is.null(task_hatsa_results$U_aligned_list)) {
        stop("task_hatsa_results$U_aligned_list must be a list of matrices or NULLs.")
    }
    # Handle cases like k=0 or no valid subjects gracefully for v and s construction
    # If U_aligned_list is completely NULL or empty, v and s will be based on that.
    valid_aligned_sketches <- Filter(Negate(is.null), task_hatsa_results$U_aligned_list)
    if (length(valid_aligned_sketches) > 0) {
        v_sum <- Reduce("+", valid_aligned_sketches)
        v <- v_sum / length(valid_aligned_sketches)
    } else {
        v <- matrix(0, nrow = V_p, ncol = k) # Default if no valid sketches
    }
  } else {
    valid_aligned_sketches <- Filter(Negate(is.null), task_hatsa_results$U_aligned_list)
    if (length(valid_aligned_sketches) > 0) {
        v_sum <- Reduce("+", valid_aligned_sketches)
        v <- v_sum / length(valid_aligned_sketches)
    } else {
        v <- matrix(0, nrow = V_p, ncol = k) 
    }
  }
  
  # s: Stacked aligned sketches ((N_subjects * V_p) x k) or appropriate if some subjects failed
  # Ensure that NULLs in U_aligned_list are handled: create NA matrices or skip
  # For simplicity, we'll rbind, which will fail if dimensions don't match or if types are mixed with NULL directly.
  # A more robust approach might be to pre-allocate and fill, or replace NULLs with NA matrices.
  # Let's assume U_aligned_list contains conformable matrices (or NA matrices for failed subjects if that's the convention)
  # For now, use only valid sketches for 's', block_indices will need to reflect this.
  if (length(valid_aligned_sketches) > 0) {
      s <- do.call(rbind, valid_aligned_sketches)
      # block_indices: needs to map to rows of 's'
      # This assumes valid_aligned_sketches are ordered by original subject index.
      # A more robust way would be to track original indices of valid sketches.
      num_rows_per_valid_sketch <- sapply(valid_aligned_sketches, nrow)
      valid_subject_counts <- rep(1, length(valid_aligned_sketches)) # placeholder for now
      
      # Create block_indices based on the actual subjects that contributed to 's'
      # This is tricky if U_aligned_list contains NULLs for some subjects. 
      # The original hatsa_projector assumes all N_subjects contribute V_p rows each.
      # For task_hatsa, some subjects might fail basis computation.
      # The `scores` matrix `s` should ideally represent all Vp*N_subjects rows, with NAs for failed ones.
      # Let's try to conform to that for multivarious compatibility.
      s_full <- matrix(NA, nrow = N_subjects * V_p, ncol = k)
      current_row <- 1
      for (subj_idx in 1:N_subjects) {
          if (!is.null(task_hatsa_results$U_aligned_list[[subj_idx]])) {
              s_full[current_row:(current_row + V_p - 1), ] <- task_hatsa_results$U_aligned_list[[subj_idx]]
          }
          current_row <- current_row + V_p
      }
      s <- s_full
      block_indices <- split(seq_len(N_subjects * V_p), rep(seq_len(N_subjects), each = V_p))

  } else {
      s <- matrix(NA, nrow = N_subjects * V_p, ncol = k) # Default if no valid sketches
      block_indices <- split(seq_len(N_subjects * V_p), rep(seq_len(N_subjects), each = V_p))
      if (N_subjects == 0) block_indices <- list() # Handle N_subjects = 0 case
  }

  # sdev: Component standard deviations (length k) - default to 1s
  sdev <- rep(1, k)

  # preproc: Standard for multivarious projector
  preproc_obj <- multivarious::prep(multivarious::pass())

  # --- Assemble the final object ----
  obj <- list(
    # multivarious components
    v = v,
    s = s,
    sdev = sdev,
    preproc = preproc_obj,
    block_indices = block_indices,
    
    # Core HATSA components (from task_hatsa_results)
    R_final_list = task_hatsa_results$R_final_list,
    U_original_list = task_hatsa_results$U_original_list,
    Lambda_original_list = task_hatsa_results$Lambda_original_list,
    T_anchor_final = task_hatsa_results$T_anchor_final,
    
    # Parameters and method
    parameters = params, # Already contains task-specific details
    method = params$method, # Should be "task_hatsa"
    
    # Task-HATSA specific slots
    qc_metrics = task_hatsa_results$qc_metrics,
    anchor_augmentation_info = task_hatsa_results$anchor_augmentation_info,
    gev_patch_data = task_hatsa_results$gev_patch_data,
    
    # Add U_aligned_list for compatibility with tests
    U_aligned_list = task_hatsa_results$U_aligned_list,
    
    # Add task_method-specific outputs 
    W_task_list = task_hatsa_results$W_task_list,
    L_task_list = task_hatsa_results$L_task_list,
    W_hybrid_list = task_hatsa_results$W_hybrid_list,
    L_hybrid_list = task_hatsa_results$L_hybrid_list,
    U_task_list = task_hatsa_results$U_task_list,
    Lambda_task_list = task_hatsa_results$Lambda_task_list
  )

  # Ensure method is correctly set if not in params (it should be)
  if (is.null(obj$method)) obj$method <- "task_hatsa"
  if (is.null(obj$parameters$method)) obj$parameters$method <- "task_hatsa"

  class(obj) <- c("task_hatsa_projector", "hatsa_projector", "multiblock_biprojector", "projector", "list")

  return(obj)
}

#' Print method for task_hatsa_projector objects
#'
#' @param x A `task_hatsa_projector` object.
#' @param ... Additional arguments passed to print.
#' @export
print.task_hatsa_projector <- function(x, ...) {
  # Call the print method for the parent class first
  NextMethod(generic = "print", object = x) # or print.hatsa_projector(x, ...)
  
  # Add Task-HATSA specific information
  cat("\nTask-HATSA Specifics:\n")
  cat("  Task Method: ", x$parameters$task_method, "\n")
  
  if (x$parameters$task_method == "lambda_blend") {
    cat("  Lambda Blend Value: ", x$parameters$lambda_blend_value, "\n")
  }
  
  if (x$parameters$row_augmentation && !is.null(x$anchor_augmentation_info)) {
    cat("  Anchor Augmentation: Enabled (", x$anchor_augmentation_info$m_task_rows_effective, " task rows added)\n")
    cat("    Omega Mode: ", x$anchor_augmentation_info$omega_mode_used, "\n")
    if (x$anchor_augmentation_info$was_residualized) {
        cat("    Condition Anchors Residualized: TRUE\n")
    }
  } else {
    cat("  Anchor Augmentation: Disabled\n")
  }
  
  if (x$parameters$task_method == "gev_patch" && !is.null(x$gev_patch_data)) {
    # Check if any patches were actually computed and stored successfully
    num_subjects_with_patches <- sum(!sapply(x$gev_patch_data$U_patch_list, is.null))
    if (num_subjects_with_patches > 0) {
        # Try to get k_gev_dims from first valid patch, or from parameters
        k_gev <- if (!is.null(x$gev_patch_data$U_patch_list[[which(!sapply(x$gev_patch_data$U_patch_list, is.null))[1]]])) {
                     ncol(x$gev_patch_data$U_patch_list[[which(!sapply(x$gev_patch_data$U_patch_list, is.null))[1]]])
                 } else {
                     x$parameters$k_gev_dims # Fallback to requested
                 }
        cat("  GEV Patch Data: Present (target k_gev_dims: ", x$parameters$k_gev_dims, ", found ~", k_gev, " dims for ", num_subjects_with_patches, " subjects)\n")
    } else {
        cat("  GEV Patch Data: No valid patches computed/stored.\n")
    }
  }
  
  invisible(x)
}

#' Summary method for task_hatsa_projector objects
#'
#' @param object A `task_hatsa_projector` object.
#' @param ... Additional arguments (unused).
#' @return A list object of class `summary.task_hatsa_projector` containing summary statistics.
#' @export
summary.task_hatsa_projector <- function(object, ...) {
  # Call summary method for the parent class to get base summary_info
  # This requires hatsa_projector to have a summary method.
  # If hatsa_projector.R has summary.hatsa_projector, NextMethod() should work.
  # Let's assume summary.hatsa_projector exists and populates basic fields.
  summary_info <- NextMethod(generic = "summary", object = object)
  
  # Add Task-HATSA specific summary information
  summary_info$task_method_used <- object$parameters$task_method
  if (object$parameters$task_method == "lambda_blend") {
    summary_info$lambda_blend_value <- object$parameters$lambda_blend_value
  }

  # QC Metrics Summary
  if (!is.null(object$qc_metrics) && length(object$qc_metrics) > 0) {
    valid_qc_metrics <- Filter(Negate(is.null), object$qc_metrics)
    if (length(valid_qc_metrics) > 0) {
        all_rhos <- sapply(valid_qc_metrics, function(qc) if(!is.null(qc$rho_redundancy) && is.finite(qc$rho_redundancy)) qc$rho_redundancy else NA)
        num_residualized <- sum(sapply(valid_qc_metrics, function(qc) qc$was_residualized), na.rm = TRUE)
        summary_info$qc_avg_rho_redundancy <- mean(all_rhos, na.rm = TRUE)
        summary_info$qc_median_rho_redundancy <- median(all_rhos, na.rm = TRUE)
        summary_info$qc_num_subjects_w_task_residualized <- num_residualized
        summary_info$qc_prop_subjects_w_task_residualized <- num_residualized / length(valid_qc_metrics)
    }
  }
  
  # Anchor Augmentation Summary
  if (!is.null(object$anchor_augmentation_info)) {
    summary_info$anchor_aug_enabled <- object$parameters$row_augmentation
    summary_info$anchor_aug_task_rows_effective <- object$anchor_augmentation_info$m_task_rows_effective
    summary_info$anchor_aug_omega_mode <- object$anchor_augmentation_info$omega_mode_used
    summary_info$anchor_aug_cond_residualized <- object$anchor_augmentation_info$was_residualized
  }
  
  # GEV Patch Summary
  if (object$parameters$task_method == "gev_patch" && !is.null(object$gev_patch_data)) {
    num_subjects_with_patches <- sum(!sapply(object$gev_patch_data$U_patch_list, is.null))
    summary_info$gev_patches_computed_for_subjects <- num_subjects_with_patches
    if (num_subjects_with_patches > 0) {
        actual_gev_dims <- sapply(Filter(Negate(is.null), object$gev_patch_data$U_patch_list), ncol)
        summary_info$gev_dims_found_summary <- summary(actual_gev_dims)
        # Could also summarize from gev_patch_data$diagnostics if that contains per-subject aggregated info
    }
  }
  
  class(summary_info) <- c("summary.task_hatsa_projector", "summary.hatsa_projector", class(summary_info))
  # Ensure correct class order, summary.hatsa_projector might already be there from NextMethod()
  # A safer way if NextMethod() already adds its class: 
  # current_class <- class(summary_info)
  # class(summary_info) <- unique(c("summary.task_hatsa_projector", current_class))
  return(summary_info)
}

#' Print method for summary.task_hatsa_projector objects
#'
#' @param x A `summary.task_hatsa_projector` object.
#' @param ... Additional arguments (unused).
#' @export
print.summary.task_hatsa_projector <- function(x, ...) {
  # Call print method for parent summary class
  NextMethod(generic = "print", object = x) 
  
  cat("\n--- Task-HATSA Specific Summary ---\n")
  cat("Task Method Used: ", x$task_method_used, "\n")
  if (x$task_method_used == "lambda_blend" && !is.null(x$lambda_blend_value)) {
    cat("  Lambda Blend Value: ", sprintf("%.2f", x$lambda_blend_value), "\n")
  }
  
  if (!is.null(x$qc_avg_rho_redundancy)) {
      cat("Redundancy (W_conn vs W_task_raw):\n")
      cat("  Avg. Spearman Rho: ", sprintf("%.3f", x$qc_avg_rho_redundancy), "\n")
      cat("  Median Spearman Rho: ", sprintf("%.3f", x$qc_median_rho_redundancy), "\n")
      cat("  W_task Residualized for: ", x$qc_num_subjects_w_task_residualized, " (", 
          sprintf("%.1f%%", x$qc_prop_subjects_w_task_residualized * 100), ") subjects\n")
  }
  
  if (!is.null(x$anchor_aug_enabled)) {
    cat("Anchor Augmentation: ", ifelse(x$anchor_aug_enabled, "Enabled", "Disabled"), "\n")
    if (x$anchor_aug_enabled) {
      cat("  Effective Task Rows Added: ", x$anchor_aug_task_rows_effective, "\n")
      cat("  Omega Mode: ", x$anchor_aug_omega_mode, "\n")
      cat("  Condition Anchors Residualized: ", x$anchor_aug_cond_residualized, "\n")
    }
  }
  
  if (x$task_method_used == "gev_patch" && !is.null(x$gev_patches_computed_for_subjects)) {
    cat("GEV Patch Data:\n")
    cat("  Patches computed for: ", x$gev_patches_computed_for_subjects, " subjects\n")
    if (x$gev_patches_computed_for_subjects > 0 && !is.null(x$gev_dims_found_summary)) {
        cat("  Dimensions found per GEV patch (summary):\n")
        print(x$gev_dims_found_summary)
    }
  }
  invisible(x)
}
</file>

<file path="R/task_hatsa.R">
#' Compute Task-Based Parcel Similarity Graph (W_task) from Activations
#'
#' Calculates a sparse, z-scored similarity graph between parcels based on their
#' activation profiles across different conditions or task features.
#'
#' @param activation_matrix A numeric matrix (`C x V_p`) where `C` is the number
#'   of conditions/features and `V_p` is the number of parcels. Each column
#'   represents the activation profile for a parcel.
#' @param parcel_names A character vector of length `V_p` specifying parcel names.
#' @param k_conn_task_pos Integer, number of strongest positive connections to
#'   retain per parcel during sparsification.
#' @param k_conn_task_neg Integer, number of strongest negative connections to
#'   retain per parcel during sparsification.
#' @param similarity_method Character string or function. Specifies the method to
#'   compute the initial `V_p x V_p` similarity matrix from `activation_matrix`.
#'   If "pearson" (default) or "spearman", `stats::cor` is used.
#'   If a function, it must take `activation_matrix` as input and return a
#'   `V_p x V_p` numeric matrix.
#' @param use_dtw Logical, defaults to `FALSE`. (Placeholder, currently unused but
#'   kept for potential future compatibility or signature consistency).
#'
#' @return A sparse, symmetric `Matrix::dgCMatrix` of size `V_p x V_p`
#'   representing the z-scored task-based similarity graph `W_task_i`.
#'
#' @importFrom Matrix Matrix sparseMatrix drop0 t forceSymmetric
#' @importFrom stats cor sd
#' @keywords internal
#compute_W_task_from_activations <- function(activation_matrix,
#                                            parcel_names,
#                                            k_conn_task_pos,
#                                            k_conn_task_neg,
#                                            similarity_method = "pearson",
#                                            use_dtw = FALSE) {
#
#  V_p <- ncol(activation_matrix)
#  if (V_p == 0) {
#    return(Matrix::Matrix(0, 0, 0, sparse = TRUE, dimnames = list(character(0), character(0))))
#  }
#  if (length(parcel_names) != V_p) {
#    stop("Length of 'parcel_names' must match the number of columns (parcels) in 'activation_matrix'.")
#  }
#
#  # 1. Compute V_p x V_p dense similarity matrix
#  sim_matrix_dense <- NULL
#  if (is.character(similarity_method) && similarity_method %in% c("pearson", "spearman")) {
#    if (nrow(activation_matrix) > 1) {
#        col_sds <- apply(activation_matrix, 2, stats::sd, na.rm = TRUE)
#        if (any(col_sds == 0, na.rm = TRUE) && interactive()) {
#            message(sprintf("compute_W_task_from_activations: Found %d parcel(s) with zero variance in activation profiles. Similarities involving these will be affected (likely 0 or NA).", sum(col_sds==0, na.rm=TRUE)))
#        }
#    }
#    sim_matrix_dense <- stats::cor(activation_matrix, method = similarity_method, use = "pairwise.complete.obs")
#  } else if (is.function(similarity_method)) {
#    sim_matrix_dense <- tryCatch({
#      similarity_method(activation_matrix)
#    }, error = function(e) {
#      stop(paste("The provided similarity_method function failed:", e$message))
#    })
#    if (!is.matrix(sim_matrix_dense) || nrow(sim_matrix_dense) != V_p || ncol(sim_matrix_dense) != V_p) {
#      stop("The custom similarity_method function must return a V_p x V_p matrix.")
#    }
#  } else {
#    stop("'similarity_method' must be 'pearson', 'spearman', or a function.")
#  }
#
#  sim_matrix_dense[is.na(sim_matrix_dense)] <- 0
#  diag(sim_matrix_dense) <- 0
#
#  # 2. Sparsify
#  row_indices_list <- vector("list", V_p)
#  col_indices_list <- vector("list", V_p)
#  values_list <- vector("list", V_p)
#
#  if (k_conn_task_pos > 0 || k_conn_task_neg > 0) {
#    for (i in 1:V_p) {
#      node_sims <- sim_matrix_dense[i, ]
#      current_selected_indices <- integer(0)
#      current_selected_values <- numeric(0)
#
#      if (k_conn_task_pos > 0) {
#        pos_candidates_idx <- which(node_sims > 1e-9) 
#        if (length(pos_candidates_idx) > 0) {
#          pos_candidates_vals <- node_sims[pos_candidates_idx]
#          num_to_keep_pos <- min(k_conn_task_pos, length(pos_candidates_vals))
#          ordered_in_pos_group <- order(pos_candidates_vals, decreasing = TRUE)
#          top_pos_in_group_idx <- head(ordered_in_pos_group, num_to_keep_pos)
#
#          current_selected_indices <- c(current_selected_indices, pos_candidates_idx[top_pos_in_group_idx])
#          current_selected_values <- c(current_selected_values, pos_candidates_vals[top_pos_in_group_idx])
#        }
#      }
#
#      if (k_conn_task_neg > 0) {
#        neg_candidates_idx <- which(node_sims < -1e-9) 
#        if (length(neg_candidates_idx) > 0) {
#          neg_candidates_vals <- node_sims[neg_candidates_idx]
#          num_to_keep_neg <- min(k_conn_task_neg, length(neg_candidates_vals))
#          ordered_in_neg_group <- order(neg_candidates_vals, decreasing = FALSE)
#          top_neg_in_group_idx <- head(ordered_in_neg_group, num_to_keep_neg)
#
#          current_selected_indices <- c(current_selected_indices, neg_candidates_idx[top_neg_in_group_idx])
#          current_selected_values <- c(current_selected_values, neg_candidates_vals[top_neg_in_group_idx])
#        }
#      }
#
#      if(length(current_selected_indices) > 0) {
#          row_indices_list[[i]] <- rep(i, length(current_selected_indices))
#          col_indices_list[[i]] <- current_selected_indices
#          values_list[[i]] <- current_selected_values
#      }
#    }
#  }
#
#  final_row_indices <- unlist(row_indices_list[!sapply(row_indices_list, is.null)])
#  final_col_indices <- unlist(col_indices_list[!sapply(col_indices_list, is.null)])
#  final_values <- unlist(values_list[!sapply(values_list, is.null)])
#
#  W_dir_task <- if (length(final_row_indices) > 0) {
#    Matrix::sparseMatrix(
#      i = final_row_indices, j = final_col_indices, x = final_values,
#      dims = c(V_p, V_p), dimnames = list(parcel_names, parcel_names)
#    )
#  } else {
#    Matrix::Matrix(0, nrow=V_p, ncol=V_p, sparse=TRUE, dimnames = list(parcel_names, parcel_names))
#  }
#  W_dir_task <- Matrix::drop0(W_dir_task)
#
#  # 3. Symmetrize
#  W_dir_task_t <- Matrix::t(W_dir_task)
#  W_sum_task <- W_dir_task + W_dir_task_t
#  
#  W_den_task_val <- as.numeric((W_dir_task != 0) + (W_dir_task_t != 0))
#  W_den_task <- Matrix::Matrix(pmax(1, W_den_task_val), nrow=V_p, ncol=V_p, sparse=TRUE)
#
#  W_symmetric_raw_task <- W_sum_task / W_den_task
#  if (inherits(W_symmetric_raw_task, "sparseMatrix")) {
#    if (any(is.nan(W_symmetric_raw_task@x))) W_symmetric_raw_task@x[is.nan(W_symmetric_raw_task@x)] <- 0
#    if (any(is.infinite(W_symmetric_raw_task@x))) W_symmetric_raw_task@x[is.infinite(W_symmetric_raw_task@x)] <- 0
#  } else {
#    W_symmetric_raw_task[is.nan(W_symmetric_raw_task)] <- 0
#    W_symmetric_raw_task[is.infinite(W_symmetric_raw_task)] <- 0
#  }
#  W_symmetric_raw_task <- Matrix::drop0(W_symmetric_raw_task)
#  W_symmetric_task <- Matrix::forceSymmetric(W_symmetric_raw_task, uplo = "U")
#
#  # 4. Z-score non-zero edge weights
#  if (length(W_symmetric_task@x) > 0) { 
#    non_zero_vals <- W_symmetric_task@x
#    mean_val <- mean(non_zero_vals)
#    sd_val <- stats::sd(non_zero_vals)
#    if (is.na(sd_val) || sd_val == 0) { 
#      W_symmetric_task@x <- rep(0, length(non_zero_vals))
#    } else {
#      W_symmetric_task@x <- (non_zero_vals - mean_val) / sd_val
#    }
#    W_task_i <- Matrix::drop0(W_symmetric_task)
#  } else {
#    W_task_i <- W_symmetric_task 
#  }
#  
#  return(as(W_task_i, "dgCMatrix"))
#}

#' Compute Task-Based Parcel Similarity Graph (W_task) from Encoding Weights
#'
#' Calculates a sparse, z-scored similarity graph between parcels based on their
#' activation profiles across different conditions or task features.
#'
#' @param activation_matrix A numeric matrix (`C x V_p`) where `C` is the number
#'   of conditions/features and `V_p` is the number of parcels. Each column
#'   represents the activation profile for a parcel.
#' @param parcel_names A character vector of length `V_p` specifying parcel names.
#' @param k_conn_task_pos Integer, number of strongest positive connections to
#'   retain per parcel during sparsification.
#' @param k_conn_task_neg Integer, number of strongest negative connections to
#'   retain per parcel during sparsification.
#' @param similarity_method Character string or function. Specifies the method to
#'   compute the initial `V_p x V_p` similarity matrix from `activation_matrix`.
#'   If "pearson" (default) or "spearman", `stats::cor` is used.
#'   If a function, it must take `activation_matrix` as input and return a
#'   `V_p x V_p` numeric matrix.
#' @param use_dtw Logical, defaults to `FALSE`. (Placeholder, currently unused but
#'   kept for potential future compatibility or signature consistency).
#'
#' @return A sparse, symmetric `Matrix::dgCMatrix` of size `V_p x V_p`
#'   representing the z-scored task-based similarity graph `W_task_i`.
#'
#' @importFrom Matrix Matrix sparseMatrix drop0 t forceSymmetric
#' @importFrom stats cor sd
#' @keywords internal
#compute_W_task_from_activations <- function(activation_matrix,
#                                            parcel_names,
#                                            k_conn_task_pos,
#                                            k_conn_task_neg,
#                                            similarity_method = "pearson",
#                                            use_dtw = FALSE) {
#
#  V_p <- ncol(activation_matrix)
#  if (V_p == 0) {
#    return(Matrix::Matrix(0, 0, 0, sparse = TRUE, dimnames = list(character(0), character(0))))
#  }
#  if (length(parcel_names) != V_p) {
#    stop("Length of 'parcel_names' must match the number of columns (parcels) in 'activation_matrix'.")
#  }
#
#  # 1. Compute V_p x V_p dense similarity matrix
#  sim_matrix_dense <- NULL
#  if (is.character(similarity_method) && similarity_method %in% c("pearson", "spearman")) {
#    if (nrow(activation_matrix) > 1) {
#        col_sds <- apply(activation_matrix, 2, stats::sd, na.rm = TRUE)
#        if (any(col_sds == 0, na.rm = TRUE) && interactive()) {
#            message(sprintf("compute_W_task_from_activations: Found %d parcel(s) with zero variance in activation profiles. Similarities involving these will be affected (likely 0 or NA).", sum(col_sds==0, na.rm=TRUE)))
#        }
#    }
#    sim_matrix_dense <- stats::cor(activation_matrix, method = similarity_method, use = "pairwise.complete.obs")
#  } else if (is.function(similarity_method)) {
#    sim_matrix_dense <- tryCatch({
#      similarity_method(activation_matrix)
#    }, error = function(e) {
#      stop(paste("The provided similarity_method function failed:", e$message))
#    })
#    if (!is.matrix(sim_matrix_dense) || nrow(sim_matrix_dense) != V_p || ncol(sim_matrix_dense) != V_p) {
#      stop("The custom similarity_method function must return a V_p x V_p matrix.")
#    }
#  } else {
#    stop("'similarity_method' must be 'pearson', 'spearman', or a function.")
#  }
#
#  sim_matrix_dense[is.na(sim_matrix_dense)] <- 0
#  diag(sim_matrix_dense) <- 0
#
#  # 2. Sparsify
#  row_indices_list <- vector("list", V_p)
#  col_indices_list <- vector("list", V_p)
#  values_list <- vector("list", V_p)
#
#  if (k_conn_task_pos > 0 || k_conn_task_neg > 0) {
#    for (i in 1:V_p) {
#      node_sims <- sim_matrix_dense[i, ]
#      current_selected_indices <- integer(0)
#      current_selected_values <- numeric(0)
#
#      if (k_conn_task_pos > 0) {
#        pos_candidates_idx <- which(node_sims > 1e-9) 
#        if (length(pos_candidates_idx) > 0) {
#          pos_candidates_vals <- node_sims[pos_candidates_idx]
#          num_to_keep_pos <- min(k_conn_task_pos, length(pos_candidates_vals))
#          ordered_in_pos_group <- order(pos_candidates_vals, decreasing = TRUE)
#          top_pos_in_group_idx <- head(ordered_in_pos_group, num_to_keep_pos)
#
#          current_selected_indices <- c(current_selected_indices, pos_candidates_idx[top_pos_in_group_idx])
#          current_selected_values <- c(current_selected_values, pos_candidates_vals[top_pos_in_group_idx])
#        }
#      }
#
#      if (k_conn_task_neg > 0) {
#        neg_candidates_idx <- which(node_sims < -1e-9) 
#        if (length(neg_candidates_idx) > 0) {
#          neg_candidates_vals <- node_sims[neg_candidates_idx]
#          num_to_keep_neg <- min(k_conn_task_neg, length(neg_candidates_vals))
#          ordered_in_neg_group <- order(neg_candidates_vals, decreasing = FALSE)
#          top_neg_in_group_idx <- head(ordered_in_neg_group, num_to_keep_neg)
#
#          current_selected_indices <- c(current_selected_indices, neg_candidates_idx[top_neg_in_group_idx])
#          current_selected_values <- c(current_selected_values, neg_candidates_vals[top_neg_in_group_idx])
#        }
#      }
#
#      if(length(current_selected_indices) > 0) {
#          row_indices_list[[i]] <- rep(i, length(current_selected_indices))
#          col_indices_list[[i]] <- current_selected_indices
#          values_list[[i]] <- current_selected_values
#      }
#    }
#  }
#
#  final_row_indices <- unlist(row_indices_list[!sapply(row_indices_list, is.null)])
#  final_col_indices <- unlist(col_indices_list[!sapply(col_indices_list, is.null)])
#  final_values <- unlist(values_list[!sapply(values_list, is.null)])
#
#  W_dir_task <- if (length(final_row_indices) > 0) {
#    Matrix::sparseMatrix(
#      i = final_row_indices, j = final_col_indices, x = final_values,
#      dims = c(V_p, V_p), dimnames = list(parcel_names, parcel_names)
#    )
#  } else {
#    Matrix::Matrix(0, nrow=V_p, ncol=V_p, sparse=TRUE, dimnames = list(parcel_names, parcel_names))
#  }
#  W_dir_task <- Matrix::drop0(W_dir_task)
#
#  # 3. Symmetrize
#  W_dir_task_t <- Matrix::t(W_dir_task)
#  W_sum_task <- W_dir_task + W_dir_task_t
#  
#  W_den_task_val <- as.numeric((W_dir_task != 0) + (W_dir_task_t != 0))
#  W_den_task <- Matrix::Matrix(pmax(1, W_den_task_val), nrow=V_p, ncol=V_p, sparse=TRUE)
#
#  W_symmetric_raw_task <- W_sum_task / W_den_task
#  if (inherits(W_symmetric_raw_task, "sparseMatrix")) {
#    if (any(is.nan(W_symmetric_raw_task@x))) W_symmetric_raw_task@x[is.nan(W_symmetric_raw_task@x)] <- 0
#    if (any(is.infinite(W_symmetric_raw_task@x))) W_symmetric_raw_task@x[is.infinite(W_symmetric_raw_task@x)] <- 0
#  } else {
#    W_symmetric_raw_task[is.nan(W_symmetric_raw_task)] <- 0
#    W_symmetric_raw_task[is.infinite(W_symmetric_raw_task)] <- 0
#  }
#  W_symmetric_raw_task <- Matrix::drop0(W_symmetric_raw_task)
#  W_symmetric_task <- Matrix::forceSymmetric(W_symmetric_raw_task, uplo = "U")
#
#  # 4. Z-score non-zero edge weights
#  if (length(W_symmetric_task@x) > 0) { 
#    non_zero_vals <- W_symmetric_task@x
#    mean_val <- mean(non_zero_vals)
#    sd_val <- stats::sd(non_zero_vals)
#    if (is.na(sd_val) || sd_val == 0) { 
#      W_symmetric_task@x <- rep(0, length(non_zero_vals))
#    } else {
#      W_symmetric_task@x <- (non_zero_vals - mean_val) / sd_val
#    }
#    W_task_i <- Matrix::drop0(W_symmetric_task)
#  } else {
#    W_task_i <- W_symmetric_task 
#  }
#  
#  return(as(W_task_i, "dgCMatrix"))
#}
</file>

<file path="R/utils.R">
#' Validate inputs for run_hatsa_core
#'
#' Checks the validity of input parameters for the main HATSA function.
#' Stops with an error if validation fails.
#'
#' @param subject_data_list A list of matrices, where each matrix `X_i` is
#'   `T_i x V_p` (time points by parcels) for subject `i`.
#' @param anchor_indices A numeric vector of 1-based indices for anchor parcels.
#' @param spectral_rank_k An integer, the spectral rank `k`. Must be `>= 0`.
#' @param k_conn_pos An integer, number of positive connections for sparsification.
#' @param k_conn_neg An integer, number of negative connections for sparsification.
#' @param n_refine An integer, number of GPA refinement iterations.
#' @param V_p An integer, the number of parcels (columns in `subject_data_list` elements).
#'
#' @return Invisibly returns a list containing potentially modified `anchor_indices`
#'   (uniquified) if validation passes.
#' @keywords internal
validate_hatsa_inputs <- function(subject_data_list, anchor_indices,
                                  spectral_rank_k, k_conn_pos, k_conn_neg,
                                  n_refine, V_p) {
  # (Code mostly unchanged from v0.2.0, but with k > m as error)
  if (!is.list(subject_data_list) || length(subject_data_list) == 0) {
    stop("`subject_data_list` must be a non-empty list.")
  }
  if (!all(sapply(subject_data_list, function(m) inherits(m, "matrix") && !inherits(m, "Matrix")))) {
    stop("All elements of `subject_data_list` must be standard R matrices (not sparse Matrix objects).")
  }
  if (V_p > 0 && !all(sapply(subject_data_list, ncol) == V_p)) {
    stop("All matrices in `subject_data_list` must have the same number of columns (parcels).")
  }
  if (any(sapply(subject_data_list, function(X) any(!is.finite(X))))) {
    stop("All matrices in `subject_data_list` must contain finite values.")
  }

  if (!is.numeric(anchor_indices) || !is.vector(anchor_indices) || any(is.na(anchor_indices))) {
      stop("`anchor_indices` must be a numeric vector without NAs.")
  }
  if (V_p > 0 && (any(anchor_indices <= 0) || any(anchor_indices > V_p))) {
    stop("`anchor_indices` must be valid 1-based parcel indices not exceeding V_p.")
  }
  
  unique_anchor_indices <- unique(anchor_indices)
  if (length(unique_anchor_indices) != length(anchor_indices)) {
      if (interactive()) message("Duplicate anchor indices provided; using unique set of anchors.")
  }
  if (length(unique_anchor_indices) == 0 && V_p > 0) {
    stop("`anchor_indices` (after taking unique) must not be empty if V_p > 0.")
  }
  m <- length(unique_anchor_indices)

  if (!is.numeric(spectral_rank_k) || length(spectral_rank_k) != 1 || spectral_rank_k < 0 || spectral_rank_k != round(spectral_rank_k)) {
    stop("`spectral_rank_k` must be a non-negative integer.")
  }
  if (V_p > 0 && spectral_rank_k == 0 && interactive()) {
      message("`spectral_rank_k` is 0. Output sketches will have 0 columns. Procrustes alignment will be trivial.")
  }
  if (V_p > 0 && spectral_rank_k > V_p) {
    stop("`spectral_rank_k` cannot exceed the number of parcels `V_p`.")
  }
  if (m > 0 && spectral_rank_k > 0 && spectral_rank_k > m) { # k=0 is fine if m=0 or m>0
    stop(sprintf("`spectral_rank_k` (%d) cannot be greater than the number of unique anchors `m` (%d) for stable Procrustes. Anchor matrix would be rank-deficient.", spectral_rank_k, m))
  }

  if (!is.numeric(k_conn_pos) || length(k_conn_pos) != 1 || k_conn_pos < 0 || k_conn_pos != round(k_conn_pos)) {
    stop("`k_conn_pos` must be a non-negative integer.")
  }
  if (!is.numeric(k_conn_neg) || length(k_conn_neg) != 1 || k_conn_neg < 0 || k_conn_neg != round(k_conn_neg)) {
    stop("`k_conn_neg` must be a non-negative integer.")
  }
  if (V_p > 1 && (k_conn_pos + k_conn_neg == 0) && interactive()) {
      message("`k_conn_pos` and `k_conn_neg` are both zero. Connectivity graphs will be empty.")
  }
   if (V_p > 1 && (k_conn_pos + k_conn_neg >= (V_p -1) ) ) { 
    if (interactive()) message(sprintf("Warning: `k_conn_pos` + `k_conn_neg` (%d) is high relative to V_p-1 (%d), may lead to dense graphs.",
                    k_conn_pos + k_conn_neg, V_p-1))
  }

  if (!is.numeric(n_refine) || length(n_refine) != 1 || n_refine < 0 || n_refine != round(n_refine)) {
    stop("`n_refine` must be a non-negative integer.")
  }

  return(invisible(list(unique_anchor_indices = unique_anchor_indices)))
}

#' Safe Z-scoring of non-zero values in a (potentially sparse) matrix
#'
#' Z-scores the non-zero elements of a matrix. If the standard deviation of these
#' non-zero values is 0 (e.g., all non-zero values are identical, or only one
#' non-zero value exists), these values are set to 0.
#' IMPORTANT: For symmetric matrices, ensure that only one triangle (and diagonal)
#' is stored in `x@x` (e.g., by `Matrix::forceSymmetric(x, uplo="U")` prior to calling)
#' to prevent breaking symmetry if `x@x` contained duplicates from both triangles.
#'
#' @param x A numeric matrix (can be a base R matrix or a sparse `Matrix` object).
#'   If sparse, it's assumed `x@x` holds the structurally non-zero values.
#' @return A matrix of the same class and dimensions as `x`, with its non-zero
#'   elements z-scored.
#' @importFrom Matrix nnzero
#' @importFrom stats sd
#' @keywords internal
zscore_nonzero_sparse <- function(x) {
  # (Code mostly unchanged from v0.2.0)
  is_sparse <- inherits(x, "Matrix")
  
  if (is_sparse) {
    nnz_x <- Matrix::nnzero(x)
    if (is.na(nnz_x) || nnz_x == 0) return(x)
    vals <- x@x 
    
    # Clean up non-finite values
    non_finite_vals <- !is.finite(vals)
    if (any(non_finite_vals)) {
      vals[non_finite_vals] <- 0
      x@x[non_finite_vals] <- 0
    }
  } else { 
    non_zero_indices <- which(x != 0)
    if (length(non_zero_indices) == 0) return(x)
    vals <- x[non_zero_indices]
    
    # Clean up non-finite values
    non_finite_vals <- !is.finite(vals)
    if (any(non_finite_vals)) {
      vals[non_finite_vals] <- 0
      x[non_zero_indices[non_finite_vals]] <- 0
    }
  }
  
  # Return early if all values are now zero after removing non-finite values
  if (all(vals == 0)) return(x)
  
  if (length(vals) < 2) {
      if (is_sparse) {
        x@x <- rep(0, length(x@x))
      } else {
        x[non_zero_indices] <- 0
      }
      return(x)
  }

  mean_val <- mean(vals, na.rm = TRUE)
  sd_val <- stats::sd(vals, na.rm = TRUE)
  
  if (is.na(sd_val) || sd_val == 0) {
    if (is_sparse) {
      x@x <- rep(0, length(x@x))
    } else {
      x[non_zero_indices] <- 0
    }
  } else {
    if (is_sparse) {
      x@x <- (vals - mean_val) / sd_val
    } else {
      x[non_zero_indices] <- (vals - mean_val) / sd_val
    }
  }
  return(x)
}
</file>

<file path="R/voxel_projection.R">
#' Compute Nyström Voxel Basis (Phi_voxel)
#'
#' Internal helper function to compute the Nyström extension basis for projecting
#' voxel-level data into a parcel-defined spectral space. This function uses
#' a k-nearest neighbors approach and a Gaussian kernel to create an affinity
#' matrix between voxels and parcels, then applies the Nyström formula.
#'
#' @param voxel_coords A numeric matrix (V_v x 3) of voxel coordinates.
#' @param parcel_coords A numeric matrix (V_p x 3) of parcel centroid coordinates.
#' @param U_orig_parcel A numeric matrix (V_p x k) of the subject's original
#'   parcel-level eigenvectors (spectral sketch).
#' @param Lambda_orig_parcel A numeric vector of length k, representing the subject's
#'   original parcel-level eigenvalues. Values should be positive.
#' @param n_nearest_parcels An integer, the number of nearest parcels (k_nn) to
#'   consider for each voxel when constructing the affinity matrix *if* `W_vox_parc`
#'   is not provided. Must be at least 1. Ignored if `W_vox_parc` is provided.
#' @param kernel_sigma A numeric scalar, the bandwidth (sigma) for the Gaussian kernel,
#'   or the string \"auto\". Used only *if* `W_vox_parc` is not provided.
#'   If \"auto\", sigma is estimated as
#'   `median(dist_to_1st_nn_parcel) / sqrt(2)`, where `dist_to_1st_nn_parcel` are
#'   the Euclidean distances from each voxel to its closest parcel centroid.
#'   A fallback value (e.g., 1.0) is used if auto-estimation is not possible.
#'   Defaults to 5.0. Ignored if `W_vox_parc` is provided.
#' @param row_normalize_W A logical. Controls whether the effective affinity matrix
#'   (`W_vox_parc`, either computed internally or provided) is row-normalized before
#'   computing Phi_voxel. The core HATSA algorithm uses an alpha-lazy random-walk
#'   normalized Laplacian (`L_rw_lazy`), and for consistency, the standard Nyström
#'   extension involves row-normalizing the voxel-parcel affinities.
#'   Defaults to `TRUE`.
#' @param eigenvalue_floor A small positive numeric value to floor near-zero eigenvalues
#'   before inversion. Defaults to 1e-8.
#' @param W_vox_parc Optional. A pre-computed sparse matrix (dgCMatrix, V_v x V_p)
#'   representing voxel-to-parcel affinities or weights. If provided, the function
#'   will skip the internal k-NN search and Gaussian kernel calculation and use
#'   this matrix directly. It must have dimensions V_v x V_p. Default is `NULL`,
#'   triggering internal calculation.
#' @param ... Additional arguments (currently unused).
#'
#' @return A dense numeric matrix \code{Phi_voxel} (V_v x k), representing the
#'   Nyström basis for voxels. Note: For large numbers of voxels (V_v) and/or
#'   components (k), this matrix can be memory-intensive (e.g., V_v=400k, k=50
#'   using 8-byte doubles is ~153MB).
#'
#' @importFrom Matrix Matrix Diagonal sparseMatrix t 
#' @keywords internal
#' @examples
#' # V_p <- 100; V_v <- 500; k <- 10; n_nearest <- 5; sigma <- 5
#' # p_coords <- matrix(rnorm(V_p*3), V_p, 3)
#' # v_coords <- matrix(rnorm(V_v*3), V_v, 3)
#' # U_p <- matrix(rnorm(V_p*k), V_p, k)
#' # L_p <- runif(k, 0.1, 1)
#' # if (requireNamespace("RANN", quietly = TRUE) && 
#' #     requireNamespace("Matrix", quietly = TRUE)) {
#' #   Phi_v <- compute_voxel_basis_nystrom(v_coords, p_coords, U_p, L_p, 
#' #                                        n_nearest, sigma)
#' #   # dim(Phi_v) # Should be V_v x k
#' # }
compute_voxel_basis_nystrom <- function(voxel_coords, parcel_coords, 
                                        U_orig_parcel, Lambda_orig_parcel,
                                        n_nearest_parcels = 10, kernel_sigma = 5.0,
                                        row_normalize_W = TRUE,
                                        eigenvalue_floor = 1e-8, 
                                        W_vox_parc = NULL, ...) {

  if (!requireNamespace("RANN", quietly = TRUE)) {
    stop("The 'RANN' package is required for compute_voxel_basis_nystrom. Please install it.")
  }
  if (!requireNamespace("Matrix", quietly = TRUE)) {
    stop("The 'Matrix' package is required. Please install it.") # Should be a dependency anyway
  }

  V_v <- nrow(voxel_coords)
  V_p <- nrow(parcel_coords)
  k <- ncol(U_orig_parcel)

  if (V_v == 0) return(matrix(0, nrow = 0, ncol = k))
  if (V_p == 0 || k == 0) return(matrix(0, nrow = V_v, ncol = k))
  if (nrow(U_orig_parcel) != V_p) stop("U_orig_parcel row count must match parcel_coords row count.")
  if (length(Lambda_orig_parcel) != k) stop("Length of Lambda_orig_parcel must match column count of U_orig_parcel (k).")
  if (any(Lambda_orig_parcel <= 0) && interactive()) {
      warning("Some Lambda_orig_parcel are <= 0. This might cause issues with inversion. Applying floor.")
  }

  if (is.null(W_vox_parc)) {
    # 1. Find k_nn nearest parcel centroids for each voxel
    if (n_nearest_parcels < 1) stop("`n_nearest_parcels` must be at least 1.")
    nn_results <- RANN::nn2(data = parcel_coords, query = voxel_coords, k = n_nearest_parcels, treetype = "kd")
    
    # nn_results$nn.idx gives V_v x n_nearest_parcels matrix of parcel indices
    # nn_results$nn.dists gives V_v x n_nearest_parcels matrix of SQUARED Euclidean distances

    # Determine effective kernel_sigma (Ticket V-R2)
    kernel_sigma_effective <- kernel_sigma
    if (is.character(kernel_sigma) && kernel_sigma == "auto") {
      if (V_v > 0 && ncol(nn_results$nn.dists) >= 1) {
        # Use distance to the 1st nearest neighbor for auto-tuning sigma
        # nn.dists contains SQUARED distances
        first_nn_sqrt_dists <- sqrt(nn_results$nn.dists[, 1])
        median_first_nn_dist <- median(first_nn_sqrt_dists, na.rm = TRUE)
        
        if (is.finite(median_first_nn_dist) && median_first_nn_dist > 1e-6) { # Avoid zero or tiny sigma
          kernel_sigma_effective <- median_first_nn_dist / sqrt(2)
          if (interactive()) {
            message(sprintf("Auto-tuned kernel_sigma to: %.3f (based on median 1st NN distance / sqrt(2))", kernel_sigma_effective))
          }
        } else {
          kernel_sigma_effective <- 1.0 # Fallback if median dist is zero/NA (e.g. all voxels on parcel centers)
          if (interactive()) {
            message(sprintf("Could not auto-tune kernel_sigma (median 1st NN distance was %.3f). Using fallback: %.3f", median_first_nn_dist, kernel_sigma_effective))
          }
        }
      } else {
        kernel_sigma_effective <- 1.0 # Fallback if no voxels or nn_results are unexpected
        if (interactive()) {
          message(sprintf("Could not auto-tune kernel_sigma (no voxel data or NN issue). Using fallback: %.3f", kernel_sigma_effective))
        }
      }
    } else if (!is.numeric(kernel_sigma) || length(kernel_sigma) != 1 || kernel_sigma <= 0) {
      stop("`kernel_sigma` must be a positive numeric value or the string \"auto\".")
    }

    # 2. Compute similarities using Gaussian kernel
    # W_ij = exp(-d_ij^2 / (2 * sigma_eff^2))
    similarities <- exp(-nn_results$nn.dists / (2 * kernel_sigma_effective^2))

    # affinities_flat: all affinity values, flattened
    affinities_flat <- as.vector(t(similarities))

    # Construct sparse affinity matrix W_vox_parc (V_v x V_p)
    # First, create in triplet form (TsparseMatrix). This allows for duplicate (i,j) entries.
    W_T <- Matrix::sparseMatrix(i = rep(1:V_v, each = n_nearest_parcels),
                              j = as.vector(t(nn_results$nn.idx)),
                              x = affinities_flat,
                              dims = c(V_v, V_p),
                              repr = "T", # Specify triplet representation
                              giveCsparse = FALSE) # Do not convert to Csparse yet

    # Convert to CsparseMatrix (dgCMatrix). This step automatically sums the 'x' values
    # for any duplicate (i,j) pairs present in the triplet form.
    # This handles cases where a voxel's k-NN includes the same parcel index multiple times,
    # or if multiple (voxel_idx, parcel_idx) raw pairs exist before aggregation.
    W_vox_parc <- as(W_T, "dgCMatrix")

  } else {
    message_stage("Using provided W_vox_parc matrix.", interactive_only=TRUE)
    if (!inherits(W_vox_parc, "dgCMatrix")) {
        warning("Provided `W_vox_parc` is not a dgCMatrix. Attempting to coerce.")
        W_vox_parc <- as(W_vox_parc, "dgCMatrix")
    }
    if (nrow(W_vox_parc) != V_v || ncol(W_vox_parc) != V_p) {
      stop(sprintf("Provided `W_vox_parc` has dimensions [%d x %d], expected [%d x %d] based on voxel/parcel counts.",
                   nrow(W_vox_parc), ncol(W_vox_parc), V_v, V_p))
    }
  }

  # 3. Optionally row-normalize W_vox_parc
  if (row_normalize_W) {
    # Row-normalize W_vox_parc so that the sum of each row is 1.
    # This is a common heuristic in Nyström extensions based on random walks.
    row_sums_W <- Matrix::rowSums(W_vox_parc)
    
    # Identify rows that are not all zero.
    non_zero_row_indices <- row_sums_W != 0
    
    inv_row_sums <- numeric(V_v) # Initialize with zeros
    if (any(non_zero_row_indices)) { # Proceed only if there are non-zero rows
        inv_row_sums[non_zero_row_indices] <- 1 / row_sums_W[non_zero_row_indices]
    }
    
    # Create a sparse diagonal matrix for efficient multiplication.
    # For rows that were originally all-zero (e.g., voxels with no affinity to any
    # selected parcels), their `inv_row_sums` entry is 0.
    # Multiplying by D_inv_sparse will therefore keep these all-zero rows as all-zero.
    # This means such voxels will have a zero projection, which is the chosen behavior.
    D_inv_sparse <- Matrix::Diagonal(n = V_v, x = inv_row_sums)
    W_vox_parc <- D_inv_sparse %*% W_vox_parc
  }

  # 4. Prepare inverse of eigenvalues (Lambda_orig_parcel_safe)
  Lambda_orig_parcel_safe <- pmax(Lambda_orig_parcel, eigenvalue_floor)
  Lambda_inv_diag <- Matrix::Diagonal(n = k, x = 1 / Lambda_orig_parcel_safe)

  # 5. Compute Phi_voxel = W_vox_parc %*% U_orig_parcel %*% Lambda_inv_diag
  # (V_v x V_p) %*% (V_p x k) %*% (k x k)  => (V_v x k)
  # Matrix::%*% handles sparse-dense multiplication efficiently
  Phi_voxel <- W_vox_parc %*% U_orig_parcel %*% Lambda_inv_diag
  
  if (!is.matrix(Phi_voxel)) { # e.g. if it became a Matrix object
      Phi_voxel <- as.matrix(Phi_voxel)
  }

  return(Phi_voxel)
}

#' Internal helper to validate coordinate system inputs
#'
#' Checks for gross inconsistencies between voxel and parcel coordinates, 
#' such as vastly different ranges or potential scale issues, by comparing
#' their bounding boxes.
#'
#' @param voxel_coords Matrix of voxel coordinates (V_v x 3).
#' @param parcel_coords Matrix of parcel coordinates (V_p x 3).
#' @param scale_threshold Factor by which coordinate spans can differ before warning.
#' @param absolute_range_threshold Warn if ranges don't overlap and are separated by more than this.
#' @keywords internal
.validate_coordinate_inputs <- function(voxel_coords, parcel_coords, 
                                        scale_threshold = 10, 
                                        absolute_range_threshold = 50) { # e.g. 50mm
  if (is.null(voxel_coords) || is.null(parcel_coords) || 
      !is.matrix(voxel_coords) || !is.matrix(parcel_coords) ||
      ncol(voxel_coords) != 3 || ncol(parcel_coords) != 3 ||
      nrow(voxel_coords) == 0 || nrow(parcel_coords) == 0) {
    # Basic checks already in project_voxels, but good for direct use too
    return(invisible(NULL)) 
  }

  dims <- c("X", "Y", "Z")
  warnings_found <- character(0)

  for (i in 1:3) {
    vox_range <- range(voxel_coords[, i], na.rm = TRUE)
    par_range <- range(parcel_coords[, i], na.rm = TRUE)

    vox_span <- diff(vox_range)
    par_span <- diff(par_range)

    # Check for non-overlapping ranges with significant separation
    # (e.g., max of one is far from min of other)
    ranges_disjoint_by_thresh <- (par_range[1] > vox_range[2] + absolute_range_threshold) || 
                                 (vox_range[1] > par_range[2] + absolute_range_threshold)
    
    if (ranges_disjoint_by_thresh) {
      warnings_found <- c(warnings_found, 
        sprintf("Dimension %s: Voxel range [%.1f, %.1f] and parcel range [%.1f, %.1f] are substantially separated. Possible coordinate system mismatch.", 
                dims[i], vox_range[1], vox_range[2], par_range[1], par_range[2]))
    }
    
    # Check for scale differences (e.g. mm vs cm)
    if (par_span > 0 && vox_span > 0) { # Avoid div by zero if one is a point cloud
      if (vox_span / par_span > scale_threshold || par_span / vox_span > scale_threshold) {
        warnings_found <- c(warnings_found,
          sprintf("Dimension %s: Voxel span (%.1f) and parcel span (%.1f) differ by more than %.0fx. Possible unit or scale mismatch.", 
                  dims[i], vox_span, par_span, scale_threshold))
      }
    }
  }
  
  # Check if parcel bounding box is roughly contained within voxel bounding box (allowing some leeway)
  # This checks if parcel min/max are "too far" outside voxel min/max respectively
  for (i in 1:3) {
      vox_min <- min(voxel_coords[,i], na.rm=TRUE); vox_max <- max(voxel_coords[,i], na.rm=TRUE)
      par_min <- min(parcel_coords[,i], na.rm=TRUE); par_max <- max(parcel_coords[,i], na.rm=TRUE)
      
      # Heuristic: if a parcel coord is outside the voxel range by more than, say, 10% of voxel span
      leeway <- diff(range(voxel_coords[,i], na.rm=TRUE)) * 0.1 
      
      if (par_min < vox_min - leeway || par_max > vox_max + leeway) {
          warnings_found <- c(warnings_found,
              sprintf("Dimension %s: Parcel coordinates range [%.1f, %.1f] extends notably beyond voxel coordinates range [%.1f, %.1f]. Ensure parcels are within the voxel volume.",
                      dims[i], par_min, par_max, vox_min, vox_max))
      }
  }

  if (length(warnings_found) > 0 && interactive()) {
    message("Coordinate system validation found potential issues (run non-interactively to suppress):")
    for(w in warnings_found) message(paste("  -", w))
  }
  return(invisible(NULL))
}

#' Project Voxel-Level Data
#'
#' Generic S3 method for projecting voxel-level data using a fitted model object.
#'
#' @param object A fitted model object (e.g., \code{hatsa_projector}).
#' @param voxel_timeseries_list A list of voxel time-series matrices.
#' @param voxel_coords Coordinates of the voxels.
#' @param parcel_coords Coordinates of the parcels used in the model.
#' @param ... Additional arguments specific to the method.
#'
#' @return A list of projected voxel data, specific to the method implementation.
#' @export
project_voxels <- function(object, voxel_timeseries_list, voxel_coords, parcel_coords, ...) {
  UseMethod("project_voxels")
}

#' Project Voxel-Level Data using a HATSA Projector
#'
#' Projects voxel-level time series data into the common aligned space defined
#' by a \code{hatsa_projector} object. This method uses Nyström extension.
#'
#' It is assumed that \code{voxel_coords} and \code{parcel_coords} are in the
#' same Cartesian coordinate system and units (e.g., millimeters in an MNI-aligned space).
#' The function includes heuristic checks for gross inconsistencies in these coordinate
#' systems (see \code{.\link{.validate_coordinate_inputs}} for details on checks performed),
#' issuing messages if potential issues like substantially different ranges or scales are detected.
#' The Nyström extension is formulated to be consistent with the unnormalized graph
#' Laplacian used in the core HATSA algorithm for parcel-level decomposition. See
#' \code{\link{compute_voxel_basis_nystrom}} for more details on parameters like
#' \code{row_normalize_W} that control the Nyström calculation.
#'
#' @section Methodological Details for Voxel Projection:
#' This section provides further details on the assumptions and computations
#' involved in the Nyström-based voxel projection implemented here.
#'
#' \subsection{Coordinate Systems and Validation:}
#' It is critically assumed that the \code{voxel_coords} (V_v x 3 matrix of voxel
#' coordinates) and \code{parcel_coords} (V_p x 3 matrix of parcel centroid
#' coordinates) are in the **same Cartesian coordinate system and units**.
#' Typically, this would be a standard neuroimaging space like MNI, with
#' coordinates in millimeters. The function \code{\link{.validate_coordinate_inputs}}
#' performs heuristic checks for gross inconsistencies (e.g., vastly different
#' data ranges or scales between voxel and parcel coordinates) and issues
#' messages if potential issues are detected. However, ensuring coordinate
#' system compatibility remains the user\'s responsibility.
#'
#' \subsection{Distance Metric for Voxel-Parcel Affinities:}
#' The affinities between voxels and parcels, used to construct the
#' \code{W_vox_parc} matrix in the Nyström extension (see
#' \code{\link{compute_voxel_basis_nystrom}}), are based on spatial proximity.
#' The k-nearest parcel neighbors for each voxel are found using
#' \code{RANN::nn2}, which, by default, computes **Euclidean distances**
#' in the provided coordinate space. These squared Euclidean distances are
#' then used in the Gaussian kernel.
#'
#' \subsection{Assumptions for Voxel Time-Series Data:}
#' The input \code{voxel_timeseries_list} should contain matrices of
#' pre-processed voxel BOLD fMRI time-series (T_i time points x V_v voxels).
#' While this function does not perform time-series pre-processing itself,
#' the quality and interpretation of the projected voxel coefficients
#' (\code{C_voxel_aligned_i}) will depend on the input data. It is generally
#' assumed that standard fMRI pre-processing steps such as motion correction,
#' slice-timing correction, spatial normalization (to the same space as
#' \code{voxel_coords} and \code{parcel_coords}), detrending, and potentially
#' temporal filtering and nuisance regression have been applied to the
#' voxel time-series prior to their use with this function. The projection
#' \code{C_voxel_coeffs_i = (1/T_i) * t(X_voxel_ts_i) %*% Phi_voxel_i}
#' effectively calculates the covariance of the voxel time series with the
#' Nyström-extended parcel-derived basis functions.
#'
#' \subsection{Kernel Sigma (\code{kernel_sigma}):}
#' The \code{kernel_sigma} parameter in \code{\link{compute_voxel_basis_nystrom}}
#' controls the bandwidth of the Gaussian kernel used to compute affinities
#' between voxels and their nearest parcel neighbors.
#' \itemize{
#'   \item If a numeric value is provided, it is used directly as sigma.
#'   \item If \code{kernel_sigma = "auto"} (the default in \code{project_voxels}),
#'     sigma is estimated as \code{median(dist_to_1st_nn_parcel) / sqrt(2)},
#'     where \code{dist_to_1st_nn_parcel} are the Euclidean distances from each
#'     voxel to its single closest parcel centroid. A fallback value (e.g., 1.0)
#'     is used if auto-estimation is problematic (e.g., all distances are zero).
#' }
#' The choice of sigma can influence the smoothness and reach of the
#' voxel-to-parcel affinities.
#'
#' \subsection{Nyström Formulation and Laplacian Consistency:}
#' The core HATSA algorithm, as implemented in \code{\link{run_hatsa_core}} and
#' its helper \code{compute_spectral_sketch_sparse} (using the updated
#' \code{compute_graph_laplacian_sparse}), derives the original
#' subject-specific parcel components (\code{U_original_list\\\[\\\[i\\\]\\\]} and
#' \code{Lambda_original_list\\\[\\\[i\\\]\\\]}) from an **alpha-lazy random-walk
#' normalized graph Laplacian** (\eqn{L_{rw\_lazy} = (I - \alpha D_p^{-1} W_p + (I - \alpha D_p^{-1} W_p)^T)/2},
#' where \eqn{W_p} is the parcel-parcel affinity matrix, \eqn{D_p} is the
#' corresponding degree matrix (typically based on absolute weights), and \eqn{\alpha}
#' is the laziness parameter, defaulting to 0.93). This symmetric form ensures compatibility
#' with symmetric eigensolvers.
#'
#' For mathematical consistency with this parcel-level eigensystem, the
#' Nyström extension formula used in \code{\link{compute_voxel_basis_nystrom}}
#' effectively involves row-normalizing the voxel-parcel affinity matrix \eqn{W_{vp}}
#' (making it row-stochastic, equivalent to pre-multiplying by \eqn{D_v^{-1}})
#' before applying the projection using the parcel eigenvectors and eigenvalues.
#' This formulation corresponds to setting the \code{row_normalize_W} parameter in
#' \code{\link{compute_voxel_basis_nystrom}} to \code{TRUE}, which is its default.
#' Using \code{row_normalize_W = FALSE} would generally be inconsistent with the
#' default HATSA Laplacian type.
#' (Note: The exact Nyström formula relating \eqn{L_{rw\_lazy}} eigenvectors/values
#' to the row-normalized extension requires careful derivation, but using the
#' eigenvectors of \eqn{L_{rw\_lazy}} with the row-normalized \eqn{W_{vp}} extension
#' is a common and practically effective approach consistent with random-walk principles).
#'
#' \subsection{Handling of DC Component in Parcel-Level Spectral Decomposition:}
#' The parcel-level eigenvectors \code{U_original_list\\\[\\\[i\\\]\\\]} (and their
#' corresponding eigenvalues \code{Lambda_original_list\\\[\\\[i\\\]\\\]}) used as input
#' to the Nyström extension are **non-DC components**.
#' In \code{compute_spectral_sketch_sparse}, eigenvectors of the parcel
#' Laplacian associated with eigenvalues near zero (i.e., numerically zero,
#' including the DC component for a connected graph) are explicitly filtered out
#' using a tolerance (\code{eigenvalue_tol}). The \code{spectral_rank_k}
#' parameter thus specifies the number of desired "informative" or non-DC
#' components from the parcel-level decomposition. This ensures that the
#' Nyström extension \code{Phi_voxel_i} is constructed based on these
#' non-trivial modes of parcel co-variation.
#'
#' @param object A fitted \code{hatsa_projector} object.
#' @param voxel_timeseries_list A list of voxel time-series matrices (T_i x V_v).
#'   It is assumed that the i-th element of this list corresponds to the i-th
#'   subject as stored in the \code{hatsa_projector} object (e.g., for U_original_list).
#' @param voxel_coords A numeric matrix (V_v x 3) of voxel coordinates.
#' @param parcel_coords A numeric matrix (V_p x 3) of parcel centroid coordinates
#'   corresponding to the parcellation used to fit the \code{object}.
#' @param n_nearest_parcels Integer, number of nearest parcels for Nyström extension.
#'   Passed to \code{\link{compute_voxel_basis_nystrom}}.
#' @param kernel_sigma Numeric or "auto", kernel bandwidth for Nyström extension.
#'   Passed to \code{\link{compute_voxel_basis_nystrom}}.
#' @param data_type Character string, either "timeseries" (default) or
#'   "coefficients". If "timeseries", the projection involves scaling by `1/T_i`
#'   (number of time points) to estimate covariance with the basis.
#'   If "coefficients" (e.g., for projecting beta maps or statistical maps),
#'   this scaling is omitted.
#' @param ... Additional arguments passed to \code{\link{compute_voxel_basis_nystrom}}.
#'
#' @return A list of aligned voxel coefficient matrices. Each element `[[i]]` is a
#'   matrix of dimensions T_i x k, where T_i is the number of time points (or rows)
#'   in the input `voxel_timeseries_list[[i]]`, and k is the number of HATSA components.
#'   These represent the projection coefficients of the voxel data onto the aligned
#'   HATSA basis for each subject.
#'
#' @examples
#' # This is a conceptual example. For it to run, you need a fitted hatsa_projector.
#' # First, let's set up parameters for a minimal run_hatsa_core call.
#' set.seed(456)
#' N_subj_fit <- 2
#' V_parc_fit <- 20 # Number of parcels in the fitted model
#' k_comp_fit <- 3   # Number of components in the fitted model
#' T_time_fit <- 40  # Number of time points for parcel data
#'
#' # Generate mock parcel-level data for fitting HATSA
#' subject_parcel_data <- lapply(1:N_subj_fit, function(i) {
#'   matrix(stats::rnorm(T_time_fit * V_parc_fit), ncol = V_parc_fit)
#' })
#' anchor_idx_fit <- sample(1:V_parc_fit, min(V_parc_fit, 5))
#'
#' # Fit a hatsa_projector object (requires Matrix and RSpectra)
#' fitted_hatsa_model <- NULL
#' if (requireNamespace("Matrix", quietly = TRUE) && 
#'     requireNamespace("RSpectra", quietly = TRUE) &&
#'     exists("run_hatsa_core")) {
#'   fitted_hatsa_model <- tryCatch({
#'     run_hatsa_core(
#'       subject_data_list = subject_parcel_data,
#'       anchor_indices    = anchor_idx_fit,
#'       spectral_rank_k = k_comp_fit,
#'       k_conn_pos        = min(5, V_parc_fit -1),
#'       k_conn_neg        = min(2, V_parc_fit -1),
#'       n_refine          = 1
#'     )
#'   }, error = function(e) NULL) # Return NULL on error for example
#' }
#'
#' if (!is.null(fitted_hatsa_model)) {
#'   # Now, prepare data for project_voxels
#'   N_subj_proj <- N_subj_fit # Number of subjects for voxel projection
#'   V_vox_proj  <- 50         # Number of voxels
#'   T_time_vox  <- 35         # Number of time points for voxel data
#'
#'   # Mock voxel time-series data
#'   voxel_ts_list <- lapply(1:N_subj_proj, function(i) {
#'     matrix(stats::rnorm(T_time_vox * V_vox_proj), ncol = V_vox_proj)
#'   })
#'
#'   # Mock coordinates
#'   voxel_coords_map <- matrix(stats::rnorm(V_vox_proj * 3), ncol = 3)
#'   # Parcel coords must match the V_p used when fitting the model
#'   parcel_coords_map <- matrix(stats::rnorm(V_parc_fit * 3), ncol = 3) 
#'
#'   # Project voxel data
#'   projected_vox_coeffs <- project_voxels(
#'     object = fitted_hatsa_model,
#'     voxel_timeseries_list = voxel_ts_list,
#'     voxel_coords = voxel_coords_map,
#'     parcel_coords = parcel_coords_map,
#'     n_nearest_parcels = 5, # Nystrom param
#'     kernel_sigma = "auto"    # Nystrom param
#'   )
#'
#'   # print(str(projected_vox_coeffs, max.level=1))
#'   # if (length(projected_vox_coeffs) > 0) {
#'   #   print(dim(projected_vox_coeffs[[1]])) # Should be V_vox_proj x k_comp_fit
#'   # }
#' } else {
#'   if (interactive()) message("Skipping project_voxels example: fitted_hatsa_model not created.")
#' }
#' @export
#' @importFrom Matrix t
project_voxels.hatsa_projector <- function(object, 
                                           voxel_timeseries_list, 
                                           voxel_coords, 
                                           parcel_coords, 
                                           n_nearest_parcels = 10, 
                                           kernel_sigma = "auto", 
                                           data_type = c("timeseries", "coefficients"),
                                           ...) {
  
  data_type <- match.arg(data_type)
  
  if (!is.list(voxel_timeseries_list)) stop("`voxel_timeseries_list` must be a list.")
  num_new_subjects <- length(voxel_timeseries_list)
  if (num_new_subjects == 0) return(list())
  
  # Basic input validation for coordinates (critical for function operation)
  if (is.null(voxel_coords) || !is.matrix(voxel_coords) || ncol(voxel_coords) != 3) {
      stop("`voxel_coords` must be a V_v x 3 matrix.")
  }
  if (is.null(parcel_coords) || !is.matrix(parcel_coords) || ncol(parcel_coords) != 3 || nrow(parcel_coords) != object$parameters$V_p) {
      stop(sprintf("`parcel_coords` must be a V_p x 3 matrix, where V_p (%d) matches the model.", object$parameters$V_p))
  }

  # Perform heuristic coordinate system consistency checks (Ticket V-R1)
  .validate_coordinate_inputs(voxel_coords, parcel_coords)

  # Validate that the number of new subjects does not exceed N_subjects in the model
  # This simple check assumes the list corresponds to the first num_new_subjects subjects
  # A more robust solution would use named lists or explicit subject matching.
  if (num_new_subjects > object$parameters$N_subjects) {
    stop(sprintf("Number of subjects in voxel_timeseries_list (%d) exceeds N_subjects in model (%d).",
                 num_new_subjects, object$parameters$N_subjects))
  }

  V_v <- nrow(voxel_coords)
  k <- object$parameters$k

  results_list <- vector("list", num_new_subjects)

  for (i in 1:num_new_subjects) {
    X_voxel_ts_i <- voxel_timeseries_list[[i]]
    if (is.null(X_voxel_ts_i) || !is.matrix(X_voxel_ts_i) || ncol(X_voxel_ts_i) != V_v) {
      warning(sprintf("Voxel timeseries for subject %d is invalid or dimensions mismatch V_v (%d). Skipping.", i, V_v))
      results_list[[i]] <- matrix(NA, nrow = V_v, ncol = k)
      next
    }
    T_i <- nrow(X_voxel_ts_i)
    if (T_i == 0) {
        warning(sprintf("Voxel timeseries for subject %d has zero timepoints. Skipping.", i))
        results_list[[i]] <- matrix(NA, nrow = V_v, ncol = k)
        next
    }

    # Retrieve subject-specific components from the fitted object
    U_orig_parcel_i <- object$U_original_list[[i]]
    Lambda_orig_parcel_i <- object$Lambda_original_list[[i]]
    R_i <- object$R_final_list[[i]]

    if (is.null(U_orig_parcel_i) || is.null(Lambda_orig_parcel_i) || is.null(R_i)) {
      warning(sprintf("Missing original components (U, Lambda, or R) for subject %d in the model. Skipping voxel projection.", i))
      results_list[[i]] <- matrix(NA, nrow = T_i, ncol = k) # Note: T_i x k size
      next
    }
    
    # --- Rotation Matrix Validation (Ticket Z-R3) ---
    if (k > 0) { # Checks only meaningful if k > 0
        # Check for proper dimensions first
        if (nrow(R_i) != k || ncol(R_i) != k) {
            warning(sprintf("Rotation matrix R for subject %d has incorrect dimensions [%s], expected [%d x %d]. Skipping validation & projection.",
                            i, paste(dim(R_i), collapse="x"), k, k))
             results_list[[i]] <- matrix(NA, nrow = T_i, ncol = k) 
             next                       
        }
        
        # Check orthonormality (approximate)
        on_check <- crossprod(R_i)
        eye_k <- diag(k)
        if (!isTRUE(all.equal(on_check, eye_k, tolerance = 1e-6))) {
            # Calculate max absolute difference from identity
            max_diff <- max(abs(on_check - eye_k))
            warning(sprintf("Rotation matrix R for subject %d may not be orthonormal (max diff from identity: %.2e). Results may be affected.", i, max_diff))
        }
        
        # Check determinant (approximate, should be +1 for proper rotation)
        det_R <- det(R_i)
        if (!isTRUE(all.equal(det_R, 1.0, tolerance = 1e-6))) {
            warning(sprintf("Rotation matrix R for subject %d has determinant %.3f (expected ~1.0). May include reflection.", i, det_R))
        }
    }
    # --- End Validation ---

    # 1. Compute Nyström voxel basis Phi_voxel_i (V_v x k)
    Phi_voxel_i <- compute_voxel_basis_nystrom(
      voxel_coords = voxel_coords,
      parcel_coords = parcel_coords,
      U_orig_parcel = U_orig_parcel_i,
      Lambda_orig_parcel = Lambda_orig_parcel_i,
      n_nearest_parcels = n_nearest_parcels,
      kernel_sigma = kernel_sigma,
      ...
    )
    
    if (is.null(Phi_voxel_i) || nrow(Phi_voxel_i) != V_v || ncol(Phi_voxel_i) != k) {
        warning(sprintf("Nystrom basis computation failed or returned incorrect dimensions for subject %d. Skipping.", i))
        results_list[[i]] <- matrix(NA, nrow = T_i, ncol = k)
        next
    }

    # 2. Compute subject-specific voxel spectral coefficients: C = X %*% Phi
    C_voxel_coeffs_i <- X_voxel_ts_i %*% Phi_voxel_i
    
    # Apply scaling factor (if specified and applicable)
    scale_factor <- 1.0
    if (data_type == "timeseries") {
        # Apply the originally intended 1/T_i scaling, assuming it's desired for interpretation
        if (T_i > 0) { 
            scale_factor <- 1 / T_i 
        } else {
            scale_factor <- 0 # Avoid division by zero
        }
    }
    C_voxel_coeffs_i_scaled <- C_voxel_coeffs_i * scale_factor
    
    # 3. Rotate to common space: C_aligned = C_scaled %*% R_i
    # (T_i x k) %*% (k x k) -> (T_i x k)
    C_voxel_aligned_i <- C_voxel_coeffs_i_scaled %*% R_i
    
    results_list[[i]] <- as.matrix(C_voxel_aligned_i) # Ensure it's a base matrix (T_i x k)
  }

  return(results_list)
}
</file>

<file path="R/weighted_procrustes.R">
#' @title Solve Weighted Procrustes Rotation Problem
#' @description Finds the optimal rotation matrix `R` that aligns a source matrix `A_source`
#'   to a target matrix `T_target`, minimizing `|| Omega (A_source R - T_target) ||_F^2`,
#'   where `Omega` is a diagonal weighting matrix.
#'
#' @param A_source The numeric source matrix (e.g., augmented anchors for one subject),
#'   dimensions `N x k` (N = total anchors, k = dimensions).
#' @param T_target The numeric target matrix (e.g., augmented template), dimensions `N x k`.
#' @param m_parcel_rows Integer, number of rows in `A_source` (and `T_target`)
#'   corresponding to parcel anchors. Assumed to be the first `m_parcel_rows`.
#' @param m_task_rows Integer, number of rows in `A_source` (and `T_target`)
#'   corresponding to task condition anchors. Assumed to be after parcel rows.
#' @param omega_mode Character string, mode for determining weights:
#'   `"fixed"` (default) or `"adaptive"`.
#' @param fixed_omega_weights List, used if `omega_mode == "fixed"`.
#'   E.g., `list(parcel = 1.0, condition = 0.5)`. If `NULL`, defaults to this.
#' @param reliability_scores Numeric vector of length `m_task_rows`, containing
#'   reliability scores (e.g., R^2_p) for each task condition. Required and used
#'   only if `omega_mode == "adaptive"`.
#' @param scale_omega_trace Logical, whether to rescale the diagonal `Omega` matrix
#'   so that `sum(diag(Omega)) == N`. Default `TRUE`.
#'
#' @return A `k x k` rotation matrix `R`.
#'
#' @importFrom Matrix Diagonal
#' @importFrom stats median
#' @export
#' @examples
#' N_parcels <- 5
#' N_tasks <- 3
#' N_total <- N_parcels + N_tasks
#' k_dims <- 4
#' A <- matrix(rnorm(N_total * k_dims), N_total, k_dims)
#' T_true <- matrix(rnorm(N_total * k_dims), N_total, k_dims)
#' R_true <- svd(matrix(rnorm(k_dims*k_dims),k_dims,k_dims))$u %*% 
#'           diag(sample(c(1,1,1,-1),k_dims,replace=TRUE)) %*% 
#'           svd(matrix(rnorm(k_dims*k_dims),k_dims,k_dims))$v
#' A_rotated_perfect <- T_true %*% t(R_true) # A_rotated_perfect R_true should be T_true
#'
#' # Fixed weights (default)
#' R_fixed <- solve_procrustes_rotation_weighted(A_rotated_perfect, T_true, 
#'                                               N_parcels, N_tasks)
#' # print(all.equal(T_true %*% t(R_fixed), A_rotated_perfect)) # Check alignment
#' print(sum((A_rotated_perfect %*% R_fixed - T_true)^2)) # Should be small
#'
#' # Adaptive weights (example with dummy reliabilities)
#' set.seed(123)
#' rel_scores <- runif(N_tasks, -0.2, 0.8)
#' R_adaptive <- solve_procrustes_rotation_weighted(A_rotated_perfect, T_true, 
#'                                                  N_parcels, N_tasks, 
#'                                                  omega_mode = "adaptive", 
#'                                                  reliability_scores = rel_scores)
#' print(sum((A_rotated_perfect %*% R_adaptive - T_true)^2)) # Should be small
#'
#' # No task rows
#' R_no_task <- solve_procrustes_rotation_weighted(A[1:N_parcels,], T_true[1:N_parcels,],
#'                                                 N_parcels, 0)
#'
solve_procrustes_rotation_weighted <- function(A_source, T_target, 
                                             m_parcel_rows, m_task_rows,
                                             omega_mode = "fixed",
                                             fixed_omega_weights = NULL,
                                             reliability_scores = NULL,
                                             scale_omega_trace = TRUE) {

  # --- Input Validation ---
  if (!is.matrix(A_source) || !is.numeric(A_source) || 
      !is.matrix(T_target) || !is.numeric(T_target)) {
    stop("A_source and T_target must be numeric matrices.")
  }
  if (nrow(A_source) != nrow(T_target) || ncol(A_source) != ncol(T_target)) {
    stop("A_source and T_target must have the same dimensions.")
  }
  N_total_rows <- nrow(A_source)
  k_dims <- ncol(A_source)
  if (N_total_rows == 0) { 
      # If k_dims is also 0, diag(0) is matrix(0,0). If k_dims > 0, return KxK identity.
      return(diag(k_dims)) 
  }
  if (m_parcel_rows < 0 || m_task_rows < 0) {
      stop("m_parcel_rows and m_task_rows must be non-negative.")
  }
  if (m_parcel_rows + m_task_rows != N_total_rows) {
    stop("Sum of m_parcel_rows and m_task_rows must equal total rows in A_source/T_target.")
  }
  omega_mode <- match.arg(omega_mode, c("fixed", "adaptive"))

  if (is.null(fixed_omega_weights)) {
    fixed_omega_weights <- list(parcel = 1.0, condition = 0.5)
  }
  if (!is.list(fixed_omega_weights) || 
      !all(c("parcel", "condition") %in% names(fixed_omega_weights)) || 
      !is.numeric(fixed_omega_weights$parcel) || length(fixed_omega_weights$parcel)!=1 ||
      !is.numeric(fixed_omega_weights$condition) || length(fixed_omega_weights$condition)!=1 ){
      stop("fixed_omega_weights must be a list with numeric elements 'parcel' and 'condition'.")
  }

  if (omega_mode == "adaptive") {
    if (m_task_rows > 0 && (is.null(reliability_scores) || !is.numeric(reliability_scores) || length(reliability_scores) != m_task_rows)) {
      stop("If omega_mode is 'adaptive' and m_task_rows > 0, reliability_scores must be a numeric vector of length m_task_rows.")
    }
    if (m_task_rows == 0 && !is.null(reliability_scores)){
        warning("reliability_scores provided but m_task_rows is 0; scores will be ignored.")
    }
  }
  
  # --- Construct Omega diagonal vector (using audit patch logic) ---
  omega_diag_vector <- numeric(N_total_rows)

  if (m_parcel_rows > 0) { # Use > 0 check for safety
      omega_diag_vector[1:m_parcel_rows] <- fixed_omega_weights$parcel
  }
  if (m_task_rows > 0) { # Use > 0 check
      idx <- (m_parcel_rows + 1):N_total_rows
      if (omega_mode == "fixed") {
          omega_diag_vector[idx] <- fixed_omega_weights$condition
      } else { # adaptive
          # Safeguard reliability scores: ensure finite, treat NA/NaN/Inf as 0
          safe_rel_scores <- reliability_scores
          non_finite_idx <- !is.finite(safe_rel_scores)
          if(any(non_finite_idx)){
              safe_rel_scores[non_finite_idx] <- 0
              warning("Non-finite reliability_scores found; treated as 0 for weighting.")
          }
          
          safe_r <- pmax(0, safe_rel_scores) # Ensure non-negative
          med <- stats::median(safe_r[safe_r > 0], na.rm = TRUE) # Median of strictly positive scores
          if (is.na(med)) med <- 0 # Handle case where no scores > 0
          med <- max(med, 1e-2) # Audit clamp: Avoid division by tiny median
          
          w_raw <- fixed_omega_weights$condition * safe_r / med
          omega_diag_vector[idx] <- pmin(w_raw, fixed_omega_weights$parcel) # Cap at parcel weight
      }
  }

  # Audit check: Early return if all weights are zero
  if (all(abs(omega_diag_vector) < 1e-14)) {
      warning("All Omega weights are effectively zero. Rotation is undefined; returning identity matrix.")
      return(diag(k_dims))
  }

  # --- Trace Rescaling (Optional) ---
  if (scale_omega_trace && N_total_rows > 0) {
      current_trace <- sum(omega_diag_vector)
      if (abs(current_trace) > 1e-9) { # Check absolute value for safety
          rescale_factor <- N_total_rows / current_trace
          omega_diag_vector <- omega_diag_vector * rescale_factor
      } else { 
          # If trace is near zero, but not all weights were zero initially (handled above),
          # it implies weights canceled out or were extremely small. Unweighted seems safest.
          warning("Sum of omega weights is near zero after potential adaptive step; proceeding as unweighted Procrustes.")
          omega_diag_vector <- rep(1.0, N_total_rows) 
      }
  }
  
  # Get square root for scaling rows (correct implementation for weighted Procrustes)
  sqrt_omega <- sqrt(pmax(0, omega_diag_vector)) # pmax guards against tiny negatives

  # --- Apply weights efficiently using sweep (Fix 2-A, 2-B) ---
  A_w <- sweep(A_source, 1, sqrt_omega, "*")
  T_w <- sweep(T_target, 1, sqrt_omega, "*")

  # --- Solve Standard Procrustes using SVD (Patch logic Fix 2-C) ---
  if (k_dims == 0) {
    return(matrix(0,0,0))
  }

  M <- crossprod(A_w, T_w) # k_dims x k_dims matrix
  
  if (all(abs(M) < 1e-14)) {
      warning("Cross-product matrix M (weighted) is near zero; rotation is ill-defined. Returning identity.")
      return(diag(k_dims))
  }
  
  svd_M <- svd(M)
  
  # Fast reflection fix using determinant sign (Fix 2-C)
  # Calculate sign of determinant of R = V U^T = sign(det(V)*det(U))
  # Note: svd()$u and svd()$v determinants are +/- 1
  sign_det <- sign(det(svd_M$v) * det(svd_M$u)) 
  # Flip the last column of V if det is -1 to ensure det(R) = +1
  V_corrected <- svd_M$v
  V_corrected[, k_dims] <- V_corrected[, k_dims] * sign_det 
  
  R <- V_corrected %*% t(svd_M$u)
  
  # Final sanity check on determinant (optional, should be close to 1 now)
  # final_det <- det(R)
  # if (abs(final_det - 1.0) > 1e-6) {
  #    warning(sprintf("Final rotation matrix determinant (%.4f) is not close to 1.", final_det))
  # }

  return(R)
}
</file>

<file path="tests/testthat/test-gev.R">
library(testthat)
library(Matrix)

# Assuming solve_gev_laplacian_primme is available from hatsa package (e.g. via devtools::load_all())

context("GEV Helper Functions: solve_gev_laplacian_primme")

test_that("solve_gev_laplacian_primme handles block-diagonal Laplacians and filters correctly", {
  skip_if_not_installed("PRIMME")

  # 1. Construct L1 (3x3 path graph Laplacian)
  W1 <- Matrix::sparseMatrix(i = c(1, 2, 2, 3), j = c(2, 1, 3, 2), x = 1, dims = c(3, 3))
  D1 <- Matrix::Diagonal(3, x = Matrix::rowSums(W1))
  L1 <- D1 - W1

  # 2. Construct L2 (2x2 path graph Laplacian)
  W2 <- Matrix::sparseMatrix(i = c(1, 2), j = c(2, 1), x = 1, dims = c(2, 2))
  D2 <- Matrix::Diagonal(2, x = Matrix::rowSums(W2))
  L2 <- D2 - W2

  # 3. Combine into a block-diagonal A
  A_mat <- Matrix::bdiag(L1, L2)
  V_p <- nrow(A_mat) # Should be 5

  # 4. B matrix (Identity)
  B_mat <- Matrix::Diagonal(V_p)

  # 5. Parameters for the test
  k_request_test <- V_p  # Request all eigenpairs
  lambda_max_thresh_test <- 1.5
  epsilon_reg_B_test <- 1e-6 # Matches default in solve_gev_laplacian_primme

  # Eigenvalues of A_mat are {0, 1, 3} from L1 and {0, 2} from L2.
  # Combined and sorted: {0, 0, 1, 2, 3}
  # PRIMME solves A v = lambda_primme B_reg v, where B_reg = (1 + epsilon_reg_B_test) I
  # So, lambda_primme = lambda_A / (1 + epsilon_reg_B_test)
  # Expected PRIMME values (approx): 0, 0, 1/(1+eps), 2/(1+eps), 3/(1+eps)
  # Approx: 0, 0, 0.999999, 1.999998, 2.999997

  # Filtering abs(lambda_primme) < 1.5 should keep the first three: ~0, ~0, ~1.
  expected_n_filtered <- 3
  expected_values_approx <- c(0, 0, 1) / (1 + epsilon_reg_B_test)

  result <- suppressMessages(solve_gev_laplacian_primme(
    A = A_mat,
    B = B_mat,
    k_request = k_request_test,
    lambda_max_thresh = lambda_max_thresh_test,
    epsilon_reg_B = epsilon_reg_B_test,
    tol = 1e-8 # Use a reasonable tolerance for PRIMME
  ))

  # Assertions
  expect_true(is.list(result))
  expect_named(result, c("vectors", "values", "n_converged", "n_filtered", "primme_stats"))

  # Check number of converged eigenvalues (PRIMME might struggle with exact multiplicity)
  # Allow for slight variation if k_request_test = V_p
  expect_gte(result$n_converged, k_request_test - 2) # e.g. at least 3 for V_p=5 if zeros are tricky
  expect_lte(result$n_converged, k_request_test)

  # Check filtered results
  expect_equal(result$n_filtered, expected_n_filtered)
  expect_equal(ncol(result$vectors), expected_n_filtered)
  expect_length(result$values, expected_n_filtered)

  # Verify that all returned values satisfy the threshold
  if (result$n_filtered > 0) {
    expect_true(all(abs(result$values) < lambda_max_thresh_test))
  }

  # Verify the actual filtered eigenvalues (sorted by absolute magnitude in the function)
  # The function sorts by abs value, so for {0,0, ~1}, order is preserved.
  if (result$n_filtered == expected_n_filtered) {
    expect_equal(result$values, expected_values_approx, tolerance = 1e-5)
  }

  # Test with a different threshold that should filter more
  lambda_max_thresh_tight <- 0.5
  expected_n_filtered_tight <- 2 # Should only keep the two ~0 eigenvalues
  expected_values_tight_approx <- c(0,0) / (1+epsilon_reg_B_test)
  
  result_tight <- suppressMessages(solve_gev_laplacian_primme(
    A = A_mat,
    B = B_mat,
    k_request = k_request_test,
    lambda_max_thresh = lambda_max_thresh_tight,
    epsilon_reg_B = epsilon_reg_B_test,
    tol = 1e-8
  ))
  
  expect_equal(result_tight$n_filtered, expected_n_filtered_tight)
  if (result_tight$n_filtered == expected_n_filtered_tight) {
      expect_equal(result_tight$values, expected_values_tight_approx, tolerance = 1e-5)
      if (result_tight$n_filtered > 0) {
        expect_true(all(abs(result_tight$values) < lambda_max_thresh_tight))
      }
  }
})
</file>

<file path="tests/testthat/test-hatsa_core_functionality.R">
describe("HATSA Core Functionality: projector object, S3 methods, and core functions", {

# Helper function to generate mock subject data (list of matrices)
# This will need to be more sophisticated for actual testing,
# ensuring data that doesn't break internal math (e.g., SVD, GPA).
.generate_mock_subject_data <- function(N_subjects, V_p, T_i_mean = 100, T_i_sd = 10) {
  lapply(1:N_subjects, function(i) {
    matrix(rnorm(round(rnorm(1, T_i_mean, T_i_sd)) * V_p), ncol = V_p)
  })
}

# Helper to get some default parameters for run_hatsa_core
.get_default_hatsa_params <- function(V_p) {
  list(
    anchor_indices = sample(1:V_p, min(V_p, 5)), # Ensure anchor_indices <= V_p
    spectral_rank_k = 3, # A small k for testing
    k_conn_pos = 5,
    k_conn_neg = 5,
    n_refine = 2
  )
}

test_that("run_hatsa_core output and hatsa_projector constructor integrity", {
  V_p <- 20
  N_subjects <- 4
  mock_data <- .generate_mock_subject_data(N_subjects, V_p)
  params <- .get_default_hatsa_params(V_p)

  # This might require mocking internal functions if they are complex or slow,
  # or carefully crafted small data that allows quick execution.
  # For now, assume run_hatsa_core can run with simple random data for basic checks.
  
  # Wrap in suppressMessages or similar if run_hatsa_core is verbose
  hatsa_obj <- suppressMessages(try(run_hatsa_core(
    subject_data_list = mock_data,
    anchor_indices = params$anchor_indices,
    spectral_rank_k = params$spectral_rank_k,
    k_conn_pos = params$k_conn_pos,
    k_conn_neg = params$k_conn_neg,
    n_refine = params$n_refine
  ), silent = TRUE))

  if (inherits(hatsa_obj, "try-error")) {
    core_error_msg <- attr(hatsa_obj, "condition")$message
    skip(paste0("run_hatsa_core failed (", core_error_msg, "), skipping dependent tests."))
    return()
  }

  # 1. Test that run_hatsa_core returns the correct S3 object
  expect_s3_class(hatsa_obj, "hatsa_projector")
  expect_s3_class(hatsa_obj, "multiblock_biprojector") # Check inheritance

  # 2. Test the hatsa_projector constructor (implicitly via run_hatsa_core output)
  #    Ensuring all components are correctly stored.
  
  # Check for presence of key elements
  expect_named(hatsa_obj, c("v", "s", "sdev", "preproc", "block_indices", 
                           "R_final_list", "U_original_list", "Lambda_original_list",
                           "Lambda_original_gaps_list", "T_anchor_final", "parameters", "method"),
               ignore.order = TRUE)

  # Check basic types/structures
  expect_true(is.matrix(hatsa_obj$v))
  expect_true(is.matrix(hatsa_obj$s))
  expect_true(is.numeric(hatsa_obj$sdev))
  expect_s3_class(hatsa_obj$preproc, "pre_processor")
  expect_true(is.list(hatsa_obj$block_indices))
  expect_true(is.list(hatsa_obj$R_final_list))
  expect_true(is.list(hatsa_obj$U_original_list))
  expect_true(is.list(hatsa_obj$Lambda_original_list))
  # T_anchor_final might be matrix or NULL depending on GPA outcome, test for type or allow NULL
  expect_true(is.matrix(hatsa_obj$T_anchor_final) || is.null(hatsa_obj$T_anchor_final)) 
  expect_true(is.list(hatsa_obj$parameters))
  expect_equal(hatsa_obj$method, "hatsa_core")

  # Check specific parameters stored
  expect_equal(hatsa_obj$parameters$k, params$spectral_rank_k)
  expect_equal(hatsa_obj$parameters$V_p, V_p)
  expect_equal(hatsa_obj$parameters$N_subjects, N_subjects)
  expect_length(hatsa_obj$Lambda_original_list, N_subjects)
  
  # Check consistency of list elements
  expect_length(hatsa_obj$R_final_list, N_subjects)
  expect_length(hatsa_obj$U_original_list, N_subjects)
  
  if (N_subjects > 0 && params$spectral_rank_k > 0) {
      if(length(hatsa_obj$U_original_list) > 0 && !is.null(hatsa_obj$U_original_list[[1]])) {
          expect_equal(ncol(hatsa_obj$U_original_list[[1]]), params$spectral_rank_k)
          expect_equal(nrow(hatsa_obj$U_original_list[[1]]), V_p)
      }
      if(length(hatsa_obj$Lambda_original_list) > 0 && !is.null(hatsa_obj$Lambda_original_list[[1]])) {
          expect_length(hatsa_obj$Lambda_original_list[[1]], params$spectral_rank_k)
      }
  }
})

test_that("S3 method output dimensions and types (coef, scores, sdev, block_indices)", {
  V_p <- 20
  N_subjects <- 3
  k <- 4 # Intentionally different from .get_default_hatsa_params for this test section if needed
  mock_data <- .generate_mock_subject_data(N_subjects, V_p)
  params <- .get_default_hatsa_params(V_p)
  params$spectral_rank_k <- k # override k

  hatsa_obj <- suppressMessages(try(run_hatsa_core(
    subject_data_list = mock_data,
    anchor_indices = params$anchor_indices,
    spectral_rank_k = params$spectral_rank_k,
    k_conn_pos = params$k_conn_pos,
    k_conn_neg = params$k_conn_neg,
    n_refine = params$n_refine
  ), silent = TRUE))

  if (inherits(hatsa_obj, "try-error")) {
    core_error_msg <- attr(hatsa_obj, "condition")$message
    skip(paste0("run_hatsa_core failed (", core_error_msg, "), skipping S3 method tests."))
    return()
  }

  # coef
  co <- coef(hatsa_obj)
  expect_true(is.matrix(co))
  expect_equal(nrow(co), V_p)
  expect_equal(ncol(co), k)

  # scores
  sc <- multivarious::scores(hatsa_obj)
  expect_true(is.matrix(sc))
  expect_equal(nrow(sc), N_subjects * V_p)
  expect_equal(ncol(sc), k)

  # sdev
  sdv <- multivarious::sdev(hatsa_obj)
  expect_true(is.numeric(sdv))
  expect_length(sdv, k)
  # expect_true(all(sdv == 1)) # As per current plan default

  # block_indices
  bi <- multivarious::block_indices(hatsa_obj)
  expect_true(is.list(bi))
  expect_length(bi, N_subjects)
  expect_equal(length(unlist(bi)), N_subjects * V_p) # All score rows covered
})

test_that("predict.hatsa_projector output dimensions and types", {
  V_p <- 25
  N_subjects_fit <- 3
  k_fit <- 3
  
  fit_data <- .generate_mock_subject_data(N_subjects_fit, V_p)
  fit_params <- .get_default_hatsa_params(V_p)
  fit_params$spectral_rank_k <- k_fit

  hatsa_obj <- suppressMessages(try(run_hatsa_core(
    subject_data_list = fit_data,
    anchor_indices = fit_params$anchor_indices,
    spectral_rank_k = fit_params$spectral_rank_k,
    k_conn_pos = fit_params$k_conn_pos,
    k_conn_neg = fit_params$k_conn_neg,
    n_refine = fit_params$n_refine
  ), silent = TRUE))
  
  if (inherits(hatsa_obj, "try-error")) {
    core_error_msg <- attr(hatsa_obj, "condition")$message
    skip(paste0("run_hatsa_core failed (", core_error_msg, "), skipping predict method tests."))
    return()
  }

  N_new_subjects <- 2
  newdata_list <- .generate_mock_subject_data(N_new_subjects, V_p)
  
  predicted_sketches <- suppressMessages(try(predict(hatsa_obj, newdata_list = newdata_list), silent = TRUE))

  if (inherits(predicted_sketches, "try-error")) {
    fail("predict.hatsa_projector failed with mock data.")
    return()
  }

  expect_true(is.list(predicted_sketches))
  expect_length(predicted_sketches, N_new_subjects)

  for (i in 1:N_new_subjects) {
    expect_true(is.matrix(predicted_sketches[[i]]))
    expect_equal(nrow(predicted_sketches[[i]]), V_p)
    expect_equal(ncol(predicted_sketches[[i]]), k_fit)
  }
  
  # Test with invalid newdata (e.g. wrong V_p) - expect warning and NULL output
  invalid_newdata_list <- .generate_mock_subject_data(1, V_p + 1) # Incorrect V_p
  
  # Call predict within try() to capture potential errors and the result
  predicted_invalid <- try(predict(hatsa_obj, newdata_list = invalid_newdata_list), silent = TRUE)

  # Check it did NOT error (only warned)
  expect_false(inherits(predicted_invalid, "try-error"))
  
  # Check that the call *does* produce the expected warning
  expect_warning(
    predict(hatsa_obj, newdata_list = invalid_newdata_list), # Call again just for warning
    regexp = "has \\d+ parcels, but model expects \\d+" 
  )
  
  # Check the output obtained from the try() call (if it didn't error)
  if (!inherits(predicted_invalid, "try-error")) {
      expect_true(is.list(predicted_invalid))
      expect_length(predicted_invalid, length(invalid_newdata_list)) # Should be 1
      if (length(predicted_invalid) == 1) { # Only check index if length is correct
          expect_null(predicted_invalid[[1]]) 
      }
  }
})

test_that("project_block.hatsa_projector output dimensions and types", {
  V_p <- 15
  N_subjects_fit <- 2
  k_fit <- 2
  
  fit_data <- .generate_mock_subject_data(N_subjects_fit, V_p)
  fit_params <- .get_default_hatsa_params(V_p)
  fit_params$spectral_rank_k <- k_fit

  hatsa_obj <- suppressMessages(try(run_hatsa_core(
    subject_data_list = fit_data,
    anchor_indices = fit_params$anchor_indices,
    spectral_rank_k = fit_params$spectral_rank_k,
    k_conn_pos = fit_params$k_conn_pos,
    k_conn_neg = fit_params$k_conn_neg,
    n_refine = fit_params$n_refine
  ), silent = TRUE))

  if (inherits(hatsa_obj, "try-error")) {
    core_error_msg <- attr(hatsa_obj, "condition")$message
    skip(paste0("run_hatsa_core failed (", core_error_msg, "), skipping project_block method tests."))
    return()
  }

  # Test with newdata = NULL (extract stored sketch)
  # Note: project_block extracts from U_aligned_list, not object$s directly in current form
  # This might need adjustment based on how project_block is intended to work with internal storage
  # For now, assuming it can retrieve something that was stored or computed during fit.
  if (N_subjects_fit > 0) {
    block1_sketch_stored <- suppressMessages(try(project_block(hatsa_obj, block = 1), silent = TRUE))
    if (!inherits(block1_sketch_stored, "try-error")) {
        expect_true(is.matrix(block1_sketch_stored))
        expect_equal(nrow(block1_sketch_stored), V_p)
        expect_equal(ncol(block1_sketch_stored), k_fit)
    } else {
        warning("project_block(hatsa_obj, block=1) failed, check implementation for stored sketch retrieval.")
    }
  }
  
  # Test with newdata provided
  singlesubject_newdata <- .generate_mock_subject_data(1, V_p)[[1]]
  block1_sketch_projected <- suppressMessages(try(project_block(hatsa_obj, newdata = singlesubject_newdata, block = 1), silent = TRUE))

  if (inherits(block1_sketch_projected, "try-error")) {
    fail("project_block with newdata failed.")
    return()
  }
  expect_true(is.matrix(block1_sketch_projected))
  expect_equal(nrow(block1_sketch_projected), V_p)
  expect_equal(ncol(block1_sketch_projected), k_fit)

  # Test error for invalid block index
  expect_error(project_block(hatsa_obj, block = N_subjects_fit + 1))
  expect_error(project_block(hatsa_obj, block = 0))
})

test_that("Edge case: spectral_rank_k = 1", {
  V_p <- 30
  N_subjects <- 2
  mock_data <- .generate_mock_subject_data(N_subjects, V_p)
  params <- .get_default_hatsa_params(V_p)
  params$spectral_rank_k <- 1 # Edge case k=1

  hatsa_obj_k1 <- suppressMessages(try(run_hatsa_core(
    subject_data_list = mock_data,
    anchor_indices = params$anchor_indices,
    spectral_rank_k = params$spectral_rank_k,
    k_conn_pos = params$k_conn_pos,
    k_conn_neg = params$k_conn_neg,
    n_refine = params$n_refine
  ), silent = TRUE))

  if (inherits(hatsa_obj_k1, "try-error")) {
    core_error_msg <- attr(hatsa_obj_k1, "condition")$message
    skip(paste0("run_hatsa_core failed for k=1 (", core_error_msg, "), skipping k=1 edge case tests."))
    return()
  }

  expect_s3_class(hatsa_obj_k1, "hatsa_projector")
  expect_equal(hatsa_obj_k1$parameters$k, 1)
  if (!is.null(coef(hatsa_obj_k1))) { # coef might be NULL if k=0 or error
      expect_equal(ncol(coef(hatsa_obj_k1)), 1)
  }
  if (!is.null(multivarious::scores(hatsa_obj_k1))) {
      expect_equal(ncol(multivarious::scores(hatsa_obj_k1)), 1)
  }
  
  # Test predict with k=1
  N_new_subjects <- 1
  newdata_list_k1 <- .generate_mock_subject_data(N_new_subjects, V_p)
  predicted_sketches_k1 <- suppressMessages(try(predict(hatsa_obj_k1, newdata_list = newdata_list_k1), silent = TRUE))

  if (!inherits(predicted_sketches_k1, "try-error") && length(predicted_sketches_k1) > 0 && !is.null(predicted_sketches_k1[[1]])) {
    expect_equal(ncol(predicted_sketches_k1[[1]]), 1)
  } else if (!inherits(predicted_sketches_k1, "try-error")) {
      warn("predict method for k=1 returned unexpected structure or empty result.")
  } else {
      warn("predict method for k=1 failed.")
  }

})

test_that("Edge case: Small N (e.g., N_subjects = 2, minimum for GPA to be non-trivial)", {
  # HATSA's GPA refinement might behave differently or be trivial for N < 2.
  # A typical "small N" test might be N=2 (minimum for Procrustes alignment)
  V_p <- 18
  N_subjects_small <- 2 
  mock_data_small_n <- .generate_mock_subject_data(N_subjects_small, V_p)
  params_small_n <- .get_default_hatsa_params(V_p)
  # Ensure anchor_indices are valid for V_p
  params_small_n$anchor_indices <- sample(1:V_p, min(V_p, params_small_n$spectral_rank_k +1 )) 


  hatsa_obj_small_n <- suppressMessages(try(run_hatsa_core(
    subject_data_list = mock_data_small_n,
    anchor_indices = params_small_n$anchor_indices,
    spectral_rank_k = params_small_n$spectral_rank_k,
    k_conn_pos = params_small_n$k_conn_pos,
    k_conn_neg = params_small_n$k_conn_neg,
    n_refine = params_small_n$n_refine
  ), silent = TRUE))

  if (inherits(hatsa_obj_small_n, "try-error")) {
    # If run_hatsa_core inherently requires N > 2 for some reason, this test might need adjustment
    # or the function should handle N=2 gracefully.
    core_error_msg <- attr(hatsa_obj_small_n, "condition")$message
    warning(paste0("run_hatsa_core failed for N_subjects=2: ", core_error_msg))
    skip(paste0("run_hatsa_core failed for N_subjects=2 (",core_error_msg,"), skipping small N tests."))
    return()
  }

  expect_s3_class(hatsa_obj_small_n, "hatsa_projector")
  expect_equal(hatsa_obj_small_n$parameters$N_subjects, N_subjects_small)
  # Add other relevant checks similar to the main constructor test
  expect_length(hatsa_obj_small_n$R_final_list, N_subjects_small)


  # Note: True N=1 case might be problematic for HATSA if GPA is essential.
  # The current run_hatsa_core might error or produce trivial rotations for N=1.
  # If N=1 should be supported with specific behavior (e.g. identity rotations),
  # that needs its own test case and code handling.
  # For now, N=2 is considered a "small N" edge case for alignment.
})

# Further tests could include:
# - Behavior when anchor_indices are problematic (e.g., too few, out of bounds - though run_hatsa_core might check this)
# - Behavior with different k_conn_pos/k_conn_neg values (e.g., 0)
# - Test if ncomp.projector and shape.projector (inherited) work as expected (Ticket 3 item)
#   (This would require multivarious to be available and its methods to be generic)
#   e.g. if (requireNamespace("multivarious", quietly = TRUE)) {
#          expect_equal(multivarious::ncomp(hatsa_obj), hatsa_obj$parameters$k)
#          # expect specific output for shape(hatsa_obj)
#        }
# - Test summary.hatsa_projector for basic execution without error.

test_that("summary.hatsa_projector executes", {
 V_p <- 20
  N_subjects <- 3
  k <- 3
  mock_data <- .generate_mock_subject_data(N_subjects, V_p)
  params <- .get_default_hatsa_params(V_p)
  params$spectral_rank_k <- k

  hatsa_obj <- suppressMessages(try(run_hatsa_core(
    subject_data_list = mock_data,
    anchor_indices = params$anchor_indices,
    spectral_rank_k = params$spectral_rank_k,
    k_conn_pos = params$k_conn_pos,
    k_conn_neg = params$k_conn_neg,
    n_refine = params$n_refine
  ), silent = TRUE))

  if (inherits(hatsa_obj, "try-error")) {
    core_error_msg <- attr(hatsa_obj, "condition")$message
    skip(paste0("run_hatsa_core failed (", core_error_msg, "), skipping summary method test."))
    return()
  }

  res_summary <- suppressMessages(try(summary(hatsa_obj), silent=TRUE))
  expect_false(inherits(res_summary, "try-error"))
  expect_s3_class(res_summary, "summary.hatsa_projector")
  expect_true(is.list(res_summary))

  # Check for key named elements from the summary object
  # e.g. expect_named(res_summary, c("N_subjects", "V_p", "k", "mean_anchor_error"))
})

# Consider adding tests for the inherited multivarious methods (Ticket 3)
# if (requireNamespace("multivarious", quietly = TRUE) && exists("ncomp.projector")) {
#   test_that("Inherited multivarious methods work (ncomp, shape)", {
#     V_p <- 10
#     N_subjects <- 2
#     k <- 2
#     mock_data <- .generate_mock_subject_data(N_subjects, V_p)
#     params <- .get_default_hatsa_params(V_p)
#     params$spectral_rank_k <- k
# 
#     hatsa_obj <- suppressMessages(try(run_hatsa_core(
#       subject_data_list = mock_data, anchor_indices = params$anchor_indices,
#       spectral_rank_k = k, k_conn_pos = params$k_conn_pos,
#       k_conn_neg = params$k_conn_neg, n_refine = params$n_refine
#     ), silent = TRUE))
# 
#     if (inherits(hatsa_obj, "try-error")) {
#       skip("run_hatsa_core failed, skipping inherited method tests.")
#       return()
#     }
# 
#     # Assuming ncomp is a generic and ncomp.projector is defined in multivarious
#     # If hatsa_projector inherits from projector, this should dispatch.
#     # This might need to be ncomp(hatsa_obj) if ncomp is generic enough.
#     # Or specific call if ncomp.projector is the actual function.
#     # For now, let's assume a generic `ncomp` that dispatches.
#     # Similar logic for `shape`.
#     
#     # This requires that `multivarious` is loaded and `ncomp` is a known generic.
#     # expect_equal(ncomp(hatsa_obj), k) 
#     # expect_true(is.list(shape(hatsa_obj))) # or whatever shape returns
#     # message("Skipping ncomp/shape tests as direct invocation from multivarious isn't set up here.")
#   })
# }
})
</file>

<file path="tests/testthat/test-hatsa_validation_metrics.R">
library(testthat)
skip_on_cran()
skip_if_not_installed("vegan")

# Helper function to create a mock hatsa_projector object
# For simplicity, it only populates fields essential for validation metric tests
# V_p = parcels, k = rank, N_subjects = subjects
create_mock_hatsa_projector <- function(v_mat = NULL, 
                                        R_list = NULL, 
                                        Lambda_list = NULL, 
                                        T_anchor = NULL, 
                                        params = list()) {
  obj <- list(
    v = v_mat,
    R_final_list = R_list,
    Lambda_original_list = Lambda_list,
    T_anchor_final = T_anchor,
    parameters = params # e.g., list(k=k, N_subjects=N, V_p=Vp)
  )
  class(obj) <- c("hatsa_projector", "multiblock_biprojector", "projector", "list")
  return(obj)
}

test_that("compute_v_recovery works correctly", {
  set.seed(123)
  Vp <- 20
  k_rank <- 5

  # Case 1: Perfect recovery
  U_true_perfect <- matrix(rnorm(Vp * k_rank), Vp, k_rank)
  mock_proj_perfect <- create_mock_hatsa_projector(v_mat = U_true_perfect)
  
  res_perfect <- compute_v_recovery(mock_proj_perfect, U_true_perfect)

  expect_type(res_perfect, "list")
  expect_named(res_perfect, c("correlation", "frobenius_norm_diff", "procrustes_result", "v_aligned"))
  # Using tolerance for these tests since Procrustes alignment might not be exact
  expect_equal(res_perfect$correlation, 1, tolerance = 0.05)
  expect_true(res_perfect$frobenius_norm_diff < 2.0)
  expect_true(inherits(res_perfect$procrustes_result, "procrustes"))

  # Case 2: Non-perfect recovery (v is different from U_true)
  v_est_imperfect <- matrix(rnorm(Vp * k_rank), Vp, k_rank)
  mock_proj_imperfect <- create_mock_hatsa_projector(v_mat = v_est_imperfect)
  res_imperfect <- compute_v_recovery(mock_proj_imperfect, U_true_perfect)
  
  expect_true(res_imperfect$correlation < 1 || is.na(res_imperfect$correlation)) # NA if var is 0
  expect_true(res_imperfect$frobenius_norm_diff > 1e-7)
  
  # Case 3: Error handling - mismatched dimensions
  U_true_wrong_dim <- matrix(rnorm(Vp * (k_rank + 1)), Vp, k_rank + 1)
  expect_error(
    compute_v_recovery(mock_proj_perfect, U_true_wrong_dim),
    "Dimensions.*must match"
  )
  
  U_true_wrong_dim_rows <- matrix(rnorm((Vp+1) * k_rank), Vp+1, k_rank)
   expect_error(
    compute_v_recovery(mock_proj_perfect, U_true_wrong_dim_rows),
    "Dimensions.*must match"
  )

  # Case 4: Error handling - hatsa_object$v is NULL
  mock_proj_null_v <- create_mock_hatsa_projector(v_mat = NULL)
  expect_error(
    compute_v_recovery(mock_proj_null_v, U_true_perfect),
    "NULL.*Cannot compute"
  )
  
  # Case 5: Error handling - U_true is not a matrix
  expect_error(
    compute_v_recovery(mock_proj_perfect, as.vector(U_true_perfect)),
    "must be a numeric matrix"
  )
  
  # Case 6: Error handling - hatsa_object is not a hatsa_projector
  expect_error(
    compute_v_recovery(list(v=U_true_perfect), U_true_perfect),
    "must be of class 'hatsa_projector'"
  )
  
  # Case 7: Zero variance inputs (should lead to NA correlation or specific handling)
  v_const <- matrix(1, Vp, k_rank)
  U_true_const <- matrix(1, Vp, k_rank)
  mock_proj_const_v <- create_mock_hatsa_projector(v_mat = v_const)
  res_const_v_true_const <- compute_v_recovery(mock_proj_const_v, U_true_const)
  # vegan::procrustes might align them perfectly. cor(c(matrix(1,2,2)), c(matrix(1,2,2))) is NA
  expect_true(is.na(res_const_v_true_const$correlation) || res_const_v_true_const$correlation == 1)
  # Skip frobenius norm check for constant matrices - can result in NaN
  
  U_true_diff_const <- matrix(2, Vp, k_rank)
  res_const_v_true_diff <- compute_v_recovery(mock_proj_const_v, U_true_diff_const)
  expect_true(is.na(res_const_v_true_diff$correlation))
  # Skip frobenius norm check for constant matrices - can result in NaN
  
})

test_that("compute_anchor_template_recovery works correctly", {
  set.seed(456)
  Vp_total <- 30 # Total parcels in U_true
  N_anchors <- 10 # Number of anchor parcels
  k_rank <- 4

  U_true_full <- matrix(rnorm(Vp_total * k_rank), Vp_total, k_rank)
  anchor_idx_true <- sample(1:Vp_total, N_anchors)
  U_true_anchors_actual <- U_true_full[anchor_idx_true, , drop = FALSE]

  # Case 1: Perfect recovery
  mock_proj_perfect_anchor <- create_mock_hatsa_projector(T_anchor = U_true_anchors_actual)
  res_perfect <- compute_anchor_template_recovery(mock_proj_perfect_anchor, U_true_full, anchor_idx_true)

  expect_type(res_perfect, "list")
  expect_named(res_perfect, c("correlation", "frobenius_norm_diff", "procrustes_result", "T_anchor_aligned"))
  # Using tolerance for these tests since Procrustes alignment might not be exact
  expect_equal(res_perfect$correlation, 1, tolerance = 0.05)
  expect_true(res_perfect$frobenius_norm_diff < 2.0)
  expect_true(inherits(res_perfect$procrustes_result, "procrustes"))

  # Case 2: Non-perfect recovery
  T_anchor_est_imperfect <- matrix(rnorm(N_anchors * k_rank), N_anchors, k_rank)
  mock_proj_imperfect_anchor <- create_mock_hatsa_projector(T_anchor = T_anchor_est_imperfect)
  res_imperfect <- compute_anchor_template_recovery(mock_proj_imperfect_anchor, U_true_full, anchor_idx_true)

  expect_true(res_imperfect$correlation < 1 || is.na(res_imperfect$correlation))
  expect_true(res_imperfect$frobenius_norm_diff > 1e-7)

  # Case 3: Error handling - mismatched dimensions for T_anchor_final and U_true_anchors
  T_anchor_wrong_dim_cols <- matrix(rnorm(N_anchors * (k_rank + 1)), N_anchors, k_rank + 1)
  mock_proj_wrong_dim_anchor <- create_mock_hatsa_projector(T_anchor = T_anchor_wrong_dim_cols)
  expect_error(
    compute_anchor_template_recovery(mock_proj_wrong_dim_anchor, U_true_full, anchor_idx_true),
    "Dimensions.*must match"
  )
  
  T_anchor_wrong_dim_rows <- matrix(rnorm((N_anchors + 1) * k_rank), N_anchors + 1, k_rank)
  mock_proj_wrong_dim_anchor_rows <- create_mock_hatsa_projector(T_anchor = T_anchor_wrong_dim_rows)
  expect_error(
    compute_anchor_template_recovery(mock_proj_wrong_dim_anchor_rows, U_true_full, anchor_idx_true),
    "Dimensions.*must match"
  )

  # Case 4: Error handling - T_anchor_final is NULL
  mock_proj_null_T <- create_mock_hatsa_projector(T_anchor = NULL)
  expect_error(
    compute_anchor_template_recovery(mock_proj_null_T, U_true_full, anchor_idx_true),
    "NULL.*Cannot compute"
  )

  # Case 5: Error handling - invalid anchor_indices_true
  expect_error(
    compute_anchor_template_recovery(mock_proj_perfect_anchor, U_true_full, c(anchor_idx_true, Vp_total + 1)),
    "Invalid.*anchor_indices"
  )
  expect_error(
    compute_anchor_template_recovery(mock_proj_perfect_anchor, U_true_full, c(0, anchor_idx_true)),
    "Invalid.*anchor_indices"
  )
  expect_error(
    compute_anchor_template_recovery(mock_proj_perfect_anchor, U_true_full, "not_numeric"),
    "must be a numeric vector"
  )
  
  # Case 6: Error handling - U_true is not a matrix
  expect_error(
    compute_anchor_template_recovery(mock_proj_perfect_anchor, as.vector(U_true_full), anchor_idx_true),
    "must be a numeric matrix"
  )
  
  # Case 7: Constant inputs
  T_const <- matrix(1, N_anchors, k_rank)
  U_true_full_const_anchors <- U_true_full
  U_true_full_const_anchors[anchor_idx_true, ] <- 1 # Make true anchors also constant and same
  mock_proj_const_T <- create_mock_hatsa_projector(T_anchor = T_const)
  res_const_T_true_const <- compute_anchor_template_recovery(mock_proj_const_T, U_true_full_const_anchors, anchor_idx_true)
  expect_true(is.na(res_const_T_true_const$correlation) || res_const_T_true_const$correlation == 1)
  # Skip frobenius norm check for constant matrices - can result in NaN
  
})

test_that("compute_rotation_recovery works correctly", {
  skip_if_not_installed("expm")
  set.seed(789)
  N_subj <- 3
  k_rank <- 3

  random_SOk <- function(k) {
    M <- matrix(rnorm(k*k), k, k)
    qr_M <- qr(M)
    Q <- qr.Q(qr_M)
    if (det(Q) < 0) {
      Q[,1] <- -Q[,1]
    }
    return(Q)
  }

  R_true_list_perfect <- replicate(N_subj, diag(k_rank), simplify = FALSE)
  R_est_list_perfect <- R_true_list_perfect
  
  mock_proj_perfect_rot <- create_mock_hatsa_projector(R_list = R_est_list_perfect)

  # Case 1: Perfect recovery (all R_est == R_true == Identity)
  res_perfect <- compute_rotation_recovery(mock_proj_perfect_rot, R_true_list_perfect)
  expect_type(res_perfect, "double")
  expect_length(res_perfect, N_subj)
  expect_true(all(abs(res_perfect - 0) < 1e-7))

  # Case 2: Perfect recovery (all R_est == R_true, but not Identity)
  R_true_list_general <- replicate(N_subj, random_SOk(k_rank), simplify = FALSE)
  R_est_list_general <- R_true_list_general
  mock_proj_general_rot <- create_mock_hatsa_projector(R_list = R_est_list_general)
  res_general <- compute_rotation_recovery(mock_proj_general_rot, R_true_list_general)
  expect_true(all(abs(res_general - 0) < 1e-7))

  # Case 3: Imperfect recovery
  R_est_list_imperfect <- R_true_list_general
  theta <- pi/18 # 10 degrees
  rot_mat_small <- matrix(c(cos(theta), -sin(theta), 0,
                          sin(theta),  cos(theta), 0,
                          0,           0,          1), nrow=3, byrow=TRUE)
  if (k_rank == 3) R_est_list_imperfect[[1]] <- R_est_list_imperfect[[1]] %*% rot_mat_small
  
  mock_proj_imperfect_rot <- create_mock_hatsa_projector(R_list = R_est_list_imperfect)
  res_imperfect <- compute_rotation_recovery(mock_proj_imperfect_rot, R_true_list_general)
  if (k_rank == 3) {
      expect_true(res_imperfect[1] > 1e-7) 
      expect_true(abs(res_imperfect[1] - 10) < 0.1) 
       if (N_subj > 1) expect_true(all(abs(res_imperfect[2:N_subj] - 0) < 1e-7)) 
  } else {
      expect_true(any(res_imperfect > 1e-7)) 
  }
  

  # Case 4: Error handling - R_final_list is NULL
  mock_proj_null_R <- create_mock_hatsa_projector(R_list = NULL)
  expect_error(
    compute_rotation_recovery(mock_proj_null_R, R_true_list_perfect),
    "NULL.*Cannot compute"
  )

  # Case 5: Error handling - R_true_list is not a list
  expect_error(
    compute_rotation_recovery(mock_proj_perfect_rot, "not_a_list"),
    "must be a list"
  )

  # Case 6: Different list lengths - no longer errors, but adapts
  R_true_shorter <- R_true_list_perfect[-1]
  res_longer_est <- compute_rotation_recovery(mock_proj_perfect_rot, R_true_shorter)
  expect_length(res_longer_est, length(R_true_shorter))

  # Case 7: Handling of non-matrix elements in lists
  R_est_list_non_matrix <- R_est_list_perfect
  R_est_list_non_matrix[[1]] <- NA  # NA is not a matrix - should trigger warning
  mock_proj_non_matrix <- create_mock_hatsa_projector(R_list = R_est_list_non_matrix)
  
  expect_warning(
    res_non_matrix_el <- compute_rotation_recovery(mock_proj_non_matrix, R_true_list_perfect),
    "Missing or non-matrix rotation"
  )
  expect_true(is.na(res_non_matrix_el[1]))
  if (N_subj > 1) expect_false(any(is.na(res_non_matrix_el[-1])))
  
  # Another case with a non-matrix value
  R_est_list_string <- R_est_list_perfect
  R_est_list_string[[1]] <- "not_a_matrix"
  mock_proj_string <- create_mock_hatsa_projector(R_list = R_est_list_string)
  
  expect_warning(
    res_string_el <- compute_rotation_recovery(mock_proj_string, R_true_list_perfect),
    "Missing or non-matrix rotation"
  )
  expect_true(is.na(res_string_el[1]))
  
  # Case for true list with non-matrix elements
  R_true_with_non_matrix <- R_true_list_perfect
  R_true_with_non_matrix[[2]] <- "not_a_matrix"
  expect_warning(
    res_non_mat_el <- compute_rotation_recovery(mock_proj_perfect_rot, R_true_with_non_matrix),
    "Missing or non-matrix rotation"
  )
  expect_true(is.na(res_non_mat_el[2]))
  
  # Case 8: Dimension mismatch for a specific subject
  R_est_dim_mismatch <- R_est_list_perfect
  if (k_rank > 1) R_est_dim_mismatch[[1]] <- diag(k_rank-1)
  mock_proj_dim_mismatch <- create_mock_hatsa_projector(R_list = R_est_dim_mismatch)
  if (k_rank > 1) {
      expect_warning(
        res_dim_mismatch <- compute_rotation_recovery(mock_proj_dim_mismatch, R_true_list_perfect),
        "Dimension mismatch"
      )
      expect_true(is.na(res_dim_mismatch[1]))
  }
  
  # Case 9: Different length lists - should work with new implementation
  R_est_shorter <- R_est_list_perfect[1:(N_subj-1)]
  mock_proj_shorter <- create_mock_hatsa_projector(R_list = R_est_shorter)
  res_shorter <- compute_rotation_recovery(mock_proj_shorter, R_true_list_perfect)
  expect_length(res_shorter, length(R_est_shorter))
})

test_that("compute_eigenvalue_fidelity works correctly", {
  set.seed(101)
  N_subj <- 2
  k_true <- 10 # True number of eigenvalues available
  k_est <- 8   # Estimated number of eigenvalues available (e.g. from k in hatsa)

  # Generate true eigenvalues (e.g., exponentially decaying)
  true_ev_subj1 <- sort(exp(-(1:k_true)/2) + rnorm(k_true, 0, 0.01), decreasing = TRUE)
  true_ev_subj2 <- sort(exp(-(1:k_true)/2) + rnorm(k_true, 0, 0.01), decreasing = TRUE)
  true_ev_list_full <- list(Sub1 = true_ev_subj1, Sub2 = true_ev_subj2)

  # Estimated eigenvalues (typically shorter or slightly different)
  est_ev_subj1 <- true_ev_subj1[1:k_est] + rnorm(k_est, 0, 0.05)
  est_ev_subj2 <- true_ev_subj2[1:k_est] + rnorm(k_est, 0, 0.05)
  est_ev_list <- list(Sub1 = est_ev_subj1, Sub2 = est_ev_subj2)
  
  mock_proj <- create_mock_hatsa_projector(Lambda_list = est_ev_list)

  # Case 1: Basic run, k_to_compare = NULL (compare min available: k_est)
  res1 <- compute_eigenvalue_fidelity(mock_proj, true_ev_list_full)
  expect_type(res1, "list")
  expect_length(res1, N_subj)
  expect_named(res1, c("Sub1", "Sub2"))
  expect_named(res1$Sub1, c("correlation", "mse", "num_compared"))
  expect_equal(res1$Sub1$num_compared, k_est)
  expect_true(is.numeric(res1$Sub1$correlation) && !is.na(res1$Sub1$correlation))
  expect_true(is.numeric(res1$Sub1$mse) && res1$Sub1$mse >= 0)

  # Case 2: k_to_compare specified and valid
  k_compare_val <- 5
  res2 <- compute_eigenvalue_fidelity(mock_proj, true_ev_list_full, k_to_compare = k_compare_val)
  expect_equal(res2$Sub1$num_compared, k_compare_val)
  expect_equal(res2$Sub2$num_compared, k_compare_val)

  # Case 3: k_to_compare larger than available (should use min available and warn)
  expect_warning(
    res3 <- compute_eigenvalue_fidelity(mock_proj, true_ev_list_full, k_to_compare = k_est + 1),
    "k_to_compare.*is greater than available eigenvalues"
  )
  expect_equal(res3$Sub1$num_compared, k_est)
  
  # Case 4: True eigenvalues as a single common vector
  common_true_ev <- sort(exp(-(1:k_true)/2), decreasing = TRUE)
  res4 <- compute_eigenvalue_fidelity(mock_proj, common_true_ev)
  expect_equal(res4$Sub1$num_compared, k_est)
  expect_true(is.numeric(res4$Sub1$correlation))

  # Case 5: Error handling - Lambda_original_list is NULL
  mock_proj_null_lambda <- create_mock_hatsa_projector(Lambda_list = NULL)
  expect_error(
    compute_eigenvalue_fidelity(mock_proj_null_lambda, true_ev_list_full),
    "NULL.*Cannot compute"
  )

  # Case 6: Error handling - list lengths differ
  expect_error(
    compute_eigenvalue_fidelity(mock_proj, true_ev_list_full[-1]),
    "Length.*must be equal"
  )
  
  # Case 7: Handling of NULL or non-numeric elements in lists
  
  # First subject has NULL eigenvalues
  mock_proj_missing_sub1 <- create_mock_hatsa_projector(Lambda_list = list(
      Sub1 = NULL,
      Sub2 = est_ev_list$Sub2
  ))
  
  expect_warning(
      res_missing_sub1 <- compute_eigenvalue_fidelity(mock_proj_missing_sub1, true_ev_list_full),
      "Missing or non-numeric"
  )
  
  expect_true(is.na(res_missing_sub1$Sub1$correlation))
  expect_true(is.na(res_missing_sub1$Sub1$mse))
  expect_equal(res_missing_sub1$Sub1$num_compared, 0)
  expect_true(is.numeric(res_missing_sub1$Sub2$correlation) && !is.na(res_missing_sub1$Sub2$correlation))
  
  # True eigenvalues has non-numeric element
  true_ev_list_bad <- list(
      Sub1 = true_ev_list_full$Sub1,
      Sub2 = "not_numeric"
  )
  
  expect_warning(
      res_bad_true <- compute_eigenvalue_fidelity(mock_proj, true_ev_list_bad),
      "Missing or non-numeric"
  )
  
  expect_true(!is.na(res_bad_true$Sub1$correlation)) 
  expect_true(is.na(res_bad_true$Sub2$correlation))

  # Case 8: Empty eigenvalue vectors
  est_ev_empty <- list(
      Sub1 = numeric(0), 
      Sub2 = est_ev_list$Sub2
  )
  
  mock_proj_empty <- create_mock_hatsa_projector(Lambda_list = est_ev_empty)
  expect_warning(
      res_empty <- compute_eigenvalue_fidelity(mock_proj_empty, true_ev_list_full),
      "Empty eigenvalue"
  )
  
  expect_true(is.na(res_empty$Sub1$correlation))
  expect_equal(res_empty$Sub1$num_compared, 0)
  expect_true(!is.na(res_empty$Sub2$correlation))
  
  # Case 9: Constant eigenvalues (correlation NA or 1, MSE)
  const_ev <- rep(1, k_est)
  mock_proj_const <- create_mock_hatsa_projector(Lambda_list = list(S1=const_ev, S2=const_ev+1))
  true_const_list <- list(S1=const_ev, S2=const_ev)
  res_const <- compute_eigenvalue_fidelity(mock_proj_const, true_const_list)
  expect_true(res_const$S1$correlation == 1) # Identical constant vectors
  expect_equal(res_const$S1$mse, 0)
  expect_true(is.na(res_const$S2$correlation)) # Different constant vectors
  expect_equal(res_const$S2$mse, 1) 
  
  # Case 10: NA values in eigenvalues
  est_ev_with_na <- est_ev_list
  est_ev_with_na$Sub1[1] <- NA
  mock_proj_na <- create_mock_hatsa_projector(Lambda_list = est_ev_with_na)
  expect_warning(
    res_na <- compute_eigenvalue_fidelity(mock_proj_na, true_ev_list_full, k_to_compare = k_est),
    "NA values found"
  )
  expect_true(is.na(res_na$Sub1$correlation))
  expect_true(is.na(res_na$Sub1$mse))
  expect_equal(res_na$Sub1$num_compared, k_est)
  
})
</file>

<file path="tests/testthat/test-metrics.R">
context("metrics - misalign_deg")

test_that("misalign_deg basic functionality and known angles", {
  # Identity matrices
  R_ident2 <- diag(2)
  expect_equal(misalign_deg(R_ident2, R_ident2), 0)

  R_ident3 <- diag(3)
  expect_equal(misalign_deg(R_ident3, R_ident3), 0)

  # Known 2D rotations
  theta_30_rad <- pi/6
  R_2d_30deg <- matrix(c(cos(theta_30_rad), -sin(theta_30_rad),
                         sin(theta_30_rad),  cos(theta_30_rad)), 2, 2)
  expect_equal(misalign_deg(R_2d_30deg, R_ident2), 30, tolerance = 1e-7)
  expect_equal(misalign_deg(R_ident2, R_2d_30deg), 30, tolerance = 1e-7) # Symmetry

  theta_45_rad <- pi/4
  R_2d_45deg <- matrix(c(cos(theta_45_rad), -sin(theta_45_rad),
                         sin(theta_45_rad),  cos(theta_45_rad)), 2, 2)
  expect_equal(misalign_deg(R_2d_45deg, R_ident2), 45, tolerance = 1e-7)

  theta_90_rad <- pi/2
  R_2d_90deg <- matrix(c(cos(theta_90_rad), -sin(theta_90_rad),
                         sin(theta_90_rad),  cos(theta_90_rad)), 2, 2)
  expect_equal(misalign_deg(R_2d_90deg, R_ident2), 90, tolerance = 1e-7)

  # Known 3D rotation (around Z-axis)
  theta_60_rad <- pi/3
  R_3d_z_60deg <- matrix(c(cos(theta_60_rad), -sin(theta_60_rad), 0,
                           sin(theta_60_rad),  cos(theta_60_rad), 0,
                           0,                  0,                 1), 3, 3, byrow = TRUE)
  expect_equal(misalign_deg(R_3d_z_60deg, R_ident3), 60, tolerance = 1e-7)
  expect_equal(misalign_deg(R_ident3, R_3d_z_60deg), 60, tolerance = 1e-7) # Symmetry
  
  # Known 3D rotation (around Y-axis)
  theta_90_rad_y <- pi/2
  R_3d_y_90deg <- matrix(c(cos(theta_90_rad_y), 0, sin(theta_90_rad_y),
                           0,                   1, 0,
                           -sin(theta_90_rad_y),0, cos(theta_90_rad_y)), 3, 3, byrow = TRUE)
  expect_equal(misalign_deg(R_3d_y_90deg, R_ident3), 90, tolerance = 1e-7)

  # Test composition: R_AB, R_BC, expect dist(R_AC, I) related to dist(R_AB,I) + dist(R_BC,I)
  # Not strictly additive for large rotations, but for small ones it's close.
  # For SO(3), angle(R1*R2) is not angle(R1)+angle(R2) in general.
  # Geodesic from R_A to R_C via R_B: d(A,C) <= d(A,B) + d(B,C)
  R_A <- R_2d_30deg
  R_B <- R_2d_45deg %*% R_A # R_B is R_A rotated by 45 deg more
  # Misalignment between R_B and R_A should be 45 deg
  expect_equal(misalign_deg(R_B, R_A), 45, tolerance = 1e-7)
  
})

test_that("misalign_deg input validation and error handling", {
  R_valid2 <- diag(2)
  R_valid3 <- diag(3)

  # Non-matrix inputs
  expect_warning(res <- misalign_deg(as.vector(R_valid2), R_valid2), "Inputs must be matrices.")
  expect_true(is.na(res))
  expect_warning(res <- misalign_deg(R_valid2, "not_a_matrix"), "Inputs must be matrices.")
  expect_true(is.na(res))

  # Mismatched dimensions
  expect_warning(res <- misalign_deg(R_valid3, R_valid2), "Rotation matrices must have the same dimensions.")
  expect_true(is.na(res))

  # Non-square matrices
  R_nonsquare1 <- matrix(1:6, 2, 3)
  R_nonsquare2 <- matrix(1:6, 2, 3)
  expect_warning(res <- misalign_deg(R_nonsquare1, R_nonsquare2), "Matrices must be square.")
  expect_true(is.na(res))
  
  # Case where M_rel is identity (already covered by basic tests but good to be explicit)
  R1 <- diag(3)
  expect_equal(misalign_deg(R1,R1), 0)
  
})

# It's hard to reliably trigger the expm::logm failure for a valid SO(k) matrix in a simple test
# without creating a matrix that is numerically very problematic (e.g., extremely close to singular,
# or far from orthogonal, which misalign_deg doesn't strictly check before crossprod).
# The safe_logm_internal is designed to catch errors from expm::logm.
# We can test the fallback if `expm` is not available by temporarily mocking `requireNamespace`.

test_that("misalign_deg fallback mechanism when expm is not available", {
  # Mock requireNamespace to simulate expm not being installed
  mockery::stub(misalign_deg, 'requireNamespace', function(pkg, ...) if(pkg=='expm') FALSE else TRUE)
  
  R1 <- diag(3)
  theta <- pi/4 # 45 degrees
  R2 <- matrix(c(cos(theta), -sin(theta), 0,
                 sin(theta),  cos(theta), 0,
                 0,           0,          1), nrow=3, byrow=TRUE)
  
  # For 3x3, the fallback (acos((tr(M_rel)-1)/2)) should be accurate
  expect_warning(
    angle_fallback <- misalign_deg(R2, R1, method="geodesic"), 
    "Package 'expm' not available for geodesic method. Using fallback trace-based calculation."
  )
  expect_equal(angle_fallback, 45, tolerance = 1e-7)
  
  # For 2x2, the fallback trace is trace(R)/2 for cos(theta)
  R1_2d <- diag(2)
  R2_2d <- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2,2)
  expect_warning(
    angle_fallback_2d <- misalign_deg(R2_2d, R1_2d, method="geodesic"),
    "Package 'expm' not available for geodesic method. Using fallback trace-based calculation."
  )
  # Also, the k_dim != 3 warning for the fallback
  expect_warning(
    misalign_deg(R2_2d, R1_2d, method="geodesic"),
    "Fallback trace-based angle is most accurate for 3x3 rotations \\(SO\\(3\\)\\)."
  )
  expect_equal(angle_fallback_2d, 45, tolerance = 1e-7)
})

test_that("misalign_deg specific k_dim warnings for fallback", {
  # Mock requireNamespace to force fallback
  mockery::stub(misalign_deg, 'requireNamespace', function(pkg, ...) if(pkg=='expm') FALSE else TRUE)
  
  # k=3 should not warn about k_dim != 3 for fallback accuracy
  R1_3d <- diag(3)
  theta <- pi/6
  R2_3d <- matrix(c(cos(theta), -sin(theta), 0, sin(theta), cos(theta), 0, 0, 0, 1), 3,3, byrow=TRUE)
  expect_warning(
    misalign_deg(R2_3d, R1_3d, method="geodesic"), # This warns about expm missing
    "Package 'expm' not available for geodesic method" 
  )
  
  # k=2 should warn about k_dim != 3 for fallback accuracy
  R1_2d <- diag(2)
  R2_2d <- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2,2)
  
  # Need to capture and check both warnings with a regex approach
  expect_warning(
    res <- misalign_deg(R2_2d, R1_2d, method="geodesic"),
    "Package 'expm' not available for geodesic method"
  )
  expect_warning(
    misalign_deg(R2_2d, R1_2d, method="geodesic"),
    "Fallback trace-based angle is most accurate for 3x3 rotations"
  )
  
  # k=4 should warn about k_dim != 3 for fallback accuracy
  R1_4d <- diag(4)
  # Simple rotation in first 2 dims
  R2_4d_block <- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), 2,2)
  R2_4d <- diag(4)
  R2_4d[1:2,1:2] <- R2_4d_block
  
  # Need to capture and check both warnings with a regex approach
  expect_warning(
    res <- misalign_deg(R2_4d, R1_4d, method="geodesic"),
    "Package 'expm' not available for geodesic method"
  )
  expect_warning(
    misalign_deg(R2_4d, R1_4d, method="geodesic"),
    "Fallback trace-based angle is most accurate for 3x3 rotations"
  )
})
</file>

<file path="tests/testthat/test-spectral_graph_construction.R">
describe("Spectral Graph Construction Functionality", {

# Helper function for z-scoring non-zero elements of a sparse matrix's upper triangle
# (Assuming zscore_nonzero_sparse exists elsewhere or is accessible)
# If not, we might need to define a simplified version here for testing SGC-004

# Test TCK-SGC-001: Basic Sparsification & Output
test_that("TCK-SGC-001: compute_subject_connectivity_graph_sparse basic output", {
  skip_if_not_installed("Matrix")
  library(Matrix)
  
  set.seed(101)
  V_p <- 10
  T_i <- 50
  k_pos <- 2
  k_neg <- 1
  p_names <- paste0("P", 1:V_p)
  
  # Create data with some correlation structure
  base_signal <- matrix(rnorm(T_i * 2), ncol = 2)
  noise <- matrix(rnorm(T_i * V_p), ncol = V_p)
  X_ts <- matrix(0, nrow = T_i, ncol = V_p)
  X_ts[, 1:4] <- base_signal[, 1] * matrix(runif(T_i*4, 0.5, 1.5), T_i, 4) + noise[, 1:4] * 0.5 # Group 1
  X_ts[, 5:7] <- base_signal[, 2] * matrix(runif(T_i*3, 0.5, 1.5), T_i, 3) + noise[, 5:7] * 0.5 # Group 2
  X_ts[, 8:10] <- noise[, 8:10] # Noise
  colnames(X_ts) <- p_names

  # Function Call
  # Assuming compute_subject_connectivity_graph_sparse exists and performs all steps
  output_W <- compute_subject_connectivity_graph_sparse(
      X_subject = X_ts,
      parcel_names = p_names,
      k_conn_pos = k_pos,
      k_conn_neg = k_neg,
      use_dtw = FALSE
  )

  # Assertions
  expect_s4_class(output_W, "dgCMatrix")
  expect_equal(dim(output_W), c(V_p, V_p))
  expect_true(Matrix::isSymmetric(output_W))
  expect_equal(unname(Matrix::diag(output_W)), rep(0, V_p), tolerance = 1e-8) # Diagonal should be zero

  # Check sparsity for node 1 (expect k_pos + k_neg = 3 outgoing originally, maybe more after symmetrization)
  # Calculate raw correlations for node 1
  cor_node1 <- stats::cor(X_ts[, 1], X_ts)[1, -1] # Exclude self-correlation
  pos_neighbors_node1 <- order(cor_node1[cor_node1 > 0], decreasing = TRUE)[1:min(k_pos, sum(cor_node1 > 0))] + 1 # +1 to adjust index after removing self
  neg_neighbors_node1 <- order(cor_node1[cor_node1 < 0], decreasing = FALSE)[1:min(k_neg, sum(cor_node1 < 0))] + 1
  direct_neighbors_node1 <- c(pos_neighbors_node1, neg_neighbors_node1)
  
  # Check non-zero count for row/col 1 (symmetric)
  # Max possible direct edges = k_pos + k_neg. Symmetrization can increase this.
  # Example: Node 1 keeps edges (1,2), (1,3) positive, (1,8) negative. 
  # If node 2 keeps edge (2,1), it adds to symmetry. If node 3 keeps (3,9), it doesn't affect node 1 directly.
  # The number of non-zeros in row 1 should reflect edges *initiated* by node 1 plus edges *initiated* by others towards node 1.
  # It's hard to predict exact non-zero count without running full symmetrization.
  # Let's check a weaker condition: number of non-zeros <= V_p - 1
  nnz_row1 <- length(output_W@i[output_W@p[1]:(output_W@p[2]-1)])
  expect_lte(nnz_row1, V_p - 1)
  expect_gt(nnz_row1, 0) # Should have some connections unless data is pathological
})

# Test TCK-SGC-002: Edge Weight Correctness
test_that("TCK-SGC-002: compute_subject_connectivity_graph_sparse edge selection & sign", {
  skip_if_not_installed("Matrix")
  library(Matrix)
  
  set.seed(102)
  V_p <- 5
  T_i <- 100
  k_pos <- 1 # Keep only the strongest positive
  k_neg <- 1 # Keep only the strongest negative
  p_names <- paste0("P", 1:V_p)
  
  # Create data with specific correlations
  # P1 correlated with P2 (positive)
  # P1 anti-correlated with P3 (negative)
  # P1 uncorrelated with P4 
  # P5 is noise
  base_signal_pos <- rnorm(T_i)
  base_signal_neg <- rnorm(T_i) # Independent noise for neg corr
  X_ts <- matrix(rnorm(T_i * V_p, sd=0.1), ncol = V_p) # Base noise
  X_ts[, 1] <- base_signal_pos + rnorm(T_i, sd=0.2) # P1 
  X_ts[, 2] <- base_signal_pos + rnorm(T_i, sd=0.2) # P2 highly correlated with P1
  X_ts[, 3] <- -base_signal_neg + rnorm(T_i, sd=0.2) # P3 anti-correlated with P1 via base_signal_neg
  X_ts[, 1] <- X_ts[, 1] + base_signal_neg # Add base_signal_neg to P1 to create neg corr with P3
  # P4 remains noise, weakly correlated with P1
  # P5 remains noise
  colnames(X_ts) <- p_names

  # Calculate raw correlations for reference (esp. for P1)
  raw_corrs <- cor(X_ts)
  diag(raw_corrs) <- 0
  # Expected strongest positive for P1 should be P2
  # Expected strongest negative for P1 should be P3
  # Correlation P1-P4 should be small
  expect_gt(raw_corrs[1, 2], 0.5) # Should be high positive
  expect_lt(raw_corrs[1, 3], -0.5) # Should be high negative
  expect_lt(abs(raw_corrs[1, 4]), 0.3) # Should be low 

  # Function Call
  output_W <- compute_subject_connectivity_graph_sparse(
      X_subject = X_ts,
      parcel_names = p_names,
      k_conn_pos = k_pos,
      k_conn_neg = k_neg,
      use_dtw = FALSE
  )
  
  # Assertions
  # Check that the strong positive connection P1-P2 exists and has a positive z-score
  # Since it's symmetric, check both [1,2] and [2,1]
  # At least one direction should have selected it initially
  # We expect output_W[1,2] to be non-zero and positive after z-scoring
  # Note: z-score depends on all edges, so we check sign mainly
  expect_gt(output_W[1, 2], 0) 

  # Check that the strong negative connection P1-P3 exists and has a negative z-score
  expect_lt(output_W[1, 3], 0)

  # Check that P1 did NOT directly select P4 (the weak connection)
  # This means raw_corrs[1,4] was not among the top k_pos/k_neg for P1.
  raw_node1_corrs_others <- raw_corrs[1, -1] # P1's corrs with P2, P3, P4, P5 (indices 2,3,4,5 of X_ts)
  # We are interested if P4 (which is at original index 4, so index 3 in raw_node1_corrs_others) was selected.
  target_original_idx_for_P4 <- 4 

  if (k_pos > 0) {
    pos_candidates_P1_indices <- which(raw_node1_corrs_others > 0)
    if (length(pos_candidates_P1_indices) > 0) {
      pos_candidates_P1_vals <- raw_node1_corrs_others[pos_candidates_P1_indices]
      num_to_keep_pos_P1 <- min(k_pos, length(pos_candidates_P1_vals))
      # Get original column indices (2 to V_p) of those selected by P1 as positive
      selected_pos_by_P1_original_col_indices <- (which(raw_node1_corrs_others > 0))[order(pos_candidates_P1_vals, decreasing = TRUE)[1:num_to_keep_pos_P1]] + 1
      expect_false(target_original_idx_for_P4 %in% selected_pos_by_P1_original_col_indices, 
                   label = "P1 direct positive selection of P4 check")
    }
  }
  
  if (k_neg > 0) {
    neg_candidates_P1_indices <- which(raw_node1_corrs_others < 0)
    if (length(neg_candidates_P1_indices) > 0) {
      neg_candidates_P1_vals <- raw_node1_corrs_others[neg_candidates_P1_indices]
      num_to_keep_neg_P1 <- min(k_neg, length(neg_candidates_P1_vals))
      # Get original column indices (2 to V_p) of those selected by P1 as negative
      selected_neg_by_P1_original_col_indices <- (which(raw_node1_corrs_others < 0))[order(neg_candidates_P1_vals, decreasing = FALSE)[1:num_to_keep_neg_P1]] + 1
      expect_false(target_original_idx_for_P4 %in% selected_neg_by_P1_original_col_indices, 
                   label = "P1 direct negative selection of P4 check")
    }
  }

  # Check overall sparsity: With k_pos=1, k_neg=1, max direct edges per node is 2.
  # Symmetrization might increase this, but total nnz should be relatively small.
  # Example: If P1->P2, P1->P3 selected. If P2->P1, P2->P5 selected. If P3->P1, P3->P4 selected.
  # Symmetric non-zero pairs could be (1,2), (1,3), (2,5), (3,4).
  # Total non-zero entries (upper or lower triangle) should be relatively low.
  
  # Use tryCatch to handle potential NA result from nnzero
  nnz_output_W <- tryCatch(
    Matrix::nnzero(output_W), 
    error = function(e) 0, 
    warning = function(w) 0
  )
  
  if (!is.na(nnz_output_W)) {
    expect_lt(nnz_output_W, V_p * (k_pos + k_neg) * 2) # Loose upper bound
  }
})

# Test TCK-SGC-003: Handling Zero Variance Columns
test_that("TCK-SGC-003: compute_subject_connectivity_graph_sparse handles zero variance cols", {
  skip_if_not_installed("Matrix")
  library(Matrix)
  
  set.seed(103)
  V_p <- 6
  T_i <- 40
  k_pos <- 2
  k_neg <- 2
  p_names <- paste0("P", 1:V_p)
  
  # Create data where column 3 is constant
  X_ts <- matrix(rnorm(T_i * V_p), ncol = V_p)
  X_ts[, 3] <- 5 # Constant column
  colnames(X_ts) <- p_names

  # Call it once to get the actual output for subsequent checks
  # The message will be produced here, but we test for it in the expect_message block.
  output_W <- compute_subject_connectivity_graph_sparse(
      X_subject = X_ts,
      parcel_names = p_names,
      k_conn_pos = k_pos,
      k_conn_neg = k_neg,
      use_dtw = FALSE
  )

  # Assertions on output_W (obtained from the first call)
  expect_s4_class(output_W, "dgCMatrix")
  expect_equal(dim(output_W), c(V_p, V_p))

  # Check that row 3 and column 3 are all zeros
  expect_equal(nnzero(output_W[3, ]), 0) # Check row 3 non-zeros
  expect_equal(nnzero(output_W[, 3]), 0) # Check column 3 non-zeros

  # Double check using direct access (might be redundant but safe)
  # Extract column 3 structure from dgCMatrix
  col3_start_idx <- output_W@p[3]
  col3_end_idx <- output_W@p[3+1]
  col3_num_entries <- col3_end_idx - col3_start_idx
  expect_equal(col3_num_entries, 0)
  
  # Check row 3 - find which non-zero entries have row index 3 (should be none)
  row3_indices_in_x <- which(output_W@i == (3-1)) # 0-based index
  expect_length(row3_indices_in_x, 0)
})

# Test TCK-SGC-004: Symmetrization Logic
test_that("TCK-SGC-004: Symmetrization rule application", {
  skip_if_not_installed("Matrix")
  library(Matrix)
  
  # Create a small, non-symmetric directed graph manually
  # Cases:
  # (1,2): Edge exists only 1->2
  # (1,3): Edge exists only 3->1
  # (2,3): Edges exist in both directions with different values
  # (1,4): No edge in either direction (implicitly zero)
  i_idx <- c(1, 3, 2, 3) # Row indices (1-based)
  j_idx <- c(2, 1, 3, 2) # Col indices (1-based)
  x_val <- c(0.5, 0.8, 0.9, 0.6) # Values W_dir[i,j]
  W_dir <- sparseMatrix(i=i_idx, j=j_idx, x=x_val, dims=c(4,4), dimnames=list(paste0("N",1:4), paste0("N",1:4)))
  # W_dir:
  #       N1  N2  N3 N4
  #   N1   . 0.5   .  .
  #   N2   .   . 0.9  .
  #   N3 0.8 0.6   .  .
  #   N4   .   .   .  .

  # Apply the symmetrization logic used in compute_subject_connectivity_graph_sparse
  W_dir_t <- Matrix::t(W_dir)
  W_sum <- W_dir + W_dir_t
  W_indicator <- (W_dir != 0)
  W_indicator_t <- (W_dir_t != 0) # or Matrix::t(W_indicator)
  W_den <- W_indicator + W_indicator_t
  W_symmetric_raw <- W_sum / W_den
  W_symmetric_raw <- Matrix::drop0(W_symmetric_raw)

  # Assertions based on the rule
  # Case (1,2): Only W_dir[1,2]=0.5 exists. Sum=0.5. Indicator = 1+0 = 1. Den=1. Expected = 0.5/1 = 0.5
  expect_equal(W_symmetric_raw[1, 2], 0.5)
  expect_equal(W_symmetric_raw[2, 1], 0.5) # Should be symmetric

  # Case (1,3): Only W_dir[3,1]=0.8 exists. Sum=0.8. Indicator = 0+1 = 1. Den=1. Expected = 0.8/1 = 0.8
  expect_equal(W_symmetric_raw[1, 3], 0.8)
  expect_equal(W_symmetric_raw[3, 1], 0.8) # Should be symmetric

  # Case (2,3): W_dir[2,3]=0.9, W_dir[3,2]=0.6. Sum=1.5. Indicator = 1+1=2. Den=2. Expected = 1.5/2 = 0.75
  expect_equal(W_symmetric_raw[2, 3], 0.75)
  expect_equal(W_symmetric_raw[3, 2], 0.75) # Should be symmetric

  # Case (1,4): No edges. Sum=0. Indicator = 0+0=0. Den=0. Expected = 0/0 -> NaN.
  # drop0 does not remove NaNs.
  expect_true(is.nan(W_symmetric_raw[1, 4]))
  expect_true(is.nan(W_symmetric_raw[4, 1]))

  # Verify overall symmetry (numerically)
  # Note: isSymmetric might be false if NaNs are present and not handled symmetrically by the check itself.
  # For this test, we are checking the arithmetic. Symmetry of NaN handling is a separate concern for the main function.
  # If W_symmetric_raw contains NaNs, isSymmetric(W_symmetric_raw) might be NA or FALSE.
  # We can test symmetry on the non-NaN part if needed, or accept that NaNs break simple symmetry check here.
  # The main function *does* convert NaN to 0, then drops, then forcesymmetry.
  # Here we test the raw arithmetic result *before* that NaN cleanup.
  # A matrix with NaN is symmetric if M[i,j] and M[j,i] are both NaN or both equal.
  expect_true(Matrix::isSymmetric(W_symmetric_raw, tol=1e-8, na.rm=FALSE)) # Check symmetry considering NaNs as comparable
})

# Test TCK-SGC-005: Correctness of alpha-lazy random-walk Laplacian
test_that("TCK-SGC-005: compute_graph_laplacian_sparse calculates L_rw_lazy_sym correctly", {
  skip_if_not_installed("Matrix")
  library(Matrix)
  
  set.seed(105)
  alpha <- 0.93
  
  # Define upper triangle for the desired symmetric matrix:
  # W_sparse:
  #      [,1] [,2] [,3] [,4]
  # [1,]  .   0.5  0.2   .
  # [2,] 0.5   .    .   0.7
  # [3,] 0.2   .    .    .
  # [4,]  .   0.7   .    .
  i_upper <- c(1,   1,   2) 
  j_upper <- c(2,   3,   4) 
  x_upper <- c(0.5, 0.2, 0.7)
  
  W_tri <- sparseMatrix(i=i_upper, j=j_upper, x=x_upper, dims=c(4,4))
  W_sparse <- W_tri + Matrix::t(W_tri) # Construct symmetric matrix
  W_sparse <- as(W_sparse, "dgCMatrix") # Ensure it's dgCMatrix for consistency

  # Manual Calculation
  V_p <- nrow(W_sparse)
  degree_vec <- Matrix::rowSums(abs(W_sparse)) # Using abs as in the function
  inv_degree_vec <- ifelse(degree_vec == 0, 0, 1 / degree_vec)
  D_inv_sparse <- Matrix::Diagonal(n = V_p, x = inv_degree_vec)
  I_mat <- Matrix::Diagonal(n = V_p)
  
  L_rw_lazy_manual <- I_mat - alpha * (D_inv_sparse %*% W_sparse)
  # Symmetrize manually
  L_rw_lazy_sym_manual <- (L_rw_lazy_manual + Matrix::t(L_rw_lazy_manual)) / 2
  L_rw_lazy_sym_manual <- Matrix::drop0(L_rw_lazy_sym_manual)
  
  # Function Call
  output_L <- compute_graph_laplacian_sparse(W_sparse, alpha = alpha)

  # Assertions
  # Use all.equal for sparse matrices or convert to dense if small
  expect_true(all.equal(output_L, L_rw_lazy_sym_manual, tolerance = 1e-8))
  
  # Check properties
  expect_s4_class(output_L, "dgCMatrix") # Should be sparse
  expect_true(Matrix::isSymmetric(output_L, tol = 1e-8))
})

# Test TCK-SGC-006: Handling Zero Degree Nodes
test_that("TCK-SGC-006: compute_graph_laplacian_sparse handles zero degree nodes", {
  skip_if_not_installed("Matrix")
  library(Matrix)
  
  set.seed(106)
  alpha <- 0.8 # Use a different alpha for variation
  
  # Create a W matrix where node 3 is isolated
  # W_sparse:
  #      [,1] [,2] [,3] [,4]
  # [1,]  .   0.5  .   0.1
  # [2,] 0.5   .    .    .
  # [3,]  .    .    .    .
  # [4,] 0.1   .    .    .
  i_upper <- c(1, 1) 
  j_upper <- c(2, 4) 
  x_upper <- c(0.5, 0.1)
  
  W_tri <- sparseMatrix(i=i_upper, j=j_upper, x=x_upper, dims=c(4,4))
  W_sparse <- W_tri + Matrix::t(W_tri) # Construct symmetric matrix
  W_sparse <- as(W_sparse, "dgCMatrix") # Ensure it's dgCMatrix

  # Function Call
  output_L <- NULL
  expect_no_error(
    output_L <- compute_graph_laplacian_sparse(W_sparse, alpha = alpha)
  )
  
  # Assertions
  expect_s4_class(output_L, "dgCMatrix")
  expect_equal(dim(output_L), c(4, 4))

  # Check properties for the isolated node (node 3)
  # For L_rw_lazy_sym = (I - alpha*Dinv*W + t(I - alpha*Dinv*W))/2
  # If degree(3)=0, then Dinv[3,3]=0.
  # Row 3 of Dinv*W is zero.
  # Col 3 of Dinv*W is zero.
  # So Row 3 of L = I - alpha*Dinv*W is [0, 0, 1, 0]
  # Col 3 of L is [0, 0, 1, 0]^T
  # Symmetrizing keeps this structure.
  expect_equal(output_L[3, 3], 1.0) # L[i,i] should be 1 for isolated node
  
  # Check other elements in row 3 and column 3 are zero
  expect_equal(output_L[3, -3], c(0, 0, 0)) # Row 3, excluding diagonal
  expect_equal(output_L[-3, 3], c(0, 0, 0)) # Col 3, excluding diagonal
})

# Test TCK-SGC-007: compute_spectral_sketch_sparse - Basic Correctness & Dimensions
test_that("TCK-SGC-007: compute_spectral_sketch_sparse basic correctness & dimensions", {
  skip_if_not_installed("Matrix")
  skip_if_not_installed("RSpectra") # Function uses RSpectra::eigs_sym
  library(Matrix)
  
  set.seed(107)
  V_p <- 6
  k_target <- 2 # Request 2 informative eigenvectors
  
  # Create a small, sparse, symmetric matrix (not necessarily a valid Laplacian)
  # Ensure it has some distinct eigenvalues
  i_upper <- c(1, 1, 2, 2, 3, 4, 5)
  j_upper <- c(1, 2, 2, 3, 4, 5, 6) # Include diagonal elements
  x_upper <- c(2, 0.5, 3, -0.1, 1.5, 0.8, 2.5)
  L_tri <- sparseMatrix(i=i_upper, j=j_upper, x=x_upper, dims=c(V_p, V_p))
  L_sparse <- L_tri + t(L_tri)
  # Correct the diagonal after symmetrization (it gets doubled)
  Matrix::diag(L_sparse) <- Matrix::diag(L_sparse) / 2 
  L_sparse <- as(L_sparse, "dgCMatrix") # Ensure type before eigen

  # Calculate ground truth using base::eigen on the dense matrix
  eigen_gt <- eigen(as.matrix(L_sparse), symmetric = TRUE)
  
  # Sort ground truth by eigenvalue (ascending, like compute_spectral_sketch_sparse)
  sorted_order_gt <- order(eigen_gt$values)
  eigen_vals_gt_sorted <- eigen_gt$values[sorted_order_gt]
  eigen_vecs_gt_sorted <- eigen_gt$vectors[, sorted_order_gt, drop = FALSE]
  
  # Identify the first k_target *non-trivial* ground truth components
  # (Assuming the first one might be trivial/near-zero, though not guaranteed for this arbitrary matrix)
  # Let's assume compute_spectral_sketch_sparse handles the trivial filtering correctly (tested in SGC-008)
  # Here, we compare against the first k_target smallest eigenvalues/vectors directly from sorted ground truth.
  # Need to be careful if L_sparse has eigenvalue ~0 as first.
  # compute_spectral_sketch_sparse filters eigenvalues > tol (e.g., 1e-8)
  eigenvalue_tol <- 1e-8
  non_trivial_gt_indices <- which(eigen_vals_gt_sorted > eigenvalue_tol)
  if (length(non_trivial_gt_indices) < k_target) {
      skip("Ground truth matrix doesn't have enough non-trivial eigenvalues for k_target.")
  }
  gt_indices_to_compare <- non_trivial_gt_indices[1:k_target]
  U_gt_k <- eigen_vecs_gt_sorted[, gt_indices_to_compare, drop=FALSE]
  Lambda_gt_k <- eigen_vals_gt_sorted[gt_indices_to_compare]

  # Function Call
  result <- NULL
  expect_no_error(
    result <- compute_spectral_sketch_sparse(L_sparse, k = k_target)
  )
  
  # Assertions
  expect_true(is.list(result))
  expect_named(result, c("vectors", "values"))
  expect_true(is.matrix(result$vectors))
  expect_true(is.numeric(result$values))
  expect_equal(nrow(result$vectors), V_p)
  expect_equal(ncol(result$vectors), k_target)
  expect_length(result$values, k_target)
  
  # Compare eigenvalues (should be close)
  expect_equal(result$values, Lambda_gt_k, tolerance = 1e-6)
  
  # Compare eigenvectors (allowing for sign flips)
  U_result_k <- result$vectors
  # Align signs for comparison: Flip columns in U_result_k if their correlation with U_gt_k is negative
  for (j in 1:k_target) {
    # Check correlation - handle potential zero vectors if L was pathological
    cor_val <- tryCatch(cor(U_result_k[, j], U_gt_k[, j]), error = function(e) 0)
    if (!is.na(cor_val) && cor_val < 0) {
      U_result_k[, j] <- -U_result_k[, j]
    }
  }
  # Now compare sign-aligned vectors
  expect_equal(U_result_k, U_gt_k, tolerance = 1e-6)
  
  # Alternative check: High absolute correlation for each component
  # correlations <- diag(abs(cor(result$vectors, U_gt_k)))
  # expect_true(all(correlations > 0.999), label = "Absolute correlation check")
})

# Test TCK-SGC-008: compute_spectral_sketch_sparse - Trivial Eigenvector Handling
test_that("TCK-SGC-008: compute_spectral_sketch_sparse handles trivial eigenvectors", {
  skip_if_not_installed("Matrix")
  skip_if_not_installed("RSpectra")
  library(Matrix)
  
  V_p <- 5 # Size of the graph
  k_target <- 2 # Request 2 informative eigenvectors
  
  # Create Laplacian for a simple connected graph (path graph 1-2-3-4-5)
  # Expected eigenvalues: 0, and 4 non-zero smallest ones.
  adj <- bandSparse(V_p, k = 1, diagonals = list(rep(1, V_p-1)), symmetric = TRUE)
  D <- Diagonal(V_p, x = Matrix::rowSums(adj))
  L_path <- D - adj
  L_path <- as(L_path, "dgCMatrix")

  # Function Call
  result <- NULL
  expect_no_error(
    result <- compute_spectral_sketch_sparse(L_path, k = k_target)
  )
  
  # Assertions
  expect_true(is.list(result))
  expect_named(result, c("vectors", "values"))
  expect_equal(ncol(result$vectors), k_target) # Should return exactly k vectors
  expect_length(result$values, k_target)     # Should return exactly k values

  # Check that the smallest returned eigenvalue is strictly greater than the internal tolerance
  eigenvalue_tol <- 1e-8 # Match the default used in the function
  expect_gt(min(result$values), eigenvalue_tol, 
            label="Smallest returned eigenvalue should be > tolerance")
            
  # Optional: Verify against known eigenvalues for path graph if needed
  # evals_analytic <- 2 * (1 - cos(pi * (0:(V_p-1)) / V_p))
  # expect_equal(sort(result$values), sort(evals_analytic)[2:(k_target+1)], tolerance=1e-6)
})

# Test TCK-SGC-009: compute_spectral_sketch_sparse - Rank Deficiency / Insufficient Eigenvectors
test_that("TCK-SGC-009: compute_spectral_sketch_sparse handles rank deficiency", {
  skip_if_not_installed("Matrix")
  skip_if_not_installed("RSpectra")
  library(Matrix)
  
  # Create a graph with 2 connected components (e.g., two path graphs)
  V_p1 <- 4
  adj1 <- bandSparse(V_p1, k = 1, diagonals = list(rep(1, V_p1-1)), symmetric = TRUE)
  D1 <- Diagonal(V_p1, x = Matrix::rowSums(adj1))
  L1 <- D1 - adj1
  
  V_p2 <- 3
  adj2 <- bandSparse(V_p2, k = 1, diagonals = list(rep(1, V_p2-1)), symmetric = TRUE)
  D2 <- Diagonal(V_p2, x = Matrix::rowSums(adj2))
  L2 <- D2 - adj2

  # Combine into a block diagonal matrix
  L_disconnected <- Matrix::bdiag(L1, L2)
  L_disconnected <- as(L_disconnected, "dgCMatrix")
  V_p_total <- nrow(L_disconnected) # Should be V_p1 + V_p2 = 7
  
  # This graph has 2 zero eigenvalues (one for each component).
  # Total number of eigenvalues is V_p_total = 7.
  # Number of non-zero eigenvalues = V_p_total - 2 = 5.
  num_non_zero_eigenvals <- V_p_total - 2 
  
  # Request k > number of non-zero eigenvalues
  k_target <- num_non_zero_eigenvals + 1 # Request 6 informative eigenvectors

  # Expect an error because we can only find 5 informative ones.
  expect_error(
    compute_spectral_sketch_sparse(L_disconnected, k = k_target),
    regexp = "Rank deficiency|too many zero eigenvalues|Found only.*informative eigenvectors.*but k=.*was requested" # Match potential error messages
  )
  
  # Also test the boundary case: requesting exactly the number of informative eigenvalues
  # In this specific case (Vp=7, k=5 requesting 6 eigs), eigs_sym only returns 4 informative ones.
  # So, the function should still error because 4 < 5.
  k_target_boundary = num_non_zero_eigenvals # Request 5
  expect_error(
    compute_spectral_sketch_sparse(L_disconnected, k = k_target_boundary),
    regexp = "Rank deficiency|too many zero eigenvalues|Found only.*informative eigenvectors.*but k=.*was requested",
    info=paste("Boundary case k=", k_target_boundary, "should error if eigs_sym doesn't return enough.")
  )
})

# Test TCK-SGC-010: compute_spectral_sketch_sparse - Edge Cases k=0, k=1
test_that("TCK-SGC-010: compute_spectral_sketch_sparse edge cases k=0, k=1", {
  skip_if_not_installed("Matrix")
  skip_if_not_installed("RSpectra")
  library(Matrix)
  
  # Use the same path graph Laplacian from TCK-SGC-008
  V_p <- 5
  adj <- bandSparse(V_p, k = 1, diagonals = list(rep(1, V_p-1)), symmetric = TRUE)
  D <- Diagonal(V_p, x = Matrix::rowSums(adj))
  L_path <- D - adj
  L_path <- as(L_path, "dgCMatrix")

  # --- Test k = 0 --- 
  result_k0 <- NULL
  expect_no_error(
    result_k0 <- compute_spectral_sketch_sparse(L_path, k = 0)
  )
  
  # Assertions for k=0
  expect_true(is.list(result_k0))
  expect_named(result_k0, c("vectors", "values"))
  expect_true(is.matrix(result_k0$vectors))
  expect_true(is.numeric(result_k0$values))
  expect_equal(nrow(result_k0$vectors), V_p) # Should still have V_p rows
  expect_equal(ncol(result_k0$vectors), 0)
  expect_length(result_k0$values, 0)

  # --- Test k = 1 --- 
  k_target_1 <- 1
  result_k1 <- NULL
  expect_no_error(
    result_k1 <- compute_spectral_sketch_sparse(L_path, k = k_target_1)
  )
  
  # Assertions for k=1
  expect_true(is.list(result_k1))
  expect_named(result_k1, c("vectors", "values"))
  expect_true(is.matrix(result_k1$vectors))
  expect_true(is.numeric(result_k1$values))
  expect_equal(nrow(result_k1$vectors), V_p)
  expect_equal(ncol(result_k1$vectors), k_target_1)
  expect_length(result_k1$values, k_target_1)
  
  # Check eigenvalue is positive (smallest non-trivial)
  eigenvalue_tol <- 1e-8
  expect_gt(result_k1$values[1], eigenvalue_tol)
  
})
})
</file>

<file path="tests/testthat/test-task_graph_construction.R">
# tests/testthat/test-task_graph_construction.R

library(testthat)
library(Matrix)
# Assuming 'hatsa' package functions are available during testing, or use devtools::load_all()

describe("compute_W_task_from_activations", {
  V_p <- 10 # Number of parcels
  C <- 5    # Number of conditions/features
  parcel_names <- paste0("P", 1:V_p)

  # Helper to check for z-scoring (approximate)
  is_z_scored <- function(x) {
    if (length(x) < 2) return(TRUE) # Cannot assess sd for < 2 values
    abs(mean(x)) < 1e-9 && abs(stats::sd(x) - 1) < 1e-9
  }

  it("produces a valid sparse graph with pearson correlation", {
    activation_matrix <- matrix(rnorm(C * V_p), nrow = C, ncol = V_p)
    colnames(activation_matrix) <- parcel_names
    
    W_task <- compute_W_task_from_activations(
      activation_matrix = activation_matrix,
      parcel_names = parcel_names,
      k_conn_task_pos = 2,
      k_conn_task_neg = 1,
      similarity_method = "pearson"
    )
    
    expect_s4_class(W_task, "dgCMatrix")
    expect_equal(dim(W_task), c(V_p, V_p))
    expect_equal(rownames(W_task), parcel_names)
    expect_equal(colnames(W_task), parcel_names)
    expect_true(Matrix::isSymmetric(W_task))
    if (length(W_task@x) > 1) { # Only check z-score if there are edges
      # This is an approximate check as k-NN and symmetrization affect distribution
      # expect_true(is_z_scored(W_task@x)) 
      # A looser check might be that values are not wildly out of z-score range
      expect_true(all(abs(W_task@x) < 5)) # Arbitrary sanity check for z-scored values
    }
  })

  it("produces a valid sparse graph with spearman correlation", {
    activation_matrix <- matrix(rnorm(C * V_p), nrow = C, ncol = V_p)
    colnames(activation_matrix) <- parcel_names
    
    W_task <- compute_W_task_from_activations(
      activation_matrix = activation_matrix,
      parcel_names = parcel_names,
      k_conn_task_pos = 2,
      k_conn_task_neg = 1,
      similarity_method = "spearman"
    )
    
    expect_s4_class(W_task, "dgCMatrix")
    expect_equal(dim(W_task), c(V_p, V_p))
    # ... other common checks ...
    expect_true(Matrix::isSymmetric(W_task))
  })
  
  it("handles custom similarity function", {
    activation_matrix <- matrix(rnorm(C * V_p), nrow = C, ncol = V_p)
    custom_sim_func <- function(act_mat) {
      # Example: simple cosine similarity (not as robust as cor for this structure)
      # For testing, ensure it returns V_p x V_p
      m <- t(act_mat) # V_p x C
      sim <- m %*% t(m) / (sqrt(rowSums(m^2)) %*% t(sqrt(rowSums(m^2))))
      diag(sim) <- 0 # Ensure diagonal is 0 before kNN
      return(sim)
    }
    
    W_task <- compute_W_task_from_activations(
      activation_matrix = activation_matrix,
      parcel_names = parcel_names,
      k_conn_task_pos = 1,
      k_conn_task_neg = 1,
      similarity_method = custom_sim_func
    )
    expect_s4_class(W_task, "dgCMatrix")
    expect_equal(dim(W_task), c(V_p, V_p))
    expect_true(Matrix::isSymmetric(W_task))
  })
  
  it("handles zero-variance columns in activation_matrix", {
    activation_matrix <- matrix(rnorm(C * V_p), nrow = C, ncol = V_p)
    activation_matrix[, 1] <- rep(1, C) # First parcel has zero variance
    colnames(activation_matrix) <- parcel_names
    
    # Expect the message from our function, suppress potential underlying cor() warning for this specific test
    expect_message( 
      suppressWarnings(W_task <- compute_W_task_from_activations(
        activation_matrix = activation_matrix,
        parcel_names = parcel_names,
        k_conn_task_pos = 2,
        k_conn_task_neg = 1,
        similarity_method = "pearson"
      )),
      regexp = "Found 1 parcel\\(s\\) with zero variance"
    )
    expect_s4_class(W_task, "dgCMatrix")
    # Further checks could be added here if W_task is not NA/NaN
  })
  
  it("handles k_conn_pos=0 and k_conn_task_neg=0", {
    activation_matrix <- matrix(rnorm(C * V_p), nrow = C, ncol = V_p)
    colnames(activation_matrix) <- parcel_names
    W_task <- compute_W_task_from_activations(
      activation_matrix = activation_matrix,
      parcel_names = parcel_names,
      k_conn_task_pos = 0,
      k_conn_task_neg = 0,
      similarity_method = "pearson"
    )
    expect_s4_class(W_task, "dgCMatrix")
    expect_equal(dim(W_task), c(V_p, V_p))
    expect_equal(length(W_task@x), 0) # Should be an empty graph (no non-zero elements)
  })
  
  it("handles input with 0 parcels (V_p=0)", {
    activation_matrix_empty_vp <- matrix(nrow = C, ncol = 0)
    W_task <- compute_W_task_from_activations(
        activation_matrix = activation_matrix_empty_vp,
        parcel_names = character(0),
        k_conn_task_pos = 1, k_conn_task_neg = 1
    )
    expect_s4_class(W_task, "dgCMatrix")
    expect_equal(dim(W_task), c(0,0))
  })
  
  it("handles input with 0 conditions (C=0)", {
    activation_matrix_empty_c <- matrix(nrow = 0, ncol = V_p)
    colnames(activation_matrix_empty_c) <- parcel_names
    
    expect_warning(
      W_task_c0 <- compute_W_task_from_activations(
          activation_matrix = activation_matrix_empty_c,
          parcel_names = parcel_names,
          k_conn_task_pos = 1, k_conn_task_neg = 1
      ),
      regexp = "activation_matrix has 0 row\\(s\\)"
    )
    expect_s4_class(W_task_c0, "dgCMatrix")
    expect_equal(dim(W_task_c0), c(V_p,V_p))
    expect_equal(length(W_task_c0@x), 0)

    activation_matrix_c1 <- matrix(rnorm(V_p), nrow = 1, ncol = V_p)
    colnames(activation_matrix_c1) <- parcel_names
    expect_warning(
      W_task_c1 <- compute_W_task_from_activations(
          activation_matrix = activation_matrix_c1,
          parcel_names = parcel_names,
          k_conn_task_pos = 1, k_conn_task_neg = 1
      ),
      regexp = "activation_matrix has 1 row\\(s\\)"
    )
    expect_s4_class(W_task_c1, "dgCMatrix")
    expect_equal(dim(W_task_c1), c(V_p,V_p))
    expect_equal(length(W_task_c1@x), 0) 
  })

}) # end describe compute_W_task_from_activations


describe("compute_W_task_from_encoding", {
  V_p <- 10 # Number of parcels
  N_features <- 5 # Number of encoding features
  parcel_names <- paste0("P", 1:V_p)

  is_z_scored <- function(x) { # Helper from above, can be defined globally in test file
    if (length(x) < 2) return(TRUE)
    abs(mean(x)) < 1e-9 && abs(stats::sd(x) - 1) < 1e-9
  }

  it("produces a valid sparse graph with pearson correlation", {
    encoding_matrix <- matrix(rnorm(V_p * N_features), nrow = V_p, ncol = N_features)
    rownames(encoding_matrix) <- parcel_names

    W_task <- compute_W_task_from_encoding(
      encoding_weights_matrix = encoding_matrix,
      parcel_names = parcel_names,
      k_conn_task_pos = 2,
      k_conn_task_neg = 1,
      similarity_method = "pearson"
    )

    expect_s4_class(W_task, "dgCMatrix")
    expect_equal(dim(W_task), c(V_p, V_p))
    expect_equal(rownames(W_task), parcel_names)
    expect_equal(colnames(W_task), parcel_names)
    expect_true(Matrix::isSymmetric(W_task))
    if (length(W_task@x) > 1) {
        expect_true(all(abs(W_task@x) < 5)) # Sanity check for z-scored values
    }
  })
  
  it("produces a valid sparse graph with spearman correlation", {
    encoding_matrix <- matrix(runif(V_p * N_features), nrow = V_p, ncol = N_features) # Use runif for spearman
    rownames(encoding_matrix) <- parcel_names
    W_task <- compute_W_task_from_encoding(
      encoding_weights_matrix = encoding_matrix,
      parcel_names = parcel_names,
      k_conn_task_pos = 2,
      k_conn_task_neg = 1,
      similarity_method = "spearman"
    )
    expect_s4_class(W_task, "dgCMatrix")
    expect_equal(dim(W_task), c(V_p, V_p))
    expect_true(Matrix::isSymmetric(W_task))
  })
  
  it("handles custom similarity function for encoding weights", {
    encoding_matrix <- matrix(rnorm(V_p * N_features), nrow = V_p, ncol = N_features)
    custom_sim_func_encoding <- function(enc_mat) { # V_p x N_features
      # Example: simple dot product similarity between parcel encoding profiles
      sim <- enc_mat %*% t(enc_mat) 
      diag(sim) <- 0
      return(sim) # Should return V_p x V_p
    }
    
    W_task <- compute_W_task_from_encoding(
      encoding_weights_matrix = encoding_matrix,
      parcel_names = parcel_names,
      k_conn_task_pos = 1,
      k_conn_task_neg = 1,
      similarity_method = custom_sim_func_encoding
    )
    expect_s4_class(W_task, "dgCMatrix")
    expect_equal(dim(W_task), c(V_p, V_p))
    expect_true(Matrix::isSymmetric(W_task))
  })
  
  it("handles zero-variance rows in encoding_matrix", {
    encoding_matrix <- matrix(rnorm(V_p * N_features), nrow = V_p, ncol = N_features)
    encoding_matrix[1, ] <- rep(0.5, N_features) # First parcel has zero variance
    rownames(encoding_matrix) <- parcel_names
    
    expect_message(
      suppressWarnings(W_task <- compute_W_task_from_encoding(
        encoding_weights_matrix = encoding_matrix,
        parcel_names = parcel_names,
        k_conn_task_pos = 2,
        k_conn_task_neg = 1,
        similarity_method = "pearson"
      )),
      regexp = "Found 1 parcel\\(s\\) with zero variance"
    )
    expect_s4_class(W_task, "dgCMatrix")
  })
  
  it("handles k_conn_pos=0 and k_conn_neg=0 for encoding", {
    encoding_matrix <- matrix(rnorm(V_p * N_features), nrow = V_p, ncol = N_features)
    rownames(encoding_matrix) <- parcel_names
    W_task <- compute_W_task_from_encoding(
      encoding_weights_matrix = encoding_matrix,
      parcel_names = parcel_names,
      k_conn_task_pos = 0,
      k_conn_task_neg = 0,
      similarity_method = "pearson"
    )
    expect_s4_class(W_task, "dgCMatrix")
    expect_equal(dim(W_task), c(V_p, V_p))
    expect_equal(length(W_task@x), 0) 
  })
  
  it("handles input with 0 parcels (V_p=0) for encoding", {
    encoding_matrix_empty_vp <- matrix(nrow = 0, ncol = N_features)
    W_task <- compute_W_task_from_encoding(
        encoding_weights_matrix = encoding_matrix_empty_vp,
        parcel_names = character(0),
        k_conn_task_pos = 1, k_conn_task_neg = 1
    )
    expect_s4_class(W_task, "dgCMatrix")
    expect_equal(dim(W_task), c(0,0))
  })
  
  it("handles input with 0 features (N_features=0) for encoding", {
    encoding_matrix_empty_nf <- matrix(nrow = V_p, ncol = 0)
    rownames(encoding_matrix_empty_nf) <- parcel_names
    
    expect_warning(
      W_task <- compute_W_task_from_encoding(
          encoding_weights_matrix = encoding_matrix_empty_nf,
          parcel_names = parcel_names,
          k_conn_task_pos = 1, k_conn_task_neg = 1
      ),
      regexp = "encoding_weights_matrix has zero features"
    )
    expect_s4_class(W_task, "dgCMatrix")
    expect_equal(dim(W_task), c(V_p,V_p))
    expect_equal(length(W_task@x), 0) # Graph should be empty
  })

}) # end describe compute_W_task_from_encoding
</file>

<file path="tests/testthat/test-task_hatsa_graphs.R">
# test-task_hatsa_graphs.R

library(testthat)
library(Matrix)
skip_on_cran()

source("../../R/task_hatsa.R")

context("compute_W_task_from_activations and related graph construction")

test_that("compute_W_task_from_activations returns correct type and dimensions", {
  set.seed(1)
  C <- 5; Vp <- 4
  mat <- matrix(rnorm(C*Vp), nrow=C, ncol=Vp)
  pnames <- paste0("P", 1:Vp)
  W <- compute_W_task_from_activations(mat, pnames, 2, 2)
  expect_s4_class(W, "dgCMatrix")
  expect_equal(dim(W), c(Vp, Vp))
  expect_true(Matrix::isSymmetric(W))
})

test_that("compute_W_task_from_activations handles zero-variance columns", {
  mat <- matrix(1, nrow=5, ncol=4)
  pnames <- paste0("P", 1:4)
  expect_warning(W <- compute_W_task_from_activations(mat, pnames, 2, 2), NA)
  expect_s4_class(W, "dgCMatrix")
})

test_that("compute_W_task_from_activations handles C=1 and C=0", {
  mat1 <- matrix(rnorm(4), nrow=1)
  mat0 <- matrix(numeric(0), nrow=0, ncol=4)
  pnames <- paste0("P", 1:4)
  W1 <- compute_W_task_from_activations(mat1, pnames, 2, 2)
  W0 <- compute_W_task_from_activations(mat0, pnames, 2, 2)
  expect_equal(dim(W1), c(4,4))
  expect_equal(dim(W0), c(4,4))
  expect_true(all(W1 == 0))
  expect_true(all(W0 == 0))
})

test_that("compute_W_task_from_activations output is symmetric and sparse", {
  mat <- matrix(rnorm(20), nrow=5)
  pnames <- paste0("P", 1:4)
  W <- compute_W_task_from_activations(mat[,1:4], pnames, 2, 2)
  expect_true(Matrix::isSymmetric(W))
  expect_true(inherits(W, "sparseMatrix"))
})

test_that("compute_W_task_from_activations z-scores nonzero edges", {
  mat <- matrix(rnorm(20), nrow=5)
  pnames <- paste0("P", 1:4)
  W <- compute_W_task_from_activations(mat[,1:4], pnames, 2, 2)
  if (length(W@x) > 0) {
    expect_equal(mean(W@x), 0, tolerance=1e-8)
    expect_equal(sd(W@x), 1, tolerance=0.1)
  }
})

test_that("compute_W_task_from_activations supports custom similarity_method", {
  mat <- matrix(rnorm(20), nrow=5)
  pnames <- paste0("P", 1:4)
  simfun <- function(x) matrix(1, ncol(x), ncol(x))
  W <- compute_W_task_from_activations(mat[,1:4], pnames, 2, 2, similarity_method=simfun)
  expect_s4_class(W, "dgCMatrix")
})

test_that("compute_W_task_from_activations handles all-zero and all-NA input", {
  mat0 <- matrix(0, nrow=5, ncol=4)
  matNA <- matrix(NA_real_, nrow=5, ncol=4)
  pnames <- paste0("P", 1:4)
  W0 <- compute_W_task_from_activations(mat0, pnames, 2, 2)
  WNA <- compute_W_task_from_activations(matNA, pnames, 2, 2)
  expect_true(all(W0 == 0))
  expect_true(all(WNA == 0))
})

test_that("compute_W_task_from_activations errors on mismatched parcel_names", {
  mat <- matrix(rnorm(20), nrow=5)
  expect_error(compute_W_task_from_activations(mat[,1:4], paste0("P", 1:3), 2, 2))
})

# If compute_W_task_from_encoding is present, add a basic test
if (exists("compute_W_task_from_encoding", mode="function")) {
  test_that("compute_W_task_from_encoding returns correct type and dimensions", {
    mat <- matrix(rnorm(20), nrow=4, ncol=5)
    pnames <- paste0("P", 1:4)
    W <- compute_W_task_from_encoding(mat, pnames, 2, 2)
    expect_s4_class(W, "dgCMatrix")
    expect_equal(dim(W), c(4,4))
    expect_true(Matrix::isSymmetric(W))
  })
}
</file>

<file path="tests/testthat/test-task_hatsa_integration.R">
# Test for end-to-end functionality of run_task_hatsa
# THFIX-007

context("Integration Test for run_task_hatsa")

test_that("run_task_hatsa with core_hatsa method runs and produces valid output (sequential)", {
  # Ensure future plan is sequential for this test block for deterministic behavior
  # If future is used internally by run_task_hatsa, it should respect this.
  # If run_task_hatsa itself sets a plan, this might be overridden.
  # For now, assume we control the top-level plan or run_task_hatsa accepts it.
  if (requireNamespace("future", quietly = TRUE)) {
    old_plan <- future::plan(future::sequential)
    on.exit(future::plan(old_plan), add = TRUE)
  }

  # 1. Synthetic Data Generation
  N_subjects_test <- 3
  V_p_test <- 10 # Number of parcels
  T_subj_test <- 50 # Time points per subject
  k_test <- 3 # Spectral rank
  num_anchors_test <- 5

  subject_data_list_test <- vector("list", N_subjects_test)
  parcel_names_test <- paste0("P", 1:V_p_test)
  for (i in 1:N_subjects_test) {
    mat <- matrix(rnorm(T_subj_test * V_p_test), nrow = T_subj_test, ncol = V_p_test)
    colnames(mat) <- parcel_names_test
    subject_data_list_test[[i]] <- mat
  }

  # 2. Parameters for run_task_hatsa
  anchor_indices_test <- 1:num_anchors_test
  
  # Ensure all necessary parameters for run_task_hatsa are provided.
  # It's assumed run_task_hatsa is available in the environment (e.g., via devtools::load_all())
  
  # We'll call expect_no_error to catch any errors during the run
  result_core_hatsa <- NULL
  expect_no_error({
    result_core_hatsa <- run_task_hatsa(
      subject_data_list = subject_data_list_test,
      task_data_list = NULL, # For core_hatsa method
      anchor_indices = anchor_indices_test,
      spectral_rank_k = k_test,
      k_conn_pos = 3,
      k_conn_neg = 3,
      n_refine = 2,
      task_method = "core_hatsa", # Single string (will be matched against c("lambda_blend", "gev_patch", "core_hatsa"))
      lambda_blend_value = 0.5, # Default, not used by core_hatsa
      row_augmentation = FALSE, # Explicitly disable row augmentation when task_data_list is NULL
      omega_mode = "fixed", # Single string (will be matched against c("fixed", "adaptive"))
      verbose = FALSE # Keep tests quiet
    )
  })

  # 3. Assertions
  expect_true(!is.null(result_core_hatsa), "run_task_hatsa should return a non-NULL object.")
  expect_s3_class(result_core_hatsa, "task_hatsa_projector")
  expect_s3_class(result_core_hatsa, "hatsa_projector") # Inherits

  # Check parameters stored in the object
  expect_equal(result_core_hatsa$parameters$k, k_test)
  expect_equal(result_core_hatsa$parameters$N_subjects, N_subjects_test)
  expect_equal(result_core_hatsa$parameters$V_p, V_p_test)
  expect_equal(length(result_core_hatsa$parameters$anchor_indices), num_anchors_test)
  expect_equal(result_core_hatsa$parameters$task_method, "core_hatsa")

  # Check dimensions of key components
  # $v: group-level template (V_p x k)
  expect_true(is.matrix(result_core_hatsa$v), "result$v should be a matrix")
  expect_equal(dim(result_core_hatsa$v), c(V_p_test, k_test))

  # $s: stacked aligned sketches ((N_subjects * V_p) x k)
  expect_true(is.matrix(result_core_hatsa$s), "result$s should be a matrix")
  expect_equal(dim(result_core_hatsa$s), c(N_subjects_test * V_p_test, k_test))
  expect_false(any(is.na(result_core_hatsa$s)), "Stacked scores 's' should not contain NAs for core_hatsa with valid inputs.")


  # $R_final_list: list of rotation matrices (k x k)
  expect_true(is.list(result_core_hatsa$R_final_list), "result$R_final_list should be a list")
  expect_equal(length(result_core_hatsa$R_final_list), N_subjects_test)
  for (i in 1:N_subjects_test) {
    expect_true(is.matrix(result_core_hatsa$R_final_list[[i]]), paste("R_final_list[[ D ]]", i, "is not a matrix"))
    expect_equal(dim(result_core_hatsa$R_final_list[[i]]), c(k_test, k_test))
  }

  # $U_original_list: list of original sketches (V_p x k)
  expect_true(is.list(result_core_hatsa$U_original_list), "result$U_original_list should be a list")
  expect_equal(length(result_core_hatsa$U_original_list), N_subjects_test)
  for (i in 1:N_subjects_test) {
    expect_true(is.matrix(result_core_hatsa$U_original_list[[i]]), paste("U_original_list[[ D ]]",i ,"is not a matrix"))
    expect_equal(dim(result_core_hatsa$U_original_list[[i]]), c(V_p_test, k_test))
  }
  
  # $U_aligned_list (stored in task_hatsa_projector from task_hatsa_helpers)
  expect_true(is.list(result_core_hatsa$U_aligned_list), "result$U_aligned_list should be a list")
  expect_equal(length(result_core_hatsa$U_aligned_list), N_subjects_test)
  for (i in 1:N_subjects_test) {
    expect_true(is.matrix(result_core_hatsa$U_aligned_list[[i]]), paste("U_aligned_list[[ D ]]",i ,"is not a matrix"))
    expect_equal(dim(result_core_hatsa$U_aligned_list[[i]]), c(V_p_test, k_test))
    # Check consistency with stacked scores 's'
    s_block <- result_core_hatsa$s[((i-1)*V_p_test + 1):(i*V_p_test), , drop = FALSE]
    expect_equal(result_core_hatsa$U_aligned_list[[i]], s_block, 
                 info = paste("U_aligned_list[[ D ]]", i, "should match corresponding block in s_stacked"))

  }


  # $T_anchor_final: group anchor template (num_anchors x k)
  expect_true(is.matrix(result_core_hatsa$T_anchor_final), "result$T_anchor_final should be a matrix")
  expect_equal(dim(result_core_hatsa$T_anchor_final), c(num_anchors_test, k_test))
  
  # $Lambda_original_list
  expect_true(is.list(result_core_hatsa$Lambda_original_list), "result$Lambda_original_list should be a list")
  expect_equal(length(result_core_hatsa$Lambda_original_list), N_subjects_test)
  for (i in 1:N_subjects_test) {
    expect_true(is.numeric(result_core_hatsa$Lambda_original_list[[i]]), paste("Lambda_original_list[[ D ]]",i ,"is not numeric"))
    expect_equal(length(result_core_hatsa$Lambda_original_list[[i]]), k_test)
  }

  # Check that task-specific components are NULL or empty for core_hatsa
  expect_true(is.null(result_core_hatsa$W_task_list) || 
              (is.list(result_core_hatsa$W_task_list) && length(result_core_hatsa$W_task_list) == 0) ||
              all(sapply(result_core_hatsa$W_task_list, is.null)))
              
  expect_true(is.null(result_core_hatsa$L_task_list) || 
              (is.list(result_core_hatsa$L_task_list) && length(result_core_hatsa$L_task_list) == 0) ||
              all(sapply(result_core_hatsa$L_task_list, is.null)))
              
  expect_true(is.null(result_core_hatsa$W_hybrid_list) || 
              (is.list(result_core_hatsa$W_hybrid_list) && length(result_core_hatsa$W_hybrid_list) == 0) ||
              all(sapply(result_core_hatsa$W_hybrid_list, is.null)))
              
  expect_true(is.null(result_core_hatsa$L_hybrid_list) || 
              (is.list(result_core_hatsa$L_hybrid_list) && length(result_core_hatsa$L_hybrid_list) == 0) ||
              all(sapply(result_core_hatsa$L_hybrid_list, is.null)))
              
  expect_true(is.null(result_core_hatsa$U_task_list) || 
              (is.list(result_core_hatsa$U_task_list) && length(result_core_hatsa$U_task_list) == 0) ||
              all(sapply(result_core_hatsa$U_task_list, is.null)))
              
  expect_true(is.null(result_core_hatsa$Lambda_task_list) || 
              (is.list(result_core_hatsa$Lambda_task_list) && length(result_core_hatsa$Lambda_task_list) == 0) ||
              all(sapply(result_core_hatsa$Lambda_task_list, is.null)))

})

test_that("run_task_hatsa with lambda_blend method runs and produces valid output (sequential)", {
  if (requireNamespace("future", quietly = TRUE)) {
    old_plan <- future::plan(future::sequential)
    on.exit(future::plan(old_plan), add = TRUE)
  }
  if (!requireNamespace("Matrix", quietly = TRUE)) {
    skip("Matrix package not available, skipping lambda_blend test that uses sparse matrices.")
  }

  # 1. Synthetic Data Generation (similar to core_hatsa test)
  N_subjects_test <- 2 # Reduced for quicker test
  V_p_test <- 8
  T_subj_test <- 40
  k_test <- 2
  num_anchors_test <- 4

  subject_data_list_test <- vector("list", N_subjects_test)
  parcel_names_test <- paste0("P", 1:V_p_test)
  for (i in 1:N_subjects_test) {
    mat <- matrix(rnorm(T_subj_test * V_p_test), nrow = T_subj_test, ncol = V_p_test)
    colnames(mat) <- parcel_names_test
    subject_data_list_test[[i]] <- mat
  }

  # Mock task_data_list: Each element is a list containing W_task_i for that subject
  # W_task_i should be a V_p x V_p sparse matrix
  task_data_list_test <- vector("list", N_subjects_test)
  for (i in 1:N_subjects_test) {
    # Create a simple sparse positive definite-like matrix for W_task_i
    diag_vals <- runif(V_p_test, 0.5, 1.5)
    off_diag_count <- floor(V_p_test * V_p_test * 0.1) # 10% sparsity
    row_indices <- sample(1:V_p_test, off_diag_count, replace = TRUE)
    col_indices <- sample(1:V_p_test, off_diag_count, replace = TRUE)
    # Ensure symmetry for off-diagonals
    all_rows <- c(row_indices, col_indices, 1:V_p_test)
    all_cols <- c(col_indices, row_indices, 1:V_p_test) # Symmetrize
    all_vals <- c(runif(off_diag_count, -0.2, 0.2), runif(off_diag_count, -0.2, 0.2), diag_vals)
    
    W_task_mat_sparse <- Matrix::sparseMatrix(i = all_rows, j = all_cols, x = all_vals, 
                                        dims = c(V_p_test, V_p_test),
                                        symmetric = FALSE) # Build then forceSymmetric
    W_task_mat_sparse <- Matrix::forceSymmetric(W_task_mat_sparse, uplo="L") # Ensure it's symmetric after creation
    # Ensure positive definiteness by adding to diagonal if needed (simplified approach)
    # A more robust way is to ensure it's a valid covariance or regularized Laplacian.
    # For testing, we hope the construction of W_hybrid results in something reasonable.
    # What shape_basis expects for lambda_blend is W_task_i (z-scored). 
    # The z-scoring happens inside compute_task_matrices based on task_data_type.
    # For simplicity, we provide a W_task_i directly if task_matrix_method="from_precomputed_Wtask"
    # Let's assume task_matrix_method = "from_features" and provide some mock task features.
    # Each task_data_list[[i]] should be a list of matrices, where each matrix is features_c x V_p
    num_task_conditions <- 2
    num_task_features <- 5 # e.g. 5 features per condition
    task_features_subj_i <- vector("list", num_task_conditions)
    for(cond in 1:num_task_conditions){
        task_features_subj_i[[cond]] <- matrix(rnorm(num_task_features * V_p_test), 
                                                 nrow=num_task_features, ncol=V_p_test)
    }
    task_data_list_test[[i]] <- task_features_subj_i
  }

  anchor_indices_test <- 1:num_anchors_test
  result_lambda_blend <- NULL

  expect_no_error({
    result_lambda_blend <- run_task_hatsa(
      subject_data_list = subject_data_list_test,
      task_data_list = task_data_list_test, 
      anchor_indices = anchor_indices_test,
      spectral_rank_k = k_test,
      k_conn_pos = 3,
      k_conn_neg = 3,
      n_refine = 1, # Reduced for quicker test
      task_method = "lambda_blend", # Single string
      lambda_blend_value = 0.5,
      omega_mode = "fixed", # Single string
      row_augmentation = TRUE, 
      scale_omega_trace = TRUE, 
      verbose = FALSE
    )
  })

  expect_true(!is.null(result_lambda_blend), "run_task_hatsa (lambda_blend) should return non-NULL.")
  expect_s3_class(result_lambda_blend, "task_hatsa_projector")

  # Basic parameter checks
  expect_equal(result_lambda_blend$parameters$k, k_test)
  expect_equal(result_lambda_blend$parameters$N_subjects, N_subjects_test)
  expect_equal(result_lambda_blend$parameters$V_p, V_p_test)
  expect_equal(result_lambda_blend$parameters$task_method, "lambda_blend")
  expect_equal(result_lambda_blend$parameters$lambda_blend_value, 0.5)

  # Check dimensions of core components (v, s, R_final_list, U_original_list, T_anchor_final)
  # (Similar checks as in core_hatsa test)
  expect_true(is.matrix(result_lambda_blend$v) && all(dim(result_lambda_blend$v) == c(V_p_test, k_test)))
  expect_true(is.matrix(result_lambda_blend$s) && all(dim(result_lambda_blend$s) == c(N_subjects_test * V_p_test, k_test)))
  expect_false(any(is.na(result_lambda_blend$s)), "Stacked scores 's' should not contain NAs for lambda_blend with valid inputs.")

  expect_true(is.list(result_lambda_blend$R_final_list) && length(result_lambda_blend$R_final_list) == N_subjects_test)
  expect_true(all(sapply(result_lambda_blend$R_final_list, function(m) is.matrix(m) && all(dim(m) == c(k_test, k_test)))))
  
  expect_true(is.list(result_lambda_blend$U_original_list) && length(result_lambda_blend$U_original_list) == N_subjects_test)
  expect_true(all(sapply(result_lambda_blend$U_original_list, function(m) is.matrix(m) && all(dim(m) == c(V_p_test, k_test)))))

  expect_true(is.matrix(result_lambda_blend$T_anchor_final) && all(dim(result_lambda_blend$T_anchor_final) == c(num_anchors_test, k_test)))
  
  # Check presence and dimensions of task-specific or hybrid components
  # For lambda_blend, we expect W_hybrid_list, L_hybrid_list, U_hybrid_list, Lambda_hybrid_list
  # Or, the U_aligned_list would be the U_hybrid_list essentially.
  # The constructor task_hatsa_projector stores U_aligned_list which contains the final sketches (hybrid in this case)

  # W_task_list check - allow for NULL, empty list, or list of NULLs
  expect_true(
    is.null(result_lambda_blend$W_task_list) ||
    (is.list(result_lambda_blend$W_task_list) && 
     (length(result_lambda_blend$W_task_list) == N_subjects_test || 
      (length(result_lambda_blend$W_task_list) > 0 && 
       all(sapply(result_lambda_blend$W_task_list, function(w) is.null(w) || inherits(w, "Matrix"))))))
  )

  # W_hybrid_list check
  expect_true(
    is.null(result_lambda_blend$W_hybrid_list) ||
    (is.list(result_lambda_blend$W_hybrid_list) && 
     (length(result_lambda_blend$W_hybrid_list) == 0 || 
      all(sapply(result_lambda_blend$W_hybrid_list, function(w) is.null(w) || inherits(w, "Matrix")))))
  )

  # L_hybrid_list check
  expect_true(
    is.null(result_lambda_blend$L_hybrid_list) ||
    (is.list(result_lambda_blend$L_hybrid_list) && 
     (length(result_lambda_blend$L_hybrid_list) == 0 || 
      all(sapply(result_lambda_blend$L_hybrid_list, function(l) is.null(l) || inherits(l, "Matrix")))))
  )

  # U_aligned_list check
  expect_true(
    is.null(result_lambda_blend$U_aligned_list) ||
    (is.list(result_lambda_blend$U_aligned_list) && 
     length(result_lambda_blend$U_aligned_list) == N_subjects_test)
  )

  # Check original connectivity-based sketches
  expect_true(
    is.null(result_lambda_blend$U_conn_list) ||
    (is.list(result_lambda_blend$U_conn_list) && 
     (length(result_lambda_blend$U_conn_list) == 0 || 
      all(sapply(result_lambda_blend$U_conn_list, function(u) is.null(u) || is.matrix(u)))))
  )
  
  expect_true(
    is.null(result_lambda_blend$Lambda_conn_list) ||
    (is.list(result_lambda_blend$Lambda_conn_list) && 
     (length(result_lambda_blend$Lambda_conn_list) == 0 || 
      all(sapply(result_lambda_blend$Lambda_conn_list, function(l) is.null(l) || is.numeric(l)))))
  )

})

# TODO THFIX-007 Task 3: Test with future_plan = "multisession" if appropriate and stable

test_that("run_task_hatsa with core_hatsa method runs with future_plan = 'multisession'", {
  if (!requireNamespace("future", quietly = TRUE)) {
    skip("future package not available, skipping multisession test.")
  }
  if (future::availableCores() < 2) {
    skip("Less than 2 cores available, skipping multisession test as it might behave like sequential.")
  }

  old_plan <- future::plan(future::multisession) # Set to multisession for this test
  on.exit(future::plan(old_plan), add = TRUE)

  # 1. Synthetic Data Generation (same as the first core_hatsa sequential test)
  N_subjects_test <- 3
  V_p_test <- 10 
  T_subj_test <- 50 
  k_test <- 3 
  num_anchors_test <- 5

  subject_data_list_test <- vector("list", N_subjects_test)
  parcel_names_test <- paste0("P", 1:V_p_test)
  for (i in 1:N_subjects_test) {
    mat <- matrix(rnorm(T_subj_test * V_p_test), nrow = T_subj_test, ncol = V_p_test)
    colnames(mat) <- parcel_names_test
    subject_data_list_test[[i]] <- mat
  }
  anchor_indices_test <- 1:num_anchors_test

  result_multisession <- NULL
  expect_no_error({
    result_multisession <- run_task_hatsa(
      subject_data_list = subject_data_list_test,
      task_data_list = NULL, 
      anchor_indices = anchor_indices_test,
      spectral_rank_k = k_test,
      k_conn_pos = 3,
      k_conn_neg = 3,
      n_refine = 2,
      task_method = "core_hatsa", # Single string
      lambda_blend_value = 0.5,
      omega_mode = "fixed", # Single string
      row_augmentation = FALSE, # Explicitly disable row augmentation when task_data_list is NULL 
      scale_omega_trace = TRUE, 
      verbose = FALSE
    )
  })

  # 3. Assertions (identical structure to the sequential core_hatsa test)
  expect_true(!is.null(result_multisession), "run_task_hatsa (multisession) should return non-NULL.")
  expect_s3_class(result_multisession, "task_hatsa_projector")
  expect_s3_class(result_multisession, "hatsa_projector")

  expect_equal(result_multisession$parameters$k, k_test)
  expect_equal(result_multisession$parameters$N_subjects, N_subjects_test)
  expect_equal(result_multisession$parameters$V_p, V_p_test)
  expect_equal(length(result_multisession$parameters$anchor_indices), num_anchors_test)
  expect_equal(result_multisession$parameters$task_method, "core_hatsa")

  expect_true(is.matrix(result_multisession$v) && all(dim(result_multisession$v) == c(V_p_test, k_test)))
  expect_true(is.matrix(result_multisession$s) && all(dim(result_multisession$s) == c(N_subjects_test * V_p_test, k_test)))
  # For multisession, NA check might be too strict if minor numerical differences lead to issues previously unseen.
  # However, with core_hatsa and simple data, it should ideally still be NA-free if sequential was.
  # If this fails, it might indicate an issue with how data is handled/aggregated in parallel.
  expect_false(any(is.na(result_multisession$s)), "Stacked scores 's' (multisession) should not contain NAs for core_hatsa with valid inputs.")

  expect_true(is.list(result_multisession$R_final_list) && length(result_multisession$R_final_list) == N_subjects_test)
  expect_true(all(sapply(result_multisession$R_final_list, function(m) is.matrix(m) && all(dim(m) == c(k_test, k_test)))))
  
  expect_true(is.list(result_multisession$U_original_list) && length(result_multisession$U_original_list) == N_subjects_test)
  expect_true(all(sapply(result_multisession$U_original_list, function(m) is.matrix(m) && all(dim(m) == c(V_p_test, k_test)))))

  expect_true(is.list(result_multisession$U_aligned_list) && length(result_multisession$U_aligned_list) == N_subjects_test)
  expect_true(all(sapply(result_multisession$U_aligned_list, function(u) is.matrix(u) && all(dim(u) == c(V_p_test, k_test)))))
  for (i in 1:N_subjects_test) {
    s_block <- result_multisession$s[((i-1)*V_p_test + 1):(i*V_p_test), , drop = FALSE]
    expect_equal(result_multisession$U_aligned_list[[i]], s_block, 
                 info = paste("U_aligned_list[[ D ]] (multisession)", i, "should match corresponding block in s_stacked"))
  }

  expect_true(is.matrix(result_multisession$T_anchor_final) && all(dim(result_multisession$T_anchor_final) == c(num_anchors_test, k_test)))
  
  expect_true(is.list(result_multisession$Lambda_original_list) && length(result_multisession$Lambda_original_list) == N_subjects_test)
  expect_true(all(sapply(result_multisession$Lambda_original_list, function(l) is.numeric(l) && length(l) == k_test)))

  expect_true(is.null(result_multisession$W_task_list) || 
              (is.list(result_multisession$W_task_list) && length(result_multisession$W_task_list) == 0) ||
              all(sapply(result_multisession$W_task_list, is.null)))
              
  expect_true(is.null(result_multisession$L_task_list) || 
              (is.list(result_multisession$L_task_list) && length(result_multisession$L_task_list) == 0) ||
              all(sapply(result_multisession$L_task_list, is.null)))
              
  expect_true(is.null(result_multisession$W_hybrid_list) || 
              (is.list(result_multisession$W_hybrid_list) && length(result_multisession$W_hybrid_list) == 0) ||
              all(sapply(result_multisession$W_hybrid_list, is.null)))
              
  expect_true(is.null(result_multisession$L_hybrid_list) || 
              (is.list(result_multisession$L_hybrid_list) && length(result_multisession$L_hybrid_list) == 0) ||
              all(sapply(result_multisession$L_hybrid_list, is.null)))
              
  expect_true(is.null(result_multisession$U_task_list) || 
              (is.list(result_multisession$U_task_list) && length(result_multisession$U_task_list) == 0) ||
              all(sapply(result_multisession$U_task_list, is.null)))
              
  expect_true(is.null(result_multisession$Lambda_task_list) || 
              (is.list(result_multisession$Lambda_task_list) && length(result_multisession$Lambda_task_list) == 0) ||
              all(sapply(result_multisession$Lambda_task_list, is.null)))

})
</file>

<file path="tests/testthat/test-task_hatsa_main.R">
# test-task_hatsa_main.R

library(testthat)
skip_on_cran()

context("run_task_hatsa end-to-end pipeline")

make_toy_subject_data <- function(N, T, V) {
  lapply(1:N, function(i) matrix(rnorm(T*V), nrow=T, ncol=V))
}
make_toy_task_data <- function(N, C, V) {
  lapply(1:N, function(i) matrix(rnorm(C*V), nrow=C, ncol=V))
}

N <- 3; T <- 10; V <- 5; C <- 4
subject_data_list <- make_toy_subject_data(N, T, V)
task_data_list <- make_toy_task_data(N, C, V)
anchor_indices <- 1:2
spectral_rank_k <- 2
parcel_names <- paste0("P", 1:V)

# Minimal working example for core_hatsa

test_that("run_task_hatsa works for core_hatsa", {
  res <- run_task_hatsa(
    subject_data_list = subject_data_list,
    anchor_indices = anchor_indices,
    spectral_rank_k = spectral_rank_k,
    task_method = "core_hatsa",
    parcel_names = parcel_names,
    verbose = FALSE
  )
  expect_type(res, "list")
  expect_true("U_aligned_list" %in% names(res))
  expect_equal(length(res$U_aligned_list), N)
  expect_true(all(sapply(res$U_aligned_list, function(x) is.null(x) || (is.matrix(x) && ncol(x) == spectral_rank_k && nrow(x) == V))))
})

# Minimal working example for lambda_blend

test_that("run_task_hatsa works for lambda_blend", {
  res <- run_task_hatsa(
    subject_data_list = subject_data_list,
    anchor_indices = anchor_indices,
    spectral_rank_k = spectral_rank_k,
    task_data_list = task_data_list,
    task_method = "lambda_blend",
    lambda_blend_value = 0.2,
    parcel_names = parcel_names,
    verbose = FALSE
  )
  expect_type(res, "list")
  expect_true("U_aligned_list" %in% names(res))
  expect_equal(length(res$U_aligned_list), N)
})

# Minimal working example for gev_patch (if supported)
test_that("run_task_hatsa works for gev_patch", {
  res <- run_task_hatsa(
    subject_data_list = subject_data_list,
    anchor_indices = anchor_indices,
    spectral_rank_k = spectral_rank_k,
    task_data_list = task_data_list,
    task_method = "gev_patch",
    k_gev_dims = 2,
    parcel_names = parcel_names,
    verbose = FALSE
  )
  expect_type(res, "list")
  expect_true("U_aligned_list" %in% names(res))
  expect_equal(length(res$U_aligned_list), N)
})

# Handles missing/NULL task_data_list for core_hatsa
test_that("run_task_hatsa allows NULL task_data_list for core_hatsa", {
  expect_warning(run_task_hatsa(
    subject_data_list = subject_data_list,
    anchor_indices = anchor_indices,
    spectral_rank_k = spectral_rank_k,
    task_method = "core_hatsa",
    parcel_names = parcel_names,
    verbose = FALSE
  ), "backward compatibility")
})

# Handles errors for missing required arguments
test_that("run_task_hatsa errors for missing required arguments", {
  expect_error(run_task_hatsa(
    subject_data_list = NULL,
    anchor_indices = anchor_indices,
    spectral_rank_k = spectral_rank_k,
    task_method = "core_hatsa",
    parcel_names = parcel_names,
    verbose = FALSE
  ))
  expect_error(run_task_hatsa(
    subject_data_list = subject_data_list,
    anchor_indices = NULL,
    spectral_rank_k = spectral_rank_k,
    task_method = "core_hatsa",
    parcel_names = parcel_names,
    verbose = FALSE
  ))
})

# Handles row_augmentation and reliability_scores_list
test_that("run_task_hatsa works with row_augmentation and reliability_scores_list", {
  reliability_scores_list <- lapply(1:N, function(i) runif(C))
  res <- run_task_hatsa(
    subject_data_list = subject_data_list,
    anchor_indices = anchor_indices,
    spectral_rank_k = spectral_rank_k,
    task_data_list = task_data_list,
    task_method = "lambda_blend",
    row_augmentation = TRUE,
    omega_mode = "adaptive",
    reliability_scores_list = reliability_scores_list,
    parcel_names = parcel_names,
    verbose = FALSE
  )
  expect_type(res, "list")
  expect_true("U_aligned_list" %in% names(res))
})

# Handles custom W_task_helper_func
test_that("run_task_hatsa works with custom W_task_helper_func", {
  custom_fun <- function(mat, parcel_names, k_conn_task_pos, k_conn_task_neg, ...) {
    Matrix::Diagonal(n = ncol(mat), x = 1)
  }
  res <- run_task_hatsa(
    subject_data_list = subject_data_list,
    anchor_indices = anchor_indices,
    spectral_rank_k = spectral_rank_k,
    task_data_list = task_data_list,
    task_method = "lambda_blend",
    W_task_helper_func = custom_fun,
    parcel_names = parcel_names,
    verbose = FALSE
  )
  expect_type(res, "list")
  expect_true("U_aligned_list" %in% names(res))
})

# Edge cases: empty subject_data_list, anchor_indices out of bounds
test_that("run_task_hatsa errors for empty subject_data_list or out-of-bounds anchors", {
  expect_error(run_task_hatsa(
    subject_data_list = list(),
    anchor_indices = anchor_indices,
    spectral_rank_k = spectral_rank_k,
    task_method = "core_hatsa",
    parcel_names = parcel_names,
    verbose = FALSE
  ))
  expect_error(run_task_hatsa(
    subject_data_list = subject_data_list,
    anchor_indices = c(100),
    spectral_rank_k = spectral_rank_k,
    task_method = "core_hatsa",
    parcel_names = parcel_names,
    verbose = FALSE
  ))
})
</file>

<file path="tests/testthat/test-task_hatsa.R">
# test-task_hatsa.R

library(testthat)
skip_on_cran()

# Helper functions to generate test data
make_toy_subject_data <- function(N, T, V) {
  lapply(1:N, function(i) matrix(rnorm(T*V), nrow=T, ncol=V))
}
make_toy_task_data <- function(N, C, V) {
  lapply(1:N, function(i) matrix(rnorm(C*V), nrow=C, ncol=V))
}

# Test data
N <- 3; T <- 10; V <- 5; C <- 4
subject_data_list <- make_toy_subject_data(N, T, V)
task_data_list <- make_toy_task_data(N, C, V)
anchor_indices <- 1:2
spectral_rank_k <- 2
parcel_names <- paste0("P", 1:V)

test_that("task_hatsa basic usage works", {
  res <- task_hatsa(
    subject_data_list = subject_data_list,
    anchor_indices = anchor_indices,
    spectral_rank_k = spectral_rank_k,
    task_method = "core_hatsa",
    verbose = FALSE
  )
  expect_type(res, "list")
  expect_true("U_aligned_list" %in% names(res))
  expect_equal(length(res$U_aligned_list), N)
})

test_that("task_hatsa_opts works correctly", {
  # Create custom options
  opts <- task_hatsa_opts(
    lambda_blend_value = 0.2,
    k_conn_pos = 5,
    k_conn_neg = 5,
    n_refine = 3
  )
  
  # Test with the options
  res <- task_hatsa(
    subject_data_list = subject_data_list,
    anchor_indices = anchor_indices,
    spectral_rank_k = spectral_rank_k,
    task_data_list = task_data_list,
    task_method = "lambda_blend",
    opts = opts,
    verbose = FALSE
  )
  
  expect_type(res, "list")
  expect_true("U_aligned_list" %in% names(res))
})

test_that("task_hatsa works with ... arguments", {
  # Test passing options directly via ...
  res <- task_hatsa(
    subject_data_list = subject_data_list,
    anchor_indices = anchor_indices,
    spectral_rank_k = spectral_rank_k,
    task_data_list = task_data_list,
    task_method = "lambda_blend",
    lambda_blend_value = 0.3,  # via ...
    k_conn_pos = 6,            # via ...
    verbose = FALSE
  )
  
  expect_type(res, "list")
  expect_true("U_aligned_list" %in% names(res))
})

test_that("task_hatsa handles graph_mode parameter", {
  res <- task_hatsa(
    subject_data_list = subject_data_list,
    anchor_indices = anchor_indices,
    spectral_rank_k = spectral_rank_k,
    task_method = "core_hatsa",
    graph_mode = "schur_complement",  # Explicit graph_mode
    verbose = FALSE
  )
  
  expect_type(res, "list")
  expect_true("U_aligned_list" %in% names(res))
})

test_that("task_hatsa works with gev_patch method", {
  res <- task_hatsa(
    subject_data_list = subject_data_list,
    anchor_indices = anchor_indices,
    spectral_rank_k = spectral_rank_k,
    task_data_list = task_data_list,
    task_method = "gev_patch",
    k_gev_dims = 2,  # via ...
    verbose = FALSE
  )
  
  expect_type(res, "list")
  expect_true("U_aligned_list" %in% names(res))
})
</file>

<file path="tests/testthat/test-voxel_projection.R">
context("Voxel Projection Functionality")

# Helper function to generate mock coordinates
.generate_mock_coords <- function(N_points, N_dim = 3) {
  matrix(rnorm(N_points * N_dim), ncol = N_dim)
}

# Helper function to generate mock spectral components (U_orig_parcel, Lambda_orig_parcel)
.generate_mock_parcel_components <- function(V_p, k) {
  list(
    U_orig_parcel = matrix(rnorm(V_p * k), ncol = k),
    Lambda_orig_parcel = sort(runif(k, 0.1, 5), decreasing = TRUE) # Ensure positive and sorted
  )
}

# Helper function to generate mock voxel time-series list
.generate_mock_voxel_ts_list <- function(N_subjects, V_v, T_i_mean = 50, T_i_sd = 5) {
  lapply(1:N_subjects, function(i) {
    matrix(rnorm(round(rnorm(1, T_i_mean, T_i_sd)) * V_v), ncol = V_v)
  })
}

# Simplified helper from the other test file (or could be shared if test helpers are centralized)
.generate_mock_subject_data_parcels <- function(N_subjects, V_p, T_i_mean = 100, T_i_sd = 10) {
  lapply(1:N_subjects, function(i) {
    matrix(rnorm(round(rnorm(1, T_i_mean, T_i_sd)) * V_p), ncol = V_p)
  })
}

.get_default_hatsa_params_for_voxel_test <- function(V_p, k) {
  list(
    anchor_indices = sample(1:V_p, min(V_p, k + 1)), # Ensure enough for anchors
    spectral_rank_k = k,
    k_conn_pos = min(5, V_p -1),
    k_conn_neg = min(5, V_p-1),
    n_refine = 1 # Keep low for speed
  )
}


test_that("compute_voxel_basis_nystrom: basic functionality and dimensions", {
  V_p <- 30  # Number of parcels
  V_v <- 100 # Number of voxels
  k <- 5    # Number of components

  parcel_coords <- .generate_mock_coords(V_p)
  voxel_coords <- .generate_mock_coords(V_v)
  parcel_comps <- .generate_mock_parcel_components(V_p, k)

  # Test with default row_normalize_W = FALSE
  phi_voxel <- compute_voxel_basis_nystrom(
    voxel_coords = voxel_coords,
    parcel_coords = parcel_coords,
    U_orig_parcel = parcel_comps$U_orig_parcel,
    Lambda_orig_parcel = parcel_comps$Lambda_orig_parcel,
    n_nearest_parcels = 5,
    kernel_sigma = 5.0
  )

  expect_true(is.matrix(phi_voxel))
  expect_equal(nrow(phi_voxel), V_v)
  expect_equal(ncol(phi_voxel), k)
  expect_true(all(is.finite(phi_voxel)))

  # Test with row_normalize_W = TRUE
  phi_voxel_norm <- compute_voxel_basis_nystrom(
    voxel_coords = voxel_coords,
    parcel_coords = parcel_coords,
    U_orig_parcel = parcel_comps$U_orig_parcel,
    Lambda_orig_parcel = parcel_comps$Lambda_orig_parcel,
    n_nearest_parcels = 5,
    kernel_sigma = 5.0,
    row_normalize_W = TRUE
  )
  expect_true(is.matrix(phi_voxel_norm))
  expect_equal(nrow(phi_voxel_norm), V_v)
  expect_equal(ncol(phi_voxel_norm), k)
  expect_true(all(is.finite(phi_voxel_norm)))
  
  # Test kernel_sigma = "auto"
  phi_voxel_auto_sigma <- compute_voxel_basis_nystrom(
    voxel_coords = voxel_coords,
    parcel_coords = parcel_coords,
    U_orig_parcel = parcel_comps$U_orig_parcel,
    Lambda_orig_parcel = parcel_comps$Lambda_orig_parcel,
    n_nearest_parcels = 5,
    kernel_sigma = "auto"
  )
  expect_true(is.matrix(phi_voxel_auto_sigma))
  expect_equal(nrow(phi_voxel_auto_sigma), V_v)
  expect_equal(ncol(phi_voxel_auto_sigma), k)

  # Edge case: k=0
  parcel_comps_k0 <- .generate_mock_parcel_components(V_p, 0)
  phi_voxel_k0 <- compute_voxel_basis_nystrom(
    voxel_coords = voxel_coords,
    parcel_coords = parcel_coords,
    U_orig_parcel = parcel_comps_k0$U_orig_parcel, # V_p x 0 matrix
    Lambda_orig_parcel = parcel_comps_k0$Lambda_orig_parcel # empty numeric
  )
  expect_true(is.matrix(phi_voxel_k0))
  expect_equal(nrow(phi_voxel_k0), V_v)
  expect_equal(ncol(phi_voxel_k0), 0)

  # Edge case: V_v = 0
  phi_voxel_Vv0 <- compute_voxel_basis_nystrom(
    voxel_coords = .generate_mock_coords(0),
    parcel_coords = parcel_coords,
    U_orig_parcel = parcel_comps$U_orig_parcel,
    Lambda_orig_parcel = parcel_comps$Lambda_orig_parcel
  )
  expect_true(is.matrix(phi_voxel_Vv0))
  expect_equal(nrow(phi_voxel_Vv0), 0)
  expect_equal(ncol(phi_voxel_Vv0), k)
  
  # Edge case: V_p = 0 (should return V_v x k matrix of zeros)
   # This case is tricky because nn2 would fail if data has 0 rows.
   # compute_voxel_basis_nystrom has a check: `if (V_p == 0 || k == 0) return(matrix(0, nrow = V_v, ncol = k))`
   # So we need to ensure U_orig_parcel also matches this V_p=0 scenario.
  U_orig_parcel_Vp0 = matrix(0, nrow=0, ncol=k)
  phi_voxel_Vp0 <- compute_voxel_basis_nystrom(
    voxel_coords = voxel_coords,
    parcel_coords = .generate_mock_coords(0), # V_p = 0
    U_orig_parcel = U_orig_parcel_Vp0,      # 0 x k matrix
    Lambda_orig_parcel = parcel_comps$Lambda_orig_parcel # k length vector
  )
  expect_true(is.matrix(phi_voxel_Vp0))
  expect_equal(nrow(phi_voxel_Vp0), V_v)
  expect_equal(ncol(phi_voxel_Vp0), k)
  if (k > 0 && V_v > 0) expect_true(all(phi_voxel_Vp0 == 0))
  
  # Error for invalid n_nearest_parcels
  expect_error(compute_voxel_basis_nystrom(
    voxel_coords, parcel_coords, parcel_comps$U_orig_parcel, parcel_comps$Lambda_orig_parcel,
    n_nearest_parcels = 0
  ))
  
  # Error for invalid kernel_sigma
  expect_error(compute_voxel_basis_nystrom(
    voxel_coords, parcel_coords, parcel_comps$U_orig_parcel, parcel_comps$Lambda_orig_parcel,
    kernel_sigma = -1
  ))
  expect_error(compute_voxel_basis_nystrom(
    voxel_coords, parcel_coords, parcel_comps$U_orig_parcel, parcel_comps$Lambda_orig_parcel,
    kernel_sigma = "invalid_string"
  ))
  
})

test_that("project_voxels.hatsa_projector: basic functionality and dimensions", {
  V_p_fit <- 20
  N_subjects_fit <- 2
  k_fit <- 3
  
  V_v_proj <- 50 

  # 1. Create a mock hatsa_projector object
  # This typically involves running run_hatsa_core or manually constructing one.
  # For simplicity, we use run_hatsa_core with small data.
  fit_parcel_data <- .generate_mock_subject_data_parcels(N_subjects_fit, V_p_fit)
  fit_params <- .get_default_hatsa_params_for_voxel_test(V_p_fit, k_fit)

  hatsa_fitted_obj <- suppressMessages(try(run_hatsa_core(
    subject_data_list = fit_parcel_data,
    anchor_indices = fit_params$anchor_indices,
    spectral_rank_k = fit_params$spectral_rank_k,
    k_conn_pos = fit_params$k_conn_pos,
    k_conn_neg = fit_params$k_conn_neg,
    n_refine = fit_params$n_refine
  ), silent = TRUE))

  if (inherits(hatsa_fitted_obj, "try-error")) {
    skip(paste0("run_hatsa_core failed during setup for project_voxels tests: ", 
                attr(hatsa_fitted_obj, "condition")$message))
    return()
  }

  # 2. Prepare inputs for project_voxels
  # Use a different number of subjects for projection to test generalizability
  N_subjects_proj <- N_subjects_fit # Must match for now due to U_orig, Lambda_orig, R retrieval
  # N_subjects_proj <- 1 # Simpler case for initial test
  
  voxel_ts_list <- .generate_mock_voxel_ts_list(N_subjects_proj, V_v_proj)
  voxel_coords_proj <- .generate_mock_coords(V_v_proj)
  # Parcel coords must match those used for the hatsa_fitted_obj. 
  # This is a bit tricky as run_hatsa_core doesn't directly store/return them.
  # For testing, we assume parcel_coords are externally known and consistent.
  # Ideally, hatsa_projector might store parcel_coords if always needed for voxel projection.
  # For now, just generate some parcel_coords with the correct V_p_fit.
  parcel_coords_fit <- .generate_mock_coords(V_p_fit)
  
  # Test .validate_coordinate_inputs directly (internal helper)
  # Using expect_message or expect_warning if applicable for its interactive messages
  # For non-interactive, it returns NULL. We can test it doesn't error.
  expect_null(.validate_coordinate_inputs(voxel_coords_proj, parcel_coords_fit))
  # Test with deliberately mismatched scales to try and trigger a message (if interactive testing was set up)
  # e.g., .validate_coordinate_inputs(voxel_coords_proj * 100, parcel_coords_fit)

  projected_voxel_data <- suppressMessages(try(project_voxels(
    object = hatsa_fitted_obj,
    voxel_timeseries_list = voxel_ts_list,
    voxel_coords = voxel_coords_proj,
    parcel_coords = parcel_coords_fit, # Must match V_p of hatsa_fitted_obj
    n_nearest_parcels = 3,
    kernel_sigma = "auto"
  ), silent = TRUE))

  if (inherits(projected_voxel_data, "try-error")) {
    fail(paste0("project_voxels.hatsa_projector failed: ", 
                attr(projected_voxel_data, "condition")$message))
    return()
  }
  
  expect_true(is.list(projected_voxel_data))
  expect_length(projected_voxel_data, N_subjects_proj)

  for (i in 1:N_subjects_proj) {
    expect_true(is.matrix(projected_voxel_data[[i]]))
    T_i_subject <- nrow(voxel_ts_list[[i]]) # Get the actual T_i for this subject
    expect_equal(nrow(projected_voxel_data[[i]]), T_i_subject)
    expect_equal(ncol(projected_voxel_data[[i]]), k_fit)
    expect_true(all(is.finite(projected_voxel_data[[i]])))
  }
  
  # Test error conditions for project_voxels
  # - voxel_timeseries_list not a list
  expect_error(project_voxels(hatsa_fitted_obj, voxel_timeseries_list = voxel_ts_list[[1]], voxel_coords_proj, parcel_coords_fit))
  # - voxel_coords not a matrix or wrong ncol
  expect_error(project_voxels(hatsa_fitted_obj, voxel_ts_list, voxel_coords = as.data.frame(voxel_coords_proj), parcel_coords_fit))
  expect_error(project_voxels(hatsa_fitted_obj, voxel_ts_list, voxel_coords = voxel_coords_proj[,1:2], parcel_coords_fit))
  # - parcel_coords not a matrix or wrong ncol or V_p mismatch
  expect_error(project_voxels(hatsa_fitted_obj, voxel_ts_list, voxel_coords_proj, parcel_coords = .generate_mock_coords(V_p_fit + 1)))
  # - voxel_timeseries_list[[i]] ncol mismatch with voxel_coords nrow
  bad_voxel_ts_list <- voxel_ts_list
  bad_voxel_ts_list[[1]] <- matrix(rnorm(10 * (V_v_proj+1)), ncol=V_v_proj+1)
  # This should ideally produce a warning and NA matrix for that subject, not a hard error for the whole call
  # The current code in project_voxels.hatsa_projector does this.
  res_bad_ts <- suppressMessages(project_voxels(hatsa_fitted_obj, bad_voxel_ts_list, voxel_coords_proj, parcel_coords_fit))
  expect_true(is.na(res_bad_ts[[1]][1,1])) # Check if it was NA-ed out
  if (length(res_bad_ts) > 1) expect_false(is.na(res_bad_ts[[2]][1,1])) # Check other subjects are fine

})

# Further tests for project_voxels could include:
# - Consistency check: If two subjects have identical voxel time-series and identical
#   Phi_voxel_i (which implies identical U_orig_i, Lambda_orig_i from the model, and identical coords),
#   then C_voxel_coeffs_i should be identical. C_voxel_aligned_i should then only differ by R_i.
#   This is complex to set up perfectly due to U_orig_i differing per subject.
# - A simpler consistency: if R_i is identity for all subjects (e.g. if T_anchor_final was based on subject 1 and U_orig_1)
#   then aligned and unaligned coefficients should be similar for subject 1.
</file>

<file path="tests/testthat.R">
library(testthat)
library(hatsa) # Assuming your package name is hatsa

test_check("hatsa")
</file>

<file path="DESCRIPTION">
Package: hatsa
Type: Package
Title: What the Package Does (one line, title case)
Version: 0.0.0.9000
Authors@R: 
    person("Brad", "Buchsbaum", , "brad.buchsbaum@gmail.com", role = c("aut", "cre"),
           comment = c(ORCID = "YOUR-ORCID-ID"))
Description: What the package does (one paragraph).
License: MIT
Encoding: UTF-8
LazyData: true
RoxygenNote: 7.3.2.9000
Imports: 
    Matrix,
    RANN,
    stats,
    multivarious,
    expm
Suggests:
    testthat (>= 3.0.0),
    knitr,
    rmarkdown,
    vegan
Config/testthat/edition: 3
</file>

</files>
